import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import json
import traceback
import re
import os
import dotenv
from io import StringIO
from PIL import Image
import time
import matplotlib.font_manager as fm
import matplotlib
import numpy as np

# --- åŸºç¤ API å’Œè³‡æ–™è™•ç†å¥—ä»¶ ---
import google.generativeai as genai
# --- RAG ç›¸é—œå¥—ä»¶æš«æ™‚ä¿ç•™ï¼Œä½†ä¸ä¸€å®šæœƒè¢«ä½¿ç”¨ (2025/06/22) ---
from openai import OpenAI
import faiss
from langchain_openai import OpenAIEmbeddings
from langchain_community.document_loaders import CSVLoader
from langchain_community.vectorstores import FAISS as LangChainFAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter

# --- åˆå§‹åŒ–èˆ‡å¸¸æ•¸å®šç¾© ---
dotenv.load_dotenv()
UPLOAD_DIR = "uploaded_files"
if not os.path.exists(UPLOAD_DIR): os.makedirs(UPLOAD_DIR)

ROLE_DEFINITIONS = {
    "summarizer": { "name": "ğŸ“ æ‘˜è¦å°ˆå®¶", "system_prompt": "ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ‘˜è¦å°ˆå®¶ã€‚ä½ çš„ä»»å‹™æ˜¯å°‡æä¾›çš„ä»»ä½•æ–‡æœ¬æˆ–å°è©±ï¼Œæ¿ƒç¸®æˆæ¸…æ™°ã€ç°¡æ½”çš„ç¹é«”ä¸­æ–‡æ‘˜è¦ã€‚å°ˆæ³¨æ–¼è¦é»å’Œé—œéµçµè«–ã€‚", "session_id": "summarizer_chat" },
    "creative_writer": { "name": "âœï¸ å‰µæ„ä½œå®¶", "system_prompt": "ä½ æ˜¯ä¸€ä½å……æ»¿æƒ³åƒåŠ›çš„å‰µæ„ä½œå®¶ã€‚ä½ çš„ä»»å‹™æ˜¯å¹«åŠ©ä½¿ç”¨è€…å®Œæˆå‰µæ„å¯«ä½œï¼Œä¾‹å¦‚å¯«æ•…äº‹ã€è©©æ­Œã€åŠ‡æœ¬æˆ–è…¦åŠ›æ¿€ç›ªï¼Œå…¨éƒ¨ä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚", "session_id": "creative_writer_chat" }
}

# --- åŸºç¤è¼”åŠ©å‡½æ•¸ ---
def save_uploaded_file(uploaded_file):
    if not os.path.exists(UPLOAD_DIR): os.makedirs(UPLOAD_DIR)
    file_path = os.path.join(UPLOAD_DIR, uploaded_file.name)
    with open(file_path, "wb") as f: f.write(uploaded_file.getbuffer())
    return file_path

def add_user_image_to_main_chat(uploaded_file):
    try:
        file_path = save_uploaded_file(uploaded_file)
        st.session_state.pending_image_for_main_gemini = Image.open(file_path)
        st.image(st.session_state.pending_image_for_main_gemini, caption="åœ–ç‰‡å·²ä¸Šå‚³ï¼Œå°‡éš¨ä¸‹ä¸€æ¢æ–‡å­—è¨Šæ¯ç™¼é€ã€‚", use_container_width=True)
    except Exception as e: st.error(f"è™•ç†ä¸Šå‚³åœ–ç‰‡æ™‚å‡ºéŒ¯: {e}")

# --- RAG æ ¸å¿ƒå‡½å¼ - æš«æ™‚ä¿ç•™ç¨‹å¼ç¢¼ä»¥å‚™æœªä¾†æ¢å¾© (2025/06/22) ---
@st.cache_resource
def create_lc_retriever(file_path: str, openai_api_key: str):
    # æ­¤å‡½æ•¸ç‚º RAG åŠŸèƒ½çš„æ ¸å¿ƒï¼Œç›®å‰å·²æš«åœå‘¼å«ã€‚
    # æœªä¾†è‹¥è¦æ¢å¾© RAGï¼Œåªéœ€åœ¨å´é‚Šæ¬„é‡æ–°å•Ÿç”¨å°æ­¤å‡½æ•¸çš„å‘¼å«å³å¯ã€‚
    with st.status("æ­£åœ¨ä½¿ç”¨ LangChain å»ºç«‹çŸ¥è­˜åº«...", expanded=True) as status:
        try:
            status.update(label="æ­¥é©Ÿ 1/3ï¼šè¼‰å…¥èˆ‡åˆ‡å‰²æ–‡ä»¶...")
            loader = CSVLoader(file_path=file_path, encoding='utf-8')
            documents = loader.load()
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
            docs = text_splitter.split_documents(documents)
            status.update(label=f"æ­¥é©Ÿ 1/3 å®Œæˆï¼å·²åˆ‡å‰²æˆ {len(docs)} å€‹å€å¡Šã€‚")

            status.update(label="æ­¥é©Ÿ 2/3ï¼šå‘¼å« OpenAI API ç”Ÿæˆå‘é‡åµŒå…¥...")
            embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
            vector_store = LangChainFAISS.from_documents(docs, embeddings)
            status.update(label="æ­¥é©Ÿ 2/3 å®Œæˆï¼å‘é‡åµŒå…¥å·²ç”Ÿæˆã€‚")

            status.update(label="æ­¥é©Ÿ 3/3ï¼šæª¢ç´¢å™¨æº–å‚™å®Œæˆï¼", state="complete", expanded=False)
            return vector_store.as_retriever(search_kwargs={'k': 5})
        except Exception as e:
            st.error(f"å»ºç«‹çŸ¥è­˜åº«éç¨‹ä¸­ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}")
            status.update(label="å»ºç«‹å¤±æ•—", state="error")
            return None

# --- Gemini API ç›¸é—œå‡½å¼ (æ‰‹å‹•æ§åˆ¶) ---
def get_gemini_client(api_key):
    genai.configure(api_key=api_key)
    return genai.GenerativeModel("gemini-1.5-flash-latest")

def get_gemini_response_with_history(client, history, user_prompt):
    gemini_history = []
    for msg in history:
        role = "user" if msg["role"] == "human" else "model"
        gemini_history.append({"role": role, "parts": [msg["content"]]})
    
    chat = client.start_chat(history=gemini_history)
    response = chat.send_message(user_prompt)
    return response.text

def get_gemini_response_for_image(api_key, user_prompt, image_pil):
    if not api_key: return "éŒ¯èª¤ï¼šæœªè¨­å®š Gemini API Keyã€‚"
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel("gemini-1.5-flash-latest")
        response = model.generate_content([user_prompt, image_pil])
        st.session_state.pending_image_for_main_gemini = None
        return response.text
    except Exception as e:
        st.error(f"Gemini åœ–ç‰‡åˆ†æè«‹æ±‚å¤±æ•—: {e}")
        return f"éŒ¯èª¤: {e}"

def get_gemini_executive_analysis(api_key, executive_role_name, full_prompt):
    if not api_key: return f"éŒ¯èª¤ï¼šé«˜ç®¡å·¥ä½œæµ ({executive_role_name}) æœªèƒ½ç²å– Gemini API Keyã€‚"
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel("gemini-1.5-pro-latest")
        response = model.generate_content(full_prompt)
        return response.text
    except Exception as e:
        st.error(f"é«˜ç®¡åˆ†æ ({executive_role_name}) å¤±æ•—: {e}")
        return f"éŒ¯èª¤: {e}"

def generate_data_profile(df):
    if df is None or df.empty: return "æ²’æœ‰è³‡æ–™å¯ä¾›åˆ†æã€‚"
    buffer = StringIO()
    df.info(buf=buffer)
    profile_parts = [f"è³‡æ–™å½¢ç‹€: {df.shape}", f"æ¬„ä½è³‡è¨Š:\n{buffer.getvalue()}"]
    try: profile_parts.append(f"\næ•¸å€¼æ¬„ä½çµ±è¨ˆ:\n{df.describe(include='number').to_string()}")
    except: pass
    try: profile_parts.append(f"\né¡åˆ¥æ¬„ä½çµ±è¨ˆ:\n{df.describe(include=['object', 'category']).to_string()}")
    except: pass
    profile_parts.append(f"\nå‰ 5 ç­†è³‡æ–™:\n{df.head().to_string()}")
    return "\n".join(profile_parts)

# ------------------------------
# ä¸»æ‡‰ç”¨å…¥å£
# ------------------------------
def main():
    st.set_page_config(page_title="Gemini Multi-Function Bot", page_icon="âœ¨", layout="wide")
    st.title("âœ¨ Gemini å¤šåŠŸèƒ½ AI åŠ©ç† ")

    # --- åˆå§‹åŒ– Session States ---
    keys_to_init = {
        "retriever_chain": None, "uploaded_file_path": None, "last_uploaded_filename": None,
        "pending_image_for_main_gemini": None, "chat_histories": {},
        "executive_workflow_stage": "idle", "executive_user_query": "",
        "executive_data_profile_str": "", "executive_rag_context": "", "cfo_analysis_text": "",
        "coo_analysis_text": "", "ceo_summary_text": "",
        "sp_workflow_stage": "idle", "sp_user_query": "",
        "sp_final_report": ""
    }
    for key, default_value in keys_to_init.items():
        if key not in st.session_state:
            st.session_state[key] = default_value

    # --- å´é‚Šæ¬„ä»‹é¢ ---
    with st.sidebar:
        st.header("âš™ï¸ API é‡‘é‘°è¨­å®š")
        st.text_input("è«‹è¼¸å…¥æ‚¨çš„ Google Gemini API Key", type="password", key="gemini_api_key_input")
        
        # --- RAG ç›¸é—œ UI æš«æ™‚åœç”¨ (2025/06/22) ---
        # st.text_input("è«‹è¼¸å…¥æ‚¨çš„ OpenAI API Key", type="password", key="openai_api_key_input")
        # st.caption("Gemini ç”¨æ–¼èŠå¤©ï¼ŒOpenAI ç”¨æ–¼ RAG è³‡æ–™åµŒå…¥ã€‚")
        st.caption("Gemini ç”¨æ–¼æ‰€æœ‰ AI åˆ†æåŠŸèƒ½ã€‚")
        st.divider()

        # --- RAG åŠŸèƒ½ä¿®æ”¹ï¼šåƒ…ä¿ç•™æª”æ¡ˆä¸Šå‚³èˆ‡è·¯å¾‘ä¿å­˜ï¼Œç§»é™¤ RAG å»ºç«‹ (2025/06/22) ---
        st.subheader("ğŸ“ ä¸Šå‚³è³‡æ–™é€²è¡Œåˆ†æ")
        st.info("RAG çŸ¥è­˜åº«åŠŸèƒ½å·²æš«æ™‚åœç”¨ä»¥ç¯€çœæˆæœ¬ã€‚ç›®å‰åƒ…æ”¯æ´ä¸Šå‚³ CSV é€²è¡Œè³‡æ–™æ‘˜è¦åˆ†æã€‚")
        uploaded_file = st.file_uploader("ä¸Šå‚³ CSV ä»¥å•Ÿç”¨å·¥ä½œæµåˆ†æ", type=["csv"])
        
        if uploaded_file:
            if uploaded_file.name != st.session_state.get("last_uploaded_filename"):
                st.session_state.last_uploaded_filename = uploaded_file.name
                file_path = save_uploaded_file(uploaded_file)
                st.session_state.uploaded_file_path = file_path
                st.success(f"æª”æ¡ˆ '{uploaded_file.name}' ä¸Šå‚³æˆåŠŸï¼")
                # ä»¥ä¸‹ç‚ºåœç”¨çš„ RAG å»ºç«‹éç¨‹
                # openai_api_key = st.session_state.get("openai_api_key_input") or os.environ.get("OPENAI_API_KEY")
                # if not openai_api_key:
                #     st.error("è«‹åœ¨å´é‚Šæ¬„è¨­å®šæ‚¨çš„ OpenAI API Keyï¼")
                # else:
                #     st.session_state.retriever_chain = create_lc_retriever(file_path, openai_api_key)
        
        # if st.session_state.retriever_chain: st.success("âœ… RAG æª¢ç´¢åŠŸèƒ½å·²å•Ÿç”¨ï¼")

        st.subheader("ğŸ–¼ï¸ åœ–ç‰‡åˆ†æ")
        uploaded_image = st.file_uploader("ä¸Šå‚³åœ–ç‰‡é€²è¡Œåˆ†æ", type=["png", "jpg", "jpeg"])
        if uploaded_image: add_user_image_to_main_chat(uploaded_image)
        
        st.divider()
        if st.button("ğŸ—‘ï¸ æ¸…é™¤æ‰€æœ‰å°è©±èˆ‡è³‡æ–™"):
            api_keys = {
                'gemini_api_key_input': st.session_state.get('gemini_api_key_input'),
                # 'openai_api_key_input': st.session_state.get('openai_api_key_input')
            }
            st.session_state.clear()
            for key, value in api_keys.items():
                if value: st.session_state[key] = value

            st.cache_resource.clear()
            st.success("æ‰€æœ‰å°è©±ã€Session è¨˜æ†¶å’Œå¿«å–å·²æ¸…é™¤ï¼")
            st.rerun()

    # --- ä¸»å·¥ä½œå€ (æ¨™ç±¤é é¢) ---
    tab_titles = ["ğŸ’¬ ä¸»è¦èŠå¤©å®¤", "ğŸ’¼ éšæ®µå¼å·¥ä½œæµ", "ğŸ§© å–®ä¸€æç¤ºæ•´åˆå·¥ä½œæµ"] + [role["name"] for role in ROLE_DEFINITIONS.values()]
    tabs = st.tabs(tab_titles)

    # --- API Key æª¢æŸ¥ ---
    gemini_api_key = st.session_state.get("gemini_api_key_input") or os.environ.get("GOOGLE_API_KEY")
    if not gemini_api_key:
        st.warning("è«‹åœ¨å´é‚Šæ¬„è¼¸å…¥æ‚¨çš„ Google Gemini API Key ä»¥å•Ÿå‹•èŠå¤©åŠŸèƒ½ã€‚")
        st.stop()
    gemini_client = get_gemini_client(gemini_api_key)

    # --- ä¸»è¦èŠå¤©å®¤ (tabs[0]) ---
    with tabs[0]:
        st.header("ğŸ’¬ ä¸»è¦èŠå¤©å®¤")
        st.caption("å¯é€²è¡Œåœ–ç‰‡åˆ†ææˆ–ä¸€èˆ¬å°è©±ã€‚RAG è³‡æ–™å•ç­”å·²æš«åœã€‚")
        session_id = "main_chat"
        if session_id not in st.session_state.chat_histories: st.session_state.chat_histories[session_id] = []
        
        for msg in st.session_state.chat_histories[session_id]:
            with st.chat_message(msg["role"]): st.markdown(msg["content"])
        
        if user_input := st.chat_input("è«‹å°åœ–ç‰‡æå•æˆ–é–‹å§‹å°è©±..."):
            st.session_state.chat_histories[session_id].append({"role": "human", "content": user_input})
            with st.chat_message("human"): st.markdown(user_input)
            
            with st.chat_message("ai"):
                with st.spinner("æ­£åœ¨æ€è€ƒä¸­..."):
                    response = ""
                    # --- RAG é‚è¼¯æš«æ™‚åœç”¨ (2025/06/22) ---
                    # if st.session_state.retriever_chain:
                    #     retrieved_docs = st.session_state.retriever_chain.invoke(user_input)
                    #     context = "\n---\n".join([doc.page_content for doc in retrieved_docs])
                    #     prompt = f"è«‹æ ¹æ“šä¸Šä¸‹æ–‡å›ç­”å•é¡Œã€‚\n[ä¸Šä¸‹æ–‡]:\n{context}\n\n[å•é¡Œ]:\n{user_input}\n\n[å›ç­”]:"
                    #     response = gemini_client.generate_content(prompt).text
                    if st.session_state.pending_image_for_main_gemini:
                        response = get_gemini_response_for_image(gemini_api_key, user_input, st.session_state.pending_image_for_main_gemini)
                    else:
                        history = st.session_state.chat_histories[session_id][:-1]
                        response = get_gemini_response_with_history(gemini_client, history, user_input)
                    
                    st.markdown(response)
                    st.session_state.chat_histories[session_id].append({"role": "ai", "content": response})

    # --- éšæ®µå¼é«˜ç®¡å·¥ä½œæµ (tabs[1]) ---
    with tabs[1]:
        st.header("ğŸ’¼ éšæ®µå¼å·¥ä½œæµ")
        st.info("**æ–¹æ³•èªªæ˜**ï¼šæ­¤æµç¨‹æ¨¡æ“¬ä¸‰ä½ç¨ç«‹çš„å°ˆå®¶ã€‚æ¯ä¸€æ­¥éƒ½æ˜¯ä¸€æ¬¡ç¨ç«‹çš„ API è«‹æ±‚ã€‚åˆ†æåƒ…åŸºæ–¼ä¸Šå‚³ CSV çš„**çµ±è¨ˆæ‘˜è¦**ã€‚")
        
        st.session_state.executive_user_query = st.text_area(
            "è«‹è¼¸å…¥å•†æ¥­å•é¡Œä»¥å•Ÿå‹•åˆ†æ:", value=st.session_state.get("executive_user_query", ""), height=100, key="original_workflow_query"
        )
        # --- RAG ä¾è³´ç§»é™¤ (2025/06/22) ---
        can_start = bool(st.session_state.get("uploaded_file_path") and st.session_state.get("executive_user_query"))
        
        if st.button("ğŸš€ å•Ÿå‹•éšæ®µå¼åˆ†æ", disabled=not can_start, key="exec_flow_button"):
            st.session_state.executive_workflow_stage = "cfo_analysis_pending"
            st.session_state.cfo_analysis_text = ""
            st.session_state.coo_analysis_text = ""
            st.session_state.ceo_summary_text = ""
            st.rerun()

        if st.session_state.executive_workflow_stage == "cfo_analysis_pending":
            with st.spinner("CFO æ­£åœ¨ç¨ç«‹åˆ†æ..."):
                df = pd.read_csv(st.session_state.uploaded_file_path)
                data_profile = generate_data_profile(df)
                st.session_state.executive_data_profile_str = data_profile
                
                # --- RAG ç›¸é—œç¨‹å¼ç¢¼åœç”¨ (2025/06/22) ---
                # retriever = st.session_state.retriever_chain
                query = st.session_state.executive_user_query
                # retrieved_docs = retriever.invoke(query)
                # rag_context = "\n---\n".join([doc.page_content for doc in retrieved_docs])
                # st.session_state.executive_rag_context = rag_context

                # --- Prompt ä¿®æ”¹ï¼Œç§»é™¤ RAG å…§å®¹ (2025/06/22) ---
                cfo_prompt = f"ä½œç‚ºè²¡å‹™é•·(CFO)ï¼Œè«‹åŸºæ–¼ä½ çš„å°ˆæ¥­çŸ¥è­˜ï¼Œä¸¦åš´æ ¼åƒè€ƒä»¥ä¸‹æä¾›çš„ã€Œçµ±è¨ˆæ‘˜è¦ã€ï¼Œç‚ºå•†æ¥­å•é¡Œæä¾›è²¡å‹™è§’åº¦çš„ç°¡æ½”åˆ†æã€‚\n\n[å•†æ¥­å•é¡Œ]:\n{query}\n\n[çµ±è¨ˆæ‘˜è¦]:\n{data_profile}"
                response = get_gemini_executive_analysis(gemini_api_key, "CFO", cfo_prompt)
                st.session_state.cfo_analysis_text = response
                st.session_state.executive_workflow_stage = "coo_analysis_pending"
                st.rerun()
        
        if st.session_state.get('executive_data_profile_str'):
            with st.expander("æŸ¥çœ‹çµ±è¨ˆæ‘˜è¦"): st.text(st.session_state.executive_data_profile_str)
        # --- RAG ç›¸é—œ UI åœç”¨ (2025/06/22) ---
        # if st.session_state.get('executive_rag_context'):
        #     with st.expander("æŸ¥çœ‹ RAG æª¢ç´¢å‡ºçš„ç›¸é—œè³‡æ–™"): st.markdown(st.session_state.executive_rag_context)

        if st.session_state.cfo_analysis_text:
            st.subheader("ğŸ“Š è²¡å‹™é•· (CFO) åˆ†æ")
            st.markdown(st.session_state.cfo_analysis_text)
        
        if st.session_state.executive_workflow_stage == "coo_analysis_pending":
            with st.spinner("COO æ­£åœ¨åŸºæ–¼ CFO å ±å‘Šé€²è¡Œåˆ†æ..."):
                coo_prompt = f"ä½œç‚ºç‡Ÿé‹é•·(COO)ï¼Œè«‹åŸºæ–¼å•†æ¥­å•é¡Œã€çµ±è¨ˆæ‘˜è¦ä»¥åŠä¸‹æ–¹ CFO çš„åˆ†æï¼Œæä¾›ç‡Ÿé‹å±¤é¢çš„ç­–ç•¥èˆ‡æ½›åœ¨é¢¨éšªã€‚\n\n[å•†æ¥­å•é¡Œ]:\n{st.session_state.executive_user_query}\n\n[CFO çš„è²¡å‹™åˆ†æ]:\n{st.session_state.cfo_analysis_text}\n\n[çµ±è¨ˆæ‘˜è¦]:\n{st.session_state.executive_data_profile_str}"
                response = get_gemini_executive_analysis(gemini_api_key, "COO", coo_prompt)
                st.session_state.coo_analysis_text = response
                st.session_state.executive_workflow_stage = "ceo_summary_pending"
                st.rerun()

        if st.session_state.coo_analysis_text:
            st.subheader("ğŸ­ ç‡Ÿé‹é•· (COO) åˆ†æ")
            st.markdown(st.session_state.coo_analysis_text)

        if st.session_state.executive_workflow_stage == "ceo_summary_pending":
            with st.spinner("CEO æ­£åœ¨æ•´åˆæ‰€æœ‰å ±å‘Šé€²è¡Œç¸½çµ..."):
                ceo_prompt = f"ä½œç‚ºåŸ·è¡Œé•·(CEO)ï¼Œè«‹æ•´åˆæ‰€æœ‰è³‡è¨Šï¼ŒåŒ…å«åŸå§‹å•é¡Œã€CFO çš„è²¡å‹™åˆ†æã€COO çš„ç‡Ÿé‹åˆ†æï¼Œæä¾›ä¸€å€‹é«˜å±¤æ¬¡çš„ã€å¯åŸ·è¡Œçš„æ±ºç­–ç¸½çµèˆ‡æ˜ç¢ºçš„è¡Œå‹•å»ºè­°ã€‚\n\n[å•†æ¥­å•é¡Œ]:\n{st.session_state.executive_user_query}\n\n[CFO çš„è²¡å‹™åˆ†æ]:\n{st.session_state.cfo_analysis_text}\n\n[COO çš„ç‡Ÿé‹åˆ†æ]:\n{st.session_state.coo_analysis_text}"
                response = get_gemini_executive_analysis(gemini_api_key, "CEO", ceo_prompt)
                st.session_state.ceo_summary_text = response
                st.session_state.executive_workflow_stage = "completed"
                st.rerun()

        if st.session_state.ceo_summary_text:
            st.subheader("ğŸ‘‘ åŸ·è¡Œé•· (CEO) æœ€çµ‚æ±ºç­–")
            st.markdown(st.session_state.ceo_summary_text)

    # --- å–®ä¸€æç¤ºæ•´åˆå·¥ä½œæµ (tabs[2]) ---
    with tabs[2]:
        st.header("ğŸ§© å–®ä¸€æç¤ºæ•´åˆå·¥ä½œæµ")
        st.info("**æ–¹æ³•èªªæ˜**ï¼šæ­¤æµç¨‹æ¨¡æ“¬ä¸€å€‹å…¨èƒ½çš„ AI é«˜ç®¡åœ˜éšŠã€‚åˆ†æåƒ…åŸºæ–¼ä¸Šå‚³ CSV çš„**çµ±è¨ˆæ‘˜è¦**ã€‚")
        
        st.session_state.sp_user_query = st.text_area(
             "è«‹è¼¸å…¥å•†æ¥­å•é¡Œä»¥å•Ÿå‹•åˆ†æ:", value=st.session_state.get("sp_user_query", ""), height=100, key="sp_workflow_query"
        )
        # --- RAG ä¾è³´ç§»é™¤ (2025/06/22) ---
        can_start_sp = bool(st.session_state.get("uploaded_file_path") and st.session_state.get("sp_user_query"))

        if st.button("ğŸš€ å•Ÿå‹•å–®ä¸€æç¤ºæ•´åˆåˆ†æ", disabled=not can_start_sp, key="sp_flow_button"):
            st.session_state.sp_workflow_stage = "running"
            st.session_state.sp_final_report = ""
            
            with st.spinner("AI é«˜ç®¡åœ˜éšŠæ­£åœ¨é€²è¡Œå…¨é¢åˆ†æ..."):
                df = pd.read_csv(st.session_state.uploaded_file_path)
                data_profile = generate_data_profile(df)
                query = st.session_state.sp_user_query
                
                # --- Prompt ä¿®æ”¹ï¼Œç§»é™¤ RAG å…§å®¹ (2025/06/22) ---
                mega_prompt = f"""
ä½ æ˜¯ä¸€å€‹é ‚å°–çš„ AI å•†æ¥­åˆ†æåœ˜éšŠï¼Œèƒ½å¤ åœ¨ä¸€æ¬¡æ€è€ƒä¸­æ‰®æ¼”å¤šå€‹é«˜ç®¡è§’è‰²ã€‚ä½ çš„ä»»å‹™æ˜¯é‡å°çµ¦å®šçš„å•†æ¥­å•é¡Œå’Œæ•¸æ“šçµ±è¨ˆæ‘˜è¦ï¼Œç”Ÿæˆä¸€ä»½åŒ…å«ä¸‰å€‹éƒ¨åˆ†çš„å®Œæ•´åˆ†æå ±å‘Šã€‚

è«‹åš´æ ¼æŒ‰ç…§ä»¥ä¸‹çµæ§‹å’Œè¦æ±‚é€²è¡Œè¼¸å‡ºï¼Œä½¿ç”¨ Markdown æ¨™é¡Œä¾†å€åˆ†æ¯å€‹éƒ¨åˆ†ï¼š
---
### ğŸ“Š è²¡å‹™é•· (CFO) åˆ†æ
åœ¨æ­¤éƒ¨åˆ†ï¼Œè«‹å®Œå…¨ä»¥è²¡å‹™é•·çš„è§’åº¦æ€è€ƒã€‚å°ˆæ³¨æ–¼è²¡å‹™æŒ‡æ¨™ã€æˆæœ¬æ•ˆç›Šã€æŠ•è³‡å›å ±ç‡ã€æ¯›åˆ©ç‡ã€æ½›åœ¨çš„è²¡å‹™é¢¨éšªç­‰ã€‚ä½ çš„åˆ†æå¿…é ˆå®Œå…¨åŸºæ–¼æä¾›çš„çµ±è¨ˆæ‘˜è¦ã€‚

### ğŸ­ ç‡Ÿé‹é•· (COO) åˆ†æ
åœ¨æ­¤éƒ¨åˆ†ï¼Œè½‰æ›ç‚ºç‡Ÿé‹é•·çš„è§’è‰²ã€‚ä½ éœ€è¦æ€è€ƒï¼Œåœ¨CFOæœƒæå‡ºçš„è²¡å‹™è€ƒé‡ä¸‹ï¼Œç‡Ÿé‹ä¸Šæ˜¯å¦å¯è¡Œï¼Ÿåˆ†ææ½›åœ¨çš„æµç¨‹ã€ä¾›æ‡‰éˆã€äººåŠ›è³‡æºæˆ–åŸ·è¡Œé¢¨éšªã€‚

### ğŸ‘‘ åŸ·è¡Œé•· (CEO) æœ€çµ‚æ±ºç­–
åœ¨æ­¤éƒ¨åˆ†ï¼Œä½œç‚ºCEOï¼Œè«‹ç¶œåˆä¸Šè¿°çš„è²¡å‹™(CFO)å’Œç‡Ÿé‹(COO)åˆ†æã€‚ä¸è¦é‡è¤‡ç´°ç¯€ï¼Œè€Œæ˜¯æä¾›ä¸€å€‹é«˜å±¤æ¬¡çš„æˆ°ç•¥ç¸½çµã€‚æœ€çµ‚ï¼Œçµ¦å‡ºä¸€å€‹æ˜ç¢ºã€æœæ–·çš„**æ±ºç­–**ï¼Œä¸¦åˆ—å‡º 2-3 å€‹æœ€é‡è¦çš„**å¾ŒçºŒè¡Œå‹•å»ºè­°**ã€‚
---
ç¾åœ¨ï¼Œè«‹æ ¹æ“šä»¥ä¸‹è³‡è¨Šé–‹å§‹åˆ†æï¼š

**[å•†æ¥­å•é¡Œ]:**
{query}

**[è³‡æ–™çµ±è¨ˆæ‘˜è¦]:**
{data_profile}
"""
                response = get_gemini_executive_analysis(gemini_api_key, "IntegratedTeam", mega_prompt)
                st.session_state.sp_final_report = response
                st.session_state.sp_workflow_stage = "completed"
                st.rerun()

        if st.session_state.sp_workflow_stage == "completed" and st.session_state.sp_final_report:
            st.subheader("ğŸ“ˆ AI é«˜ç®¡åœ˜éšŠæ•´åˆå ±å‘Š")
            st.markdown(st.session_state.sp_final_report)

    # --- å…¶ä»– AI è§’è‰²æ¨™ç±¤ (tabs[3] å’Œ tabs[4]) ---
    role_tab_offset = 3 
    for i, (role_id, role_info) in enumerate(ROLE_DEFINITIONS.items()):
        with tabs[i + role_tab_offset]:
            st.header(role_info["name"])
            st.caption(role_info["system_prompt"])
            session_id = role_info["session_id"]
            if session_id not in st.session_state.chat_histories: st.session_state.chat_histories[session_id] = []

            for msg in st.session_state.chat_histories[session_id]:
                with st.chat_message(msg["role"]): st.markdown(msg["content"])

            if user_input := st.chat_input(f"èˆ‡ {role_info['name']} å°è©±...", key=session_id):
                st.session_state.chat_histories[session_id].append({"role": "human", "content": user_input})
                with st.chat_message("human"): st.markdown(user_input)
                
                with st.chat_message("ai"):
                    with st.spinner("æ­£åœ¨ç”Ÿæˆå›æ‡‰..."):
                        client_with_prompt = get_gemini_client(gemini_api_key)
                        try: client_with_prompt.system_instruction = role_info["system_prompt"]
                        except: pass
                        history = st.session_state.chat_histories[session_id][:-1]
                        final_prompt = f"{role_info['system_prompt']}\n\n{user_input}"
                        response = get_gemini_response_with_history(client_with_prompt, history, final_prompt)
                        st.markdown(response)
                        st.session_state.chat_histories[session_id].append({"role": "ai", "content": response})

if __name__ == "__main__":
    main()
