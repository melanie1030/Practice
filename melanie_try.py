import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import json
import traceback
import re
import os
import dotenv
from io import StringIO
from PIL import Image
import google.generativeai as genai
import time
import matplotlib.font_manager as fm
import matplotlib
from operator import itemgetter

# --- LangChain Imports ---
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.chat_history import ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_google_genai import ChatGoogleGenerativeAI
# --- æœ¬åœ°åµŒå…¥æ¨¡å‹ Import ---
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.document_loaders import CSVLoader
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter

# --- Role & Workflow Definitions ---
ROLE_DEFINITIONS = {
    "summarizer": {
        "name": "ğŸ“ æ‘˜è¦å°ˆå®¶",
        "system_prompt": "ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ‘˜è¦å°ˆå®¶ã€‚ä½ çš„ä»»å‹™æ˜¯å°‡æä¾›çš„ä»»ä½•æ–‡æœ¬æˆ–å°è©±ï¼Œæ¿ƒç¸®æˆæ¸…æ™°ã€ç°¡æ½”çš„ç¹é«”ä¸­æ–‡æ‘˜è¦ã€‚å°ˆæ³¨æ–¼è¦é»å’Œé—œéµçµè«–ã€‚",
        "session_id": "summarizer_session",
    },
    "creative_writer": {
        "name": "âœï¸ å‰µæ„ä½œå®¶",
        "system_prompt": "ä½ æ˜¯ä¸€ä½å……æ»¿æƒ³åƒåŠ›çš„å‰µæ„ä½œå®¶ã€‚ä½ çš„ä»»å‹™æ˜¯å¹«åŠ©ä½¿ç”¨è€…å®Œæˆå‰µæ„å¯«ä½œï¼Œä¾‹å¦‚å¯«æ•…äº‹ã€è©©æ­Œã€åŠ‡æœ¬æˆ–è…¦åŠ›æ¿€ç›ªï¼Œå…¨éƒ¨ä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚",
        "session_id": "creative_writer_session",
    }
}
EXECUTIVE_ROLE_IDS = { "CFO": "cfo_exec", "COO": "coo_exec", "CEO": "ceo_exec" }

# --- åˆå§‹åŒ–è¨­ç½® ---
dotenv.load_dotenv()
UPLOAD_DIR = "uploaded_files"
if not os.path.exists(UPLOAD_DIR): os.makedirs(UPLOAD_DIR)

# --- åŸºç¤è¼”åŠ©å‡½æ•¸ ---
def save_uploaded_file(uploaded_file):
    if not os.path.exists(UPLOAD_DIR): os.makedirs(UPLOAD_DIR)
    file_path = os.path.join(UPLOAD_DIR, uploaded_file.name)
    with open(file_path, "wb") as f: f.write(uploaded_file.getbuffer())
    return file_path

def add_user_image_to_main_chat(uploaded_file):
    try:
        file_path = save_uploaded_file(uploaded_file)
        st.session_state.pending_image_for_main_gemini = Image.open(file_path)
        st.image(st.session_state.pending_image_for_main_gemini, caption="åœ–ç‰‡å·²ä¸Šå‚³ï¼Œå°‡éš¨ä¸‹ä¸€æ¢æ–‡å­—è¨Šæ¯ç™¼é€ã€‚", use_container_width=True)
    except Exception as e: st.error(f"è™•ç†ä¸Šå‚³åœ–ç‰‡æ™‚å‡ºéŒ¯: {e}")

# --- LangChain æ ¸å¿ƒå‡½å¼ ---
def get_session_history(session_id: str) -> BaseChatMessageHistory:
    if "history_store" not in st.session_state: st.session_state.history_store = {}
    if session_id not in st.session_state.history_store:
        st.session_state.history_store[session_id] = ChatMessageHistory()
    return st.session_state.history_store[session_id]

@st.cache_resource
def create_vector_db_from_csv(file_path: str):
    """å¾ CSV æª”æ¡ˆå»ºç«‹å‘é‡è³‡æ–™åº« (100% ä½¿ç”¨æœ¬åœ°åµŒå…¥æ¨¡å‹)"""
    with st.status("æ­£åœ¨åˆå§‹åŒ–çŸ¥è­˜åº« (æœ¬åœ°æ¨¡å¼)...", expanded=True) as status:
        status.update(label="æ­¥é©Ÿ 1/3ï¼šæ­£åœ¨è¼‰å…¥èˆ‡æ¸…ç†è³‡æ–™...")
        df = pd.read_csv(file_path, encoding='utf-8')
        df.dropna(how='all', inplace=True)
        df.fillna("", inplace=True)
        clean_file_path = os.path.join(UPLOAD_DIR, f"clean_{os.path.basename(file_path)}")
        df.to_csv(clean_file_path, index=False)
        
        loader = CSVLoader(file_path=clean_file_path, encoding='utf-8')
        documents = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        docs = text_splitter.split_documents(documents)
        status.update(label=f"æ­¥é©Ÿ 1/3 å®Œæˆï¼å·²å°‡æ–‡ä»¶åˆ‡å‰²æˆ {len(docs)} å€‹å€å¡Šã€‚")

        status.update(label="æ­¥é©Ÿ 2/3ï¼šæ­£åœ¨ä½¿ç”¨æœ¬åœ°æ¨¡å‹ç”Ÿæˆå‘é‡åµŒå…¥ (åˆæ¬¡åŸ·è¡Œæœƒä¸‹è¼‰æ¨¡å‹ï¼Œè«‹ç¨å€™)...")
        model_name = "paraphrase-multilingual-MiniLM-L12-v2"
        # å¼·åˆ¶ä½¿ç”¨ CPUï¼Œé¿å…åœ¨æ²’æœ‰ GPU çš„ç’°å¢ƒä¸­å‡ºéŒ¯
        embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs={'device': 'cpu'})
        
        vector_store = FAISS.from_documents(docs, embeddings)
        status.update(label="æ­¥é©Ÿ 2/3 å®Œæˆï¼å‘é‡åµŒå…¥å·²åœ¨æœ¬åœ°ç”Ÿæˆã€‚")

        status.update(label="æ­¥é©Ÿ 3/3ï¼šçŸ¥è­˜åº«æº–å‚™å®Œæˆï¼", state="complete", expanded=False)
    st.success("çŸ¥è­˜åº«å»ºç«‹å®Œæˆï¼")
    return vector_store

def create_rag_chain(vector_store):
    """å»ºç«‹ä¸€å€‹æ•´åˆäº†ã€Œå°è©±å…§è¨˜æ†¶ã€çš„ RAG éˆ"""
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest", temperature=0.7)
    retriever = vector_store.as_retriever()
    prompt_template = ChatPromptTemplate.from_messages([
        ("system", "è«‹æ ¹æ“šã€Œä¸Šä¸‹æ–‡ã€å›ç­”å•é¡Œã€‚\n[ä¸Šä¸‹æ–‡]:\n{context}"),
        ("human", "{input}"),
    ])
    base_rag_chain = ({ "context": itemgetter("input") | retriever, "input": itemgetter("input")} | prompt_template | llm | StrOutputParser())
    rag_chain_with_history = RunnableWithMessageHistory(base_rag_chain, get_session_history, input_messages_key="input", history_messages_key="history")
    return rag_chain_with_history

def create_generic_chat_chain(system_prompt: str):
    """ç‚ºç‰¹å®šè§’è‰²å»ºç«‹ä¸€å€‹å…·å‚™ã€Œå°è©±å…§è¨˜æ†¶ã€çš„é€šç”¨èŠå¤©éˆ"""
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest", temperature=0.7)
    prompt_template = ChatPromptTemplate.from_messages([("system", system_prompt), ("human", "{input}")])
    base_chain = prompt_template | llm | StrOutputParser()
    chain_with_history = RunnableWithMessageHistory(base_chain, get_session_history, input_messages_key="input", history_messages_key="history")
    return chain_with_history

# --- å…¶ä»– Gemini å‘¼å«å‡½å¼ ---
def get_gemini_response_for_image(user_prompt, image_pil):
    """å°ˆé–€ç”¨æ–¼è™•ç†åœ–ç‰‡åˆ†æçš„å‡½å¼"""
    api_key = st.session_state.get("gemini_api_key_input") or os.environ.get("GOOGLE_API_KEY")
    if not api_key: return "éŒ¯èª¤ï¼šæœªè¨­å®š Gemini API Keyã€‚"
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel("gemini-1.5-flash-latest")
        response = model.generate_content([user_prompt, image_pil])
        st.session_state.pending_image_for_main_gemini = None # æ¸…é™¤å¾…è™•ç†åœ–ç‰‡
        return response.text
    except Exception as e:
        st.error(f"Gemini åœ–ç‰‡åˆ†æè«‹æ±‚å¤±æ•—: {e}")
        return f"éŒ¯èª¤: {e}"

def get_gemini_executive_analysis(executive_role_name, full_prompt):
    api_key = st.session_state.get("gemini_api_key_input") or os.environ.get("GOOGLE_API_KEY")
    if not api_key: return f"éŒ¯èª¤ï¼šé«˜ç®¡å·¥ä½œæµ ({executive_role_name}) æœªèƒ½ç²å– Gemini API Keyã€‚"
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel("gemini-1.5-pro-latest")
        response = model.generate_content(full_prompt)
        return response.text
    except Exception as e:
        st.error(f"é«˜ç®¡åˆ†æ ({executive_role_name}) å¤±æ•—: {e}")
        return f"éŒ¯èª¤: {e}"
        
def generate_data_profile(df):
    if df is None or df.empty: return "æ²’æœ‰è³‡æ–™å¯ä¾›åˆ†æã€‚"
    buffer = StringIO()
    df.info(buf=buffer)
    profile_parts = [f"è³‡æ–™å½¢ç‹€: {df.shape}", f"æ¬„ä½è³‡è¨Š:\n{buffer.getvalue()}"]
    try: profile_parts.append(f"\næ•¸å€¼æ¬„ä½çµ±è¨ˆ:\n{df.describe(include='number').to_string()}")
    except: pass
    try: profile_parts.append(f"\né¡åˆ¥æ¬„ä½çµ±è¨ˆ:\n{df.describe(include=['object', 'category']).to_string()}")
    except: pass
    profile_parts.append(f"\nå‰ 5 ç­†è³‡æ–™:\n{df.head().to_string()}")
    return "\n".join(profile_parts)

# ------------------------------
# ä¸»æ‡‰ç”¨å…¥å£
# ------------------------------
def main():
    st.set_page_config(page_title="Gemini Multi-Function Bot", page_icon="âœ¨", layout="wide")
    st.title("âœ¨ Gemini å¤šåŠŸèƒ½ AI åŠ©ç†")

    # --- åˆå§‹åŒ– Session States ---
    keys_to_init = {
        "rag_chain": None, "uploaded_file_path": None, "last_uploaded_filename": None,
        "pending_image_for_main_gemini": None, "executive_workflow_stage": "idle", "executive_user_query": "",
        "executive_data_profile_str": "", "cfo_analysis_text": "", "coo_analysis_text": "", "ceo_summary_text": "",
        "debug_mode": False
    }
    for key, default_value in keys_to_init.items():
        if key not in st.session_state:
            st.session_state[key] = default_value

    # --- å´é‚Šæ¬„ä»‹é¢ ---
    with st.sidebar:
        st.header("âš™ï¸ è¨­å®š")
        st.text_input(
            "è«‹è¼¸å…¥æ‚¨çš„ Google Gemini API Key",
            value=st.session_state.get("gemini_api_key_input", ""), type="password", key="gemini_api_key_input"
        )
        st.caption("å„ªå…ˆä½¿ç”¨æ­¤è™•è¼¸å…¥çš„é‡‘é‘°ã€‚")
        st.divider()

        st.subheader("ğŸ“ è³‡æ–™å•ç­” (RAG)")
        uploaded_file = st.file_uploader("ä¸Šå‚³ CSV ä»¥å•Ÿç”¨ RAG å•ç­”åŠŸèƒ½", type=["csv"])
        
        if uploaded_file:
            if uploaded_file.name != st.session_state.get("last_uploaded_filename"):
                st.session_state.last_uploaded_filename = uploaded_file.name
                file_path = save_uploaded_file(uploaded_file)
                st.session_state.uploaded_file_path = file_path
                vector_store = create_vector_db_from_csv(file_path)
                st.session_state.rag_chain = create_rag_chain(vector_store)
        
        if st.session_state.rag_chain:
            st.success("âœ… RAG å•ç­”åŠŸèƒ½å·²å•Ÿç”¨ï¼")

        st.subheader("ğŸ–¼ï¸ åœ–ç‰‡åˆ†æ")
        uploaded_image = st.file_uploader("ä¸Šå‚³åœ–ç‰‡é€²è¡Œåˆ†æ", type=["png", "jpg", "jpeg"])
        if uploaded_image: add_user_image_to_main_chat(uploaded_image)
        st.divider()

        if st.button("ğŸ—‘ï¸ æ¸…é™¤æ‰€æœ‰å°è©±èˆ‡è³‡æ–™"):
            keys_to_clear = [k for k in st.session_state.keys() if k != 'gemini_api_key_input']
            for key in keys_to_clear:
                st.session_state.pop(key)
            st.cache_resource.clear()
            st.success("æ‰€æœ‰å°è©±ã€Session è¨˜æ†¶å’Œå¿«å–å·²æ¸…é™¤ï¼")
            st.rerun()

        st.session_state.debug_mode = st.checkbox("å•Ÿç”¨åµéŒ¯æ¨¡å¼", value=st.session_state.debug_mode)

    # --- ä¸»å·¥ä½œå€ (æ¨™ç±¤é é¢) ---
    tab_titles = ["ğŸ’¬ ä¸»è¦èŠå¤©å®¤", "ğŸ’¼ é«˜ç®¡å·¥ä½œæµ"] + [role["name"] for role in ROLE_DEFINITIONS.values()]
    tabs = st.tabs(tab_titles)

    # --- ä¸»è¦èŠå¤©å®¤ ---
    with tabs[0]:
        st.header("ğŸ’¬ ä¸»è¦èŠå¤©å®¤")
        st.caption("å¯é€²è¡Œ RAG è³‡æ–™å•ç­”ã€åœ–ç‰‡åˆ†æã€æˆ–ä¸€èˆ¬å°è©± (å‡å…·å‚™ Session è¨˜æ†¶)ã€‚")
        
        MAIN_CHAT_SESSION_ID = "main_chat_session"
        history = get_session_history(MAIN_CHAT_SESSION_ID)

        for msg in history.messages:
            with st.chat_message(msg.type):
                st.markdown(msg.content)
        
        if user_input := st.chat_input("è«‹å°æ•¸æ“šæˆ–åœ–ç‰‡æå•..."):
            with st.chat_message("human"):
                st.markdown(user_input)
            
            with st.chat_message("ai"):
                with st.spinner("Gemini æ­£åœ¨æ€è€ƒä¸­..."):
                    response = ""
                    # æƒ…å¢ƒ1ï¼šRAG å•ç­”
                    if st.session_state.rag_chain:
                        response = st.session_state.rag_chain.invoke(
                            {"input": user_input},
                            config={"configurable": {"session_id": MAIN_CHAT_SESSION_ID}}
                        )
                    # æƒ…å¢ƒ2ï¼šåœ–ç‰‡å•ç­”
                    elif st.session_state.pending_image_for_main_gemini:
                        response = get_gemini_response_for_image(
                            user_input, st.session_state.pending_image_for_main_gemini
                        )
                        history.add_user_message(user_input)
                        history.add_ai_message(response)
                    # æƒ…å¢ƒ3ï¼šä¸€èˆ¬èŠå¤©
                    else:
                        if "general_chat_chain" not in st.session_state:
                            st.session_state.general_chat_chain = create_generic_chat_chain("ä½ æ˜¯ä¸€å€‹æ¨‚æ–¼åŠ©äººçš„ AI åŠ©ç†ã€‚")
                        response = st.session_state.general_chat_chain.invoke(
                            {"input": user_input},
                            config={"configurable": {"session_id": MAIN_CHAT_SESSION_ID}}
                        )
                    st.markdown(response)

    # --- é«˜ç®¡å·¥ä½œæµæ¨™ç±¤ ---
    with tabs[1]:
        st.header("ğŸ’¼ é«˜ç®¡å·¥ä½œæµ (ç”± Gemini Pro é©…å‹•)")
        st.write("è«‹å…ˆåœ¨å´é‚Šæ¬„ä¸Šå‚³CSVè³‡æ–™ï¼Œç„¶å¾Œåœ¨æ­¤è¼¸å…¥å•†æ¥­å•é¡Œï¼Œæœ€å¾Œé»æ“ŠæŒ‰éˆ•å•Ÿå‹•åˆ†æã€‚")
        st.session_state.executive_user_query = st.text_area(
            "è«‹è¼¸å…¥å•†æ¥­å•é¡Œä»¥å•Ÿå‹•åˆ†æ:", value=st.session_state.get("executive_user_query", ""), height=100
        )
        can_start = bool(st.session_state.get("uploaded_file_path") and st.session_state.get("executive_user_query"))
        
        if st.button("ğŸš€ å•Ÿå‹•/é‡å•Ÿé«˜ç®¡åˆ†æ", disabled=not can_start):
             st.session_state.executive_workflow_stage = "data_profiling_pending"
             st.session_state.cfo_analysis_text = ""
             st.session_state.coo_analysis_text = ""
             st.session_state.ceo_summary_text = ""
             st.rerun()
        
        if st.session_state.executive_workflow_stage == "data_profiling_pending":
             with st.spinner("æ­£åœ¨ç”Ÿæˆè³‡æ–™æ‘˜è¦..."):
                df = pd.read_csv(st.session_state.uploaded_file_path)
                st.session_state.executive_data_profile_str = generate_data_profile(df)
                st.session_state.executive_workflow_stage = "cfo_analysis_pending"
                st.rerun()

        if st.session_state.get('executive_data_profile_str'):
            with st.expander("æŸ¥çœ‹è³‡æ–™æ‘˜è¦"):
                st.text(st.session_state.executive_data_profile_str)
        
        # --- CFO/COO/CEO åŸ·è¡Œé‚è¼¯ ---
        # (æ­¤è™•çœç•¥äº†å…·é«”çš„ CFO/COO/CEO å‘¼å«é‚è¼¯ä»¥ä¿æŒç°¡æ½”ï¼Œä½†å®ƒå€‘åœ¨æ‚¨çš„å®Œæ•´æª”æ¡ˆä¸­æ‡‰è©²è¢«ä¿ç•™)

    # --- å…¶ä»– AI è§’è‰²æ¨™ç±¤ ---
    for i, (role_id, role_info) in enumerate(ROLE_DEFINITIONS.items()):
        with tabs[i + 2]:
            st.header(role_info["name"])
            st.caption(role_info["system_prompt"])
            
            if f"{role_id}_chain" not in st.session_state:
                st.session_state[f"{role_id}_chain"] = create_generic_chat_chain(role_info["system_prompt"])
            
            chain = st.session_state[f"{role_id}_chain"]
            session_id = role_info["session_id"]
            history = get_session_history(session_id)
            
            for msg in history.messages:
                with st.chat_message(msg.type):
                    st.markdown(msg.content)
            
            if user_input := st.chat_input(f"èˆ‡ {role_info['name']} å°è©±..."):
                with st.chat_message("human"):
                    st.markdown(user_input)
                with st.chat_message("ai"):
                    with st.spinner("æ­£åœ¨ç”Ÿæˆå›æ‡‰..."):
                        response = chain.invoke(
                            {"input": user_input},
                            config={"configurable": {"session_id": session_id}}
                        )
                        st.markdown(response)

if __name__ == "__main__":
    main()
