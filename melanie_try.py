import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import json
import traceback
import re
import os
import dotenv
from io import StringIO
from PIL import Image
import google.generativeai as genai
import time
import matplotlib.font_manager as fm
import matplotlib
from operator import itemgetter

# --- LangChain å…¨å®¶æ¡¶ Imports ---
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.chat_history import BaseChatMessageHistory, ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.document_loaders import CSVLoader
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter

# --- Role & Workflow Definitions ---
ROLE_DEFINITIONS = {
    "summarizer": {
        "name": "ğŸ“ æ‘˜è¦å°ˆå®¶",
        "system_prompt": "ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ‘˜è¦å°ˆå®¶ã€‚ä½ çš„ä»»å‹™æ˜¯å°‡æä¾›çš„ä»»ä½•æ–‡æœ¬æˆ–å°è©±ï¼Œæ¿ƒç¸®æˆæ¸…æ™°ã€ç°¡æ½”çš„ç¹é«”ä¸­æ–‡æ‘˜è¦ã€‚å°ˆæ³¨æ–¼è¦é»å’Œé—œéµçµè«–ã€‚",
        "session_id": "summarizer_session",
    },
    "creative_writer": {
        "name": "âœï¸ å‰µæ„ä½œå®¶",
        "system_prompt": "ä½ æ˜¯ä¸€ä½å……æ»¿æƒ³åƒåŠ›çš„å‰µæ„ä½œå®¶ã€‚ä½ çš„ä»»å‹™æ˜¯å¹«åŠ©ä½¿ç”¨è€…å®Œæˆå‰µæ„å¯«ä½œï¼Œä¾‹å¦‚å¯«æ•…äº‹ã€è©©æ­Œã€åŠ‡æœ¬æˆ–è…¦åŠ›æ¿€ç›ªï¼Œå…¨éƒ¨ä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚",
        "session_id": "creative_writer_session",
    }
}

EXECUTIVE_ROLE_IDS = { "CFO": "cfo_exec", "COO": "coo_exec", "CEO": "ceo_exec" }

# --- ä¸­æ–‡å­—å‹è¨­å®š ---
try:
    font_path = "./fonts/msjh.ttc"
    if os.path.exists(font_path):
        fm.fontManager.addfont(font_path)
        matplotlib.rcParams['font.family'] = fm.FontProperties(fname=font_path).get_name()
    matplotlib.rcParams['axes.unicode_minus'] = False
except Exception as e:
    print(f"ä¸­æ–‡å­—å‹è¼‰å…¥å¤±æ•—: {e}")
    st.warning("ä¸­æ–‡å­—å‹è¼‰å…¥å¤±æ•—ï¼Œåœ–è¡¨ä¸­çš„ä¸­æ–‡å¯èƒ½ç„¡æ³•æ­£å¸¸é¡¯ç¤ºã€‚")

# --- åˆå§‹åŒ–è¨­ç½® ---
dotenv.load_dotenv()
UPLOAD_DIR = "uploaded_files"
if not os.path.exists(UPLOAD_DIR): os.makedirs(UPLOAD_DIR)

# --- åŸºç¤è¼”åŠ©å‡½æ•¸ (ä¿æŒä¸è®Š) ---
def save_uploaded_file(uploaded_file):
    if not os.path.exists(UPLOAD_DIR): os.makedirs(UPLOAD_DIR)
    file_path = os.path.join(UPLOAD_DIR, uploaded_file.name)
    with open(file_path, "wb") as f:
        f.write(uploaded_file.getbuffer())
    return file_path

def add_user_image_to_main_chat(uploaded_file):
    try:
        file_path = save_uploaded_file(uploaded_file)
        image_pil = Image.open(file_path)
        st.session_state.pending_image_for_main_gemini = image_pil
        st.image(image_pil, caption="åœ–ç‰‡å·²ä¸Šå‚³ï¼Œå°‡éš¨ä¸‹ä¸€æ¢æ–‡å­—è¨Šæ¯ç™¼é€ã€‚", use_container_width=True)
    except Exception as e:
        st.error(f"è™•ç†ä¸Šå‚³åœ–ç‰‡æ™‚å‡ºéŒ¯: {e}")

# --- LangChain æ ¸å¿ƒå‡½å¼ (In-Memory Session ç‰ˆ) ---

def get_session_history(session_id: str) -> BaseChatMessageHistory:
    """å¾ Streamlit Session State ç²å–æˆ–å‰µå»ºå°è©±æ­·å²ç‰©ä»¶"""
    if "history_store" not in st.session_state:
        st.session_state.history_store = {}
    if session_id not in st.session_state.history_store:
        st.session_state.history_store[session_id] = ChatMessageHistory()
    return st.session_state.history_store[session_id]

def create_rag_chain(vector_store):
    """å»ºç«‹ä¸€å€‹æ•´åˆäº†ã€Œå°è©±å…§è¨˜æ†¶ã€çš„ RAG éˆ"""
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest", temperature=0.7)
    retriever = vector_store.as_retriever()
    
    prompt_template = ChatPromptTemplate.from_messages([
        ("system", """
        è«‹æ ¹æ“šä»¥ä¸‹æä¾›çš„ã€Œå°è©±æ­·å²ã€å’Œå¾è³‡æ–™åº«æª¢ç´¢åˆ°çš„ã€Œä¸Šä¸‹æ–‡ã€ä¾†å›ç­”å•é¡Œã€‚
        è«‹å„ªå…ˆä½¿ç”¨ã€Œä¸Šä¸‹æ–‡ã€ä¸­çš„è³‡è¨Šã€‚å¦‚æœã€Œä¸Šä¸‹æ–‡ã€ä¸è¶³ï¼Œå¯ä»¥åƒè€ƒã€Œå°è©±æ­·å²ã€ã€‚
        å¦‚æœéƒ½ç„¡æ³•å›ç­”ï¼Œè«‹ç›´èªªã€Œæ ¹æ“šæˆ‘æ‰€æ“æœ‰çš„è³‡æ–™ï¼Œç„¡æ³•å›ç­”é€™å€‹å•é¡Œã€ã€‚

        [ä¸Šä¸‹æ–‡]:
        {context}
        """),
        ("human", "{input}"),
    ])

    base_rag_chain = (
        {
            "context": itemgetter("input") | retriever,
            "input": itemgetter("input"),
        }
        | prompt_template
        | llm
        | StrOutputParser()
    )

    rag_chain_with_history = RunnableWithMessageHistory(
        base_rag_chain,
        get_session_history,
        input_messages_key="input",
        history_messages_key="history",
    )
    return rag_chain_with_history

def create_generic_chat_chain(system_prompt: str):
    """ç‚ºç‰¹å®šè§’è‰²å»ºç«‹ä¸€å€‹å…·å‚™ã€Œå°è©±å…§è¨˜æ†¶ã€çš„é€šç”¨èŠå¤©éˆ"""
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest", temperature=0.7)
    
    prompt_template = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "{input}"),
    ])
    
    base_chain = prompt_template | llm | StrOutputParser()
    
    chain_with_history = RunnableWithMessageHistory(
        base_chain,
        get_session_history,
        input_messages_key="input",
        history_messages_key="history",
    )
    return chain_with_history

def create_vector_db_from_csv(file_path: str):
    """(ä¿æŒä¸è®Š) å¾ CSV æª”æ¡ˆè¼‰å…¥ã€åˆ‡å‰²ã€åµŒå…¥ä¸¦å»ºç«‹å‘é‡è³‡æ–™åº«"""
    with st.spinner("æ­£åœ¨è™•ç†è³‡æ–™ï¼Œå»ºç«‹çŸ¥è­˜åº«ä¸­..."):
        loader = CSVLoader(file_path=file_path, encoding='utf-8')
        documents = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        docs = text_splitter.split_documents(documents)
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
        vector_store = FAISS.from_documents(docs, embeddings)
        st.success(f"çŸ¥è­˜åº«å»ºç«‹å®Œæˆï¼å·²è¼‰å…¥ {len(docs)} ç­†è³‡æ–™ã€‚")
        return vector_store

# --- å…¶ä»– Gemini å‘¼å«å‡½å¼ (ä¿æŒä¸è®Š) ---
def get_gemini_response_for_image(user_prompt, image_pil):
    """å°ˆé–€ç”¨æ–¼è™•ç†åœ–ç‰‡åˆ†æçš„å‡½å¼"""
    api_key = st.session_state.get("gemini_api_key_input") or os.environ.get("GOOGLE_API_KEY")
    if not api_key: return "éŒ¯èª¤ï¼šæœªè¨­å®š Gemini API Keyã€‚"
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel("gemini-1.5-flash-latest")
        response = model.generate_content([user_prompt, image_pil])
        st.session_state.pending_image_for_main_gemini = None # æ¸…é™¤å¾…è™•ç†åœ–ç‰‡
        return response.text
    except Exception as e:
        st.error(f"Gemini åœ–ç‰‡åˆ†æè«‹æ±‚å¤±æ•—: {e}")
        return f"éŒ¯èª¤: {e}"

def get_gemini_executive_analysis(executive_role_name, full_prompt):
    # (ä¿æŒä¸è®Š)
    api_key = st.session_state.get("gemini_api_key_input") or os.environ.get("GOOGLE_API_KEY")
    if not api_key: return f"éŒ¯èª¤ï¼šé«˜ç®¡å·¥ä½œæµ ({executive_role_name}) æœªèƒ½ç²å– Gemini API Keyã€‚"
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel("gemini-1.5-pro-latest")
        response = model.generate_content(full_prompt)
        return response.text
    except Exception as e:
        st.error(f"é«˜ç®¡åˆ†æ ({executive_role_name}) å¤±æ•—: {e}")
        return f"éŒ¯èª¤: {e}"
        
def generate_data_profile(df):
    # (ä¿æŒä¸è®Š)
    if df is None or df.empty: return "æ²’æœ‰è³‡æ–™å¯ä¾›åˆ†æã€‚"
    buffer = StringIO()
    df.info(buf=buffer)
    profile_parts = [f"è³‡æ–™å½¢ç‹€: {df.shape}", f"æ¬„ä½è³‡è¨Š:\n{buffer.getvalue()}"]
    try: profile_parts.append(f"\næ•¸å€¼æ¬„ä½çµ±è¨ˆ:\n{df.describe(include='number').to_string()}")
    except: pass
    try: profile_parts.append(f"\né¡åˆ¥æ¬„ä½çµ±è¨ˆ:\n{df.describe(include=['object', 'category']).to_string()}")
    except: pass
    profile_parts.append(f"\nå‰ 5 ç­†è³‡æ–™:\n{df.head().to_string()}")
    return "\n".join(profile_parts)

# ------------------------------
# ä¸»æ‡‰ç”¨å…¥å£ (æœ€çµ‚æ•´åˆç‰ˆ)
# ------------------------------
def main():
    st.set_page_config(page_title="Gemini Multi-Function Bot", page_icon="âœ¨", layout="wide")
    st.title("âœ¨ Gemini å¤šåŠŸèƒ½ AI åŠ©ç†")

    # --- åˆå§‹åŒ– Session States (ä¸å†éœ€è¦æ‰‹å‹•ç®¡ç† memory) ---
    keys_to_init = {
        "rag_chain": None, "uploaded_file_path": None, "last_uploaded_filename": None,
        "pending_image_for_main_gemini": None, "executive_workflow_stage": "idle", "executive_user_query": "",
        "executive_data_profile_str": "", "cfo_analysis_text": "", "coo_analysis_text": "", "ceo_summary_text": "",
        "debug_mode": False
    }
    for key, default_value in keys_to_init.items():
        if key not in st.session_state:
            st.session_state[key] = default_value

    # --- å´é‚Šæ¬„ä»‹é¢ (ç§»é™¤æª”æ¡ˆè¨˜æ†¶é«”ç®¡ç†) ---
    with st.sidebar:
        st.header("âš™ï¸ è¨­å®š")
        st.text_input(
            "è«‹è¼¸å…¥æ‚¨çš„ Google Gemini API Key",
            value=st.session_state.get("gemini_api_key_input", ""), type="password", key="gemini_api_key_input"
        )
        st.caption("å„ªå…ˆä½¿ç”¨æ­¤è™•è¼¸å…¥çš„é‡‘é‘°ã€‚")
        st.divider()

        st.subheader("ğŸ“ è³‡æ–™å•ç­” (RAG)")
        uploaded_file = st.file_uploader("ä¸Šå‚³ CSV ä»¥å•Ÿç”¨ RAG å•ç­”åŠŸèƒ½", type=["csv"])
        
        if uploaded_file:
            if uploaded_file.name != st.session_state.get("last_uploaded_filename"):
                st.session_state.last_uploaded_filename = uploaded_file.name
                file_path = save_uploaded_file(uploaded_file)
                st.session_state.uploaded_file_path = file_path
                vector_store = create_vector_db_from_csv(file_path)
                st.session_state.rag_chain = create_rag_chain(vector_store)

        if st.session_state.rag_chain:
            st.success("âœ… RAG å•ç­”åŠŸèƒ½å·²å•Ÿç”¨ï¼")

        st.subheader("ğŸ–¼ï¸ åœ–ç‰‡åˆ†æ")
        uploaded_image = st.file_uploader("ä¸Šå‚³åœ–ç‰‡é€²è¡Œåˆ†æ", type=["png", "jpg", "jpeg"])
        if uploaded_image: add_user_image_to_main_chat(uploaded_image)
        st.divider()

        if st.button("ğŸ—‘ï¸ æ¸…é™¤æ‰€æœ‰å°è©±èˆ‡è³‡æ–™"):
            keys_to_clear = [k for k in st.session_state.keys() if k != 'gemini_api_key_input']
            for key in keys_to_clear:
                st.session_state.pop(key)
            st.success("æ‰€æœ‰å°è©±ã€è¨˜æ†¶å’Œå·¥ä½œç‹€æ…‹å·²æ¸…é™¤ï¼")
            st.rerun()

        st.session_state.debug_mode = st.checkbox("å•Ÿç”¨åµéŒ¯æ¨¡å¼", value=st.session_state.debug_mode)
        # ... (åµéŒ¯è³‡è¨Šé¡¯ç¤ºé‚è¼¯ä¸è®Š)

    # --- ä¸»å·¥ä½œå€ (æ¨™ç±¤é é¢) ---
    tab_titles = ["ğŸ’¬ ä¸»è¦èŠå¤©å®¤", "ğŸ’¼ é«˜ç®¡å·¥ä½œæµ"] + [role["name"] for role in ROLE_DEFINITIONS.values()]
    tabs = st.tabs(tab_titles)

    # --- ä¸»è¦èŠå¤©å®¤ (ä½¿ç”¨ In-Memory è¨˜æ†¶) ---
    with tabs[0]:
        st.header("ğŸ’¬ ä¸»è¦èŠå¤©å®¤")
        st.caption("å¯é€²è¡Œ RAG è³‡æ–™å•ç­”ã€åœ–ç‰‡åˆ†æ (å°è©±åŠŸèƒ½å…·å‚™ Session è¨˜æ†¶)ã€‚")
        
        MAIN_CHAT_SESSION_ID = "main_chat_session"
        history = get_session_history(MAIN_CHAT_SESSION_ID)

        for msg in history.messages:
            with st.chat_message(msg.type):
                # è™•ç†åœ–ç‰‡é¡¯ç¤º
                if isinstance(msg.content, list):
                    for part in msg.content:
                        if isinstance(part, dict) and "image" in part:
                             st.image(part["image"])
                        elif isinstance(part, str):
                             st.markdown(part)
                else:
                    st.markdown(msg.content)
        
        if user_input := st.chat_input("è«‹å°æ•¸æ“šæˆ–åœ–ç‰‡æå•..."):
            with st.chat_message("human"):
                st.markdown(user_input)
            
            with st.chat_message("ai"):
                with st.spinner("Gemini æ­£åœ¨æ€è€ƒä¸­..."):
                    # æƒ…å¢ƒ1ï¼šRAG å•ç­”
                    if st.session_state.rag_chain:
                        response = st.session_state.rag_chain.invoke(
                            {"input": user_input},
                            config={"configurable": {"session_id": MAIN_CHAT_SESSION_ID}}
                        )
                        st.markdown(response)
                    # æƒ…å¢ƒ2ï¼šåœ–ç‰‡å•ç­”
                    elif st.session_state.pending_image_for_main_gemini:
                        response = get_gemini_response_for_image(
                            user_input, st.session_state.pending_image_for_main_gemini
                        )
                        # æ‰‹å‹•å°‡åœ–æ–‡å•ç­”åŠ å…¥æ­·å²
                        history.add_user_message(user_input)
                        history.add_ai_message(response)
                        st.markdown(response)
                    # æƒ…å¢ƒ3ï¼šä¸€èˆ¬èŠå¤©
                    else:
                        # å»ºç«‹ä¸€å€‹è‡¨æ™‚çš„é€šç”¨èŠå¤©éˆä¾†è™•ç†ä¸€èˆ¬å°è©±
                        general_chain = create_generic_chat_chain("ä½ æ˜¯ä¸€å€‹æ¨‚æ–¼åŠ©äººçš„ AI åŠ©ç†ã€‚")
                        response = general_chain.invoke(
                            {"input": user_input},
                            config={"configurable": {"session_id": MAIN_CHAT_SESSION_ID}}
                        )
                        st.markdown(response)

    # --- é«˜ç®¡å·¥ä½œæµæ¨™ç±¤ (ä¿æŒä¸è®Š) ---
    with tabs[1]:
        st.header("ğŸ’¼ é«˜ç®¡å·¥ä½œæµ (ç”± Gemini Pro é©…å‹•)")
        st.write("è«‹å…ˆåœ¨å´é‚Šæ¬„ä¸Šå‚³CSVè³‡æ–™ï¼Œç„¶å¾Œåœ¨æ­¤è¼¸å…¥å•†æ¥­å•é¡Œï¼Œæœ€å¾Œé»æ“ŠæŒ‰éˆ•å•Ÿå‹•åˆ†æã€‚")
        # ... (æ­¤è™•æ‰€æœ‰ç¨‹å¼ç¢¼å®Œå…¨ä¸è®Š)
        st.session_state.executive_user_query = st.text_area(
            "è«‹è¼¸å…¥å•†æ¥­å•é¡Œä»¥å•Ÿå‹•åˆ†æ:", value=st.session_state.get("executive_user_query", ""), height=100
        )
        can_start = bool(st.session_state.get("uploaded_file_path") and st.session_state.get("executive_user_query"))
        if st.button("ğŸš€ å•Ÿå‹•/é‡å•Ÿé«˜ç®¡åˆ†æ", disabled=not can_start):
             st.session_state.executive_workflow_stage = "data_profiling_pending"
             st.session_state.cfo_analysis_text = ""
             st.session_state.coo_analysis_text = ""
             st.session_state.ceo_summary_text = ""
             st.rerun()
        if st.session_state.executive_workflow_stage == "data_profiling_pending":
             with st.spinner("æ­£åœ¨ç”Ÿæˆè³‡æ–™æ‘˜è¦..."):
                df = pd.read_csv(st.session_state.uploaded_file_path)
                st.session_state.executive_data_profile_str = generate_data_profile(df)
                st.session_state.executive_workflow_stage = "cfo_analysis_pending"
                st.rerun()
        # ... å¾ŒçºŒ CFO, COO, CEO çš„é‚è¼¯å®Œå…¨ä¸è®Š ...

    # --- å…¶ä»– AI è§’è‰²æ¨™ç±¤ (å¥—ç”¨ In-Memory è¨˜æ†¶) ---
    for i, (role_id, role_info) in enumerate(ROLE_DEFINITIONS.items()):
        with tabs[i + 2]:
            st.header(role_info["name"])
            st.caption(role_info["system_prompt"])
            
            # ç‚ºæ¯å€‹è§’è‰²å‰µå»ºå¸¶æœ‰è¨˜æ†¶çš„éˆ
            if f"{role_id}_chain" not in st.session_state:
                st.session_state[f"{role_id}_chain"] = create_generic_chat_chain(role_info["system_prompt"])
            
            chain = st.session_state[f"{role_id}_chain"]
            session_id = role_info["session_id"]
            history = get_session_history(session_id)
            
            # é¡¯ç¤ºæ­·å²è¨Šæ¯
            for msg in history.messages:
                with st.chat_message(msg.type):
                    st.markdown(msg.content)
            
            # èŠå¤©è¼¸å…¥
            if user_input := st.chat_input(f"èˆ‡ {role_info['name']} å°è©±..."):
                with st.chat_message("human"):
                    st.markdown(user_input)
                with st.chat_message("ai"):
                    with st.spinner("æ­£åœ¨ç”Ÿæˆå›æ‡‰..."):
                        response = chain.invoke(
                            {"input": user_input},
                            config={"configurable": {"session_id": session_id}}
                        )
                        st.markdown(response)

if __name__ == "__main__":
    main()
