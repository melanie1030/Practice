import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import json
import traceback
import re
import os
import dotenv
from io import StringIO
from PIL import Image
import time
import matplotlib.font_manager as fm
import matplotlib
import numpy as np

# --- åŸºç¤ API å’Œè³‡æ–™è™•ç†å¥—ä»¶ ---
import google.generativeai as genai
from openai import OpenAI
import faiss

# --- åªåœ¨ RAG å»ºç«‹/æª¢ç´¢æ™‚ä½¿ç”¨çš„ LangChain å…ƒä»¶ ---
from langchain_openai import OpenAIEmbeddings
from langchain_community.document_loaders import CSVLoader
from langchain_community.vectorstores import FAISS as LangChainFAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.runnables import RunnableLambda

# --- åˆå§‹åŒ–èˆ‡å¸¸æ•¸å®šç¾© ---
dotenv.load_dotenv()
UPLOAD_DIR = "uploaded_files"
if not os.path.exists(UPLOAD_DIR): os.makedirs(UPLOAD_DIR)

ROLE_DEFINITIONS = {
    "summarizer": { "name": "ğŸ“ æ‘˜è¦å°ˆå®¶", "system_prompt": "ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ‘˜è¦å°ˆå®¶...", "session_id": "summarizer_chat" },
    "creative_writer": { "name": "âœï¸ å‰µæ„ä½œå®¶", "system_prompt": "ä½ æ˜¯ä¸€ä½å……æ»¿æƒ³åƒåŠ›çš„å‰µæ„ä½œå®¶...", "session_id": "creative_writer_chat" }
}

# --- åŸºç¤è¼”åŠ©å‡½æ•¸ ---
def save_uploaded_file(uploaded_file):
    # ... (æ­¤å‡½å¼ä¿æŒä¸è®Š)
    if not os.path.exists(UPLOAD_DIR): os.makedirs(UPLOAD_DIR)
    file_path = os.path.join(UPLOAD_DIR, uploaded_file.name)
    with open(file_path, "wb") as f: f.write(uploaded_file.getbuffer())
    return file_path

def add_user_image_to_main_chat(uploaded_file):
    # ... (æ­¤å‡½å¼ä¿æŒä¸è®Š)
    try:
        file_path = save_uploaded_file(uploaded_file)
        st.session_state.pending_image_for_main_gemini = Image.open(file_path)
        st.image(st.session_state.pending_image_for_main_gemini, caption="åœ–ç‰‡å·²ä¸Šå‚³...", use_container_width=True)
    except Exception as e: st.error(f"è™•ç†ä¸Šå‚³åœ–ç‰‡æ™‚å‡ºéŒ¯: {e}")

# --- æ··åˆæ¶æ§‹ RAG æ ¸å¿ƒå‡½å¼ ---
@st.cache_resource
def create_lc_retriever(file_path: str, openai_api_key: str):
    """(ä½¿ç”¨ LangChain) å¾ CSV å»ºç«‹ä¸€å€‹åƒ…ç”¨æ–¼ã€Œæª¢ç´¢ã€çš„å·¥å…·"""
    with st.status("æ­£åœ¨ä½¿ç”¨ LangChain å»ºç«‹çŸ¥è­˜åº«...", expanded=True) as status:
        try:
            status.update(label="æ­¥é©Ÿ 1/3ï¼šè¼‰å…¥èˆ‡åˆ‡å‰²æ–‡ä»¶...")
            loader = CSVLoader(file_path=file_path, encoding='utf-8')
            documents = loader.load()
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
            docs = text_splitter.split_documents(documents)
            status.update(label=f"æ­¥é©Ÿ 1/3 å®Œæˆï¼å·²åˆ‡å‰²æˆ {len(docs)} å€‹å€å¡Šã€‚")

            status.update(label="æ­¥é©Ÿ 2/3ï¼šå‘¼å« OpenAI API ç”Ÿæˆå‘é‡åµŒå…¥...")
            embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
            vector_store = LangChainFAISS.from_documents(docs, embeddings)
            status.update(label="æ­¥é©Ÿ 2/3 å®Œæˆï¼å‘é‡åµŒå…¥å·²ç”Ÿæˆã€‚")

            status.update(label="æ­¥é©Ÿ 3/3ï¼šæª¢ç´¢å™¨æº–å‚™å®Œæˆï¼", state="complete", expanded=False)
            # æˆ‘å€‘åªè¿”å›ä¸€å€‹å¯ä»¥æ¥æ”¶å•é¡Œä¸¦è¿”å›æ–‡ä»¶çš„æª¢ç´¢å™¨
            return vector_store.as_retriever(search_kwargs={'k': 5})
        except Exception as e:
            st.error(f"å»ºç«‹çŸ¥è­˜åº«éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
            status.update(label="å»ºç«‹å¤±æ•—", state="error")
            return None

# --- Gemini API ç›¸é—œå‡½å¼ (æ‰‹å‹•æ§åˆ¶) ---
def get_gemini_client(api_key):
    genai.configure(api_key=api_key)
    return genai.GenerativeModel("gemini-1.5-flash-latest")

def get_gemini_response_with_history(client, history, user_prompt):
    # å°‡æˆ‘å€‘çš„ history æ ¼å¼è½‰æ›ç‚º gemini çš„æ ¼å¼
    gemini_history = []
    for msg in history:
        # Gemini API æœŸæœ›çš„ role æ˜¯ 'user' å’Œ 'model'
        role = "user" if msg["role"] == "human" else "model"
        gemini_history.append({"role": role, "parts": [msg["content"]]})
    
    chat = client.start_chat(history=gemini_history)
    response = chat.send_message(user_prompt)
    return response.text

# --- ä¸»æ‡‰ç”¨å…¥å£ ---
def main():
    st.set_page_config(page_title="Gemini Multi-Function Bot", page_icon="âœ¨", layout="wide")
    st.title("âœ¨ Gemini å¤šåŠŸèƒ½ AI åŠ©ç† (æ··åˆæ¶æ§‹ç‰ˆ)")

    # --- åˆå§‹åŒ– Session States ---
    if "retriever_chain" not in st.session_state: st.session_state.retriever_chain = None
    if "uploaded_file_path" not in st.session_state: st.session_state.uploaded_file_path = None
    if "last_uploaded_filename" not in st.session_state: st.session_state.last_uploaded_filename = None
    if "chat_histories" not in st.session_state: st.session_state.chat_histories = {}
    # ... å…¶ä»–éœ€è¦çš„ session state ...

    # --- å´é‚Šæ¬„ä»‹é¢ ---
    with st.sidebar:
        st.header("âš™ï¸ API é‡‘é‘°è¨­å®š")
        st.text_input("è«‹è¼¸å…¥æ‚¨çš„ Google Gemini API Key", type="password", key="gemini_api_key_input")
        st.text_input("è«‹è¼¸å…¥æ‚¨çš„ OpenAI API Key", type="password", key="openai_api_key_input")
        st.caption("Gemini ç”¨æ–¼èŠå¤©ï¼ŒOpenAI ç”¨æ–¼ RAG è³‡æ–™åµŒå…¥ã€‚")
        st.divider()

        st.subheader("ğŸ“ è³‡æ–™å•ç­” (RAG)")
        uploaded_file = st.file_uploader("ä¸Šå‚³ CSV ä»¥å•Ÿç”¨ RAG å•ç­”åŠŸèƒ½", type=["csv"])
        
        if uploaded_file:
            if uploaded_file.name != st.session_state.get("last_uploaded_filename"):
                openai_api_key = st.session_state.get("openai_api_key_input") or os.environ.get("OPENAI_API_KEY")
                if not openai_api_key:
                    st.error("è«‹åœ¨å´é‚Šæ¬„è¨­å®šæ‚¨çš„ OpenAI API Keyï¼")
                else:
                    st.session_state.last_uploaded_filename = uploaded_file.name
                    file_path = save_uploaded_file(uploaded_file)
                    st.session_state.uploaded_file_path = file_path
                    # å»ºç«‹ä¸¦å„²å­˜ LangChain æª¢ç´¢å™¨
                    st.session_state.retriever_chain = create_lc_retriever(file_path, openai_api_key)
        
        if st.session_state.retriever_chain: st.success("âœ… RAG æª¢ç´¢åŠŸèƒ½å·²å•Ÿç”¨ï¼")
        
        st.divider()
        if st.button("ğŸ—‘ï¸ æ¸…é™¤æ‰€æœ‰å°è©±èˆ‡è³‡æ–™"):
            st.session_state.clear()
            st.cache_resource.clear()
            st.success("æ‰€æœ‰å°è©±ã€Session è¨˜æ†¶å’Œå¿«å–å·²æ¸…é™¤ï¼")
            st.rerun()

    # --- ä¸»å·¥ä½œå€ (æ¨™ç±¤é é¢) ---
    tab_titles = ["ğŸ’¬ ä¸»è¦èŠå¤©å®¤"] + [role["name"] for role in ROLE_DEFINITIONS.values()]
    tabs = st.tabs(tab_titles)

    # --- API Key æª¢æŸ¥ ---
    gemini_api_key = st.session_state.get("gemini_api_key_input") or os.environ.get("GOOGLE_API_KEY")
    if not gemini_api_key:
        st.warning("è«‹åœ¨å´é‚Šæ¬„è¼¸å…¥æ‚¨çš„ Google Gemini API Key ä»¥å•Ÿå‹•èŠå¤©åŠŸèƒ½ã€‚")
        st.stop()
    gemini_client = get_gemini_client(gemini_api_key)

    # --- ä¸»è¦èŠå¤©å®¤ (æ··åˆæ¨¡å¼) ---
    with tabs[0]:
        st.header("ğŸ’¬ ä¸»è¦èŠå¤©å®¤")
        session_id = "main_chat"
        if session_id not in st.session_state.chat_histories: st.session_state.chat_histories[session_id] = []
        
        # é¡¯ç¤ºæ­·å²è¨Šæ¯
        for msg in st.session_state.chat_histories[session_id]:
            with st.chat_message(msg["role"]): st.markdown(msg["content"])
        
        if user_input := st.chat_input("è«‹å°æ•¸æ“šæå•æˆ–é–‹å§‹å°è©±..."):
            st.session_state.chat_histories[session_id].append({"role": "human", "content": user_input})
            with st.chat_message("human"): st.markdown(user_input)
            
            with st.chat_message("ai"):
                with st.spinner("æ­£åœ¨æ€è€ƒä¸­..."):
                    response = ""
                    # --- æ··åˆé‚è¼¯é–‹å§‹ ---
                    # å¦‚æœ RAG æª¢ç´¢å™¨å·²å•Ÿç”¨ï¼Œå‰‡åŸ·è¡Œ RAG æµç¨‹
                    if st.session_state.retriever_chain:
                        # 1. ä½¿ç”¨ LangChain æª¢ç´¢ä¸Šä¸‹æ–‡
                        retrieved_docs = st.session_state.retriever_chain.invoke(user_input)
                        context = "\n---\n".join([doc.page_content for doc in retrieved_docs])
                        
                        # 2. æ‰‹å‹•çµ„åˆ Prompt
                        prompt = f"""
                        è«‹æ ¹æ“šä»¥ä¸‹æä¾›çš„ã€Œä¸Šä¸‹æ–‡ã€ä¾†å›ç­”å•é¡Œã€‚è«‹åªä½¿ç”¨ä¸Šä¸‹æ–‡ä¸­çš„è³‡è¨Šã€‚

                        [ä¸Šä¸‹æ–‡]:
                        {context}

                        [å•é¡Œ]:
                        {user_input}

                        [å›ç­”]:
                        """
                        # 3. æ‰‹å‹•å‘¼å« Gemini API
                        response = gemini_client.generate_content(prompt).text
                    # å¦å‰‡ï¼ŒåŸ·è¡Œä¸€èˆ¬èŠå¤©
                    else:
                        history = st.session_state.chat_histories[session_id][:-1]
                        response = get_gemini_response_with_history(gemini_client, history, user_input)
                    
                    st.markdown(response)
                    st.session_state.chat_histories[session_id].append({"role": "ai", "content": response})

    # --- å…¶ä»– AI è§’è‰²æ¨™ç±¤ (æ‰‹å‹•æ¨¡å¼) ---
    for i, (role_id, role_info) in enumerate(ROLE_DEFINITIONS.items()):
        with tabs[i + 1]: # æ³¨æ„ç´¢å¼•å¾ 1 é–‹å§‹
            st.header(role_info["name"])
            st.caption(role_info["system_prompt"])
            session_id = role_info["session_id"]
            if session_id not in st.session_state.chat_histories: st.session_state.chat_histories[session_id] = []

            # é¡¯ç¤ºæ­·å²è¨Šæ¯
            for msg in st.session_state.chat_histories[session_id]:
                with st.chat_message(msg["role"]): st.markdown(msg["content"])

            if user_input := st.chat_input(f"èˆ‡ {role_info['name']} å°è©±..."):
                st.session_state.chat_histories[session_id].append({"role": "human", "content": user_input})
                with st.chat_message("human"): st.markdown(user_input)
                
                with st.chat_message("ai"):
                    with st.spinner("æ­£åœ¨ç”Ÿæˆå›æ‡‰..."):
                        # ç‚ºè§’è‰²å°ˆå®¶åŠ ä¸Š system_prompt
                        client_with_prompt = get_gemini_client(gemini_api_key)
                        client_with_prompt.system_instruction = role_info["system_prompt"]
                        
                        history = st.session_state.chat_histories[session_id][:-1]
                        response = get_gemini_response_with_history(client_with_prompt, history, user_input)
                        
                        st.markdown(response)
                        st.session_state.chat_histories[session_id].append({"role": "ai", "content": response})

if __name__ == "__main__":
    main()
