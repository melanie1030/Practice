import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import json
import traceback
import re
import os
import dotenv
from io import StringIO
from PIL import Image
import time
import matplotlib.font_manager as fm
import matplotlib
import numpy as np

# --- åŸºç¤ API å’Œè³‡æ–™è™•ç†å¥—ä»¶ ---
import google.generativeai as genai
from openai import OpenAI
import faiss

# --- åªåœ¨ RAG å»ºç«‹/æª¢ç´¢æ™‚ä½¿ç”¨çš„ LangChain å…ƒä»¶ ---
from langchain_openai import OpenAIEmbeddings
from langchain_community.document_loaders import CSVLoader
from langchain_community.vectorstores import FAISS as LangChainFAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter

# --- åˆå§‹åŒ–èˆ‡å¸¸æ•¸å®šç¾© ---
dotenv.load_dotenv()
UPLOAD_DIR = "uploaded_files"
if not os.path.exists(UPLOAD_DIR): os.makedirs(UPLOAD_DIR)

ROLE_DEFINITIONS = {
    "summarizer": { "name": "ğŸ“ æ‘˜è¦å°ˆå®¶", "system_prompt": "ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ‘˜è¦å°ˆå®¶ã€‚ä½ çš„ä»»å‹™æ˜¯å°‡æä¾›çš„ä»»ä½•æ–‡æœ¬æˆ–å°è©±ï¼Œæ¿ƒç¸®æˆæ¸…æ™°ã€ç°¡æ½”çš„ç¹é«”ä¸­æ–‡æ‘˜è¦ã€‚å°ˆæ³¨æ–¼è¦é»å’Œé—œéµçµè«–ã€‚", "session_id": "summarizer_chat" },
    "creative_writer": { "name": "âœï¸ å‰µæ„ä½œå®¶", "system_prompt": "ä½ æ˜¯ä¸€ä½å……æ»¿æƒ³åƒåŠ›çš„å‰µæ„ä½œå®¶ã€‚ä½ çš„ä»»å‹™æ˜¯å¹«åŠ©ä½¿ç”¨è€…å®Œæˆå‰µæ„å¯«ä½œï¼Œä¾‹å¦‚å¯«æ•…äº‹ã€è©©æ­Œã€åŠ‡æœ¬æˆ–è…¦åŠ›æ¿€ç›ªï¼Œå…¨éƒ¨ä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚", "session_id": "creative_writer_chat" }
}

# --- åŸºç¤è¼”åŠ©å‡½æ•¸ ---
def save_uploaded_file(uploaded_file):
    if not os.path.exists(UPLOAD_DIR): os.makedirs(UPLOAD_DIR)
    file_path = os.path.join(UPLOAD_DIR, uploaded_file.name)
    with open(file_path, "wb") as f: f.write(uploaded_file.getbuffer())
    return file_path

def add_user_image_to_main_chat(uploaded_file):
    try:
        file_path = save_uploaded_file(uploaded_file)
        st.session_state.pending_image_for_main_gemini = Image.open(file_path)
        st.image(st.session_state.pending_image_for_main_gemini, caption="åœ–ç‰‡å·²ä¸Šå‚³ï¼Œå°‡éš¨ä¸‹ä¸€æ¢æ–‡å­—è¨Šæ¯ç™¼é€ã€‚", use_container_width=True)
    except Exception as e: st.error(f"è™•ç†ä¸Šå‚³åœ–ç‰‡æ™‚å‡ºéŒ¯: {e}")

# --- æ··åˆæ¶æ§‹ RAG æ ¸å¿ƒå‡½å¼ ---
@st.cache_resource
def create_lc_retriever(file_path: str, openai_api_key: str):
    with st.status("æ­£åœ¨ä½¿ç”¨ LangChain å»ºç«‹çŸ¥è­˜åº«...", expanded=True) as status:
        try:
            status.update(label="æ­¥é©Ÿ 1/3ï¼šè¼‰å…¥èˆ‡åˆ‡å‰²æ–‡ä»¶...")
            loader = CSVLoader(file_path=file_path, encoding='utf-8')
            documents = loader.load()
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
            docs = text_splitter.split_documents(documents)
            status.update(label=f"æ­¥é©Ÿ 1/3 å®Œæˆï¼å·²åˆ‡å‰²æˆ {len(docs)} å€‹å€å¡Šã€‚")

            status.update(label="æ­¥é©Ÿ 2/3ï¼šå‘¼å« OpenAI API ç”Ÿæˆå‘é‡åµŒå…¥...")
            embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
            vector_store = LangChainFAISS.from_documents(docs, embeddings)
            status.update(label="æ­¥é©Ÿ 2/3 å®Œæˆï¼å‘é‡åµŒå…¥å·²ç”Ÿæˆã€‚")

            status.update(label="æ­¥é©Ÿ 3/3ï¼šæª¢ç´¢å™¨æº–å‚™å®Œæˆï¼", state="complete", expanded=False)
            return vector_store.as_retriever(search_kwargs={'k': 5})
        except Exception as e:
            st.error(f"å»ºç«‹çŸ¥è­˜åº«éç¨‹ä¸­ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}")
            status.update(label="å»ºç«‹å¤±æ•—", state="error")
            return None

# --- Gemini API ç›¸é—œå‡½å¼ (æ‰‹å‹•æ§åˆ¶) ---
def get_gemini_client(api_key):
    genai.configure(api_key=api_key)
    return genai.GenerativeModel("gemini-1.5-flash-latest")

def get_gemini_response_with_history(client, history, user_prompt):
    gemini_history = []
    for msg in history:
        role = "user" if msg["role"] == "human" else "model"
        # Gemini API expects a list of parts
        gemini_history.append({"role": role, "parts": [msg["content"]]})
    
    chat = client.start_chat(history=gemini_history)
    response = chat.send_message(user_prompt)
    return response.text

def get_gemini_response_for_image(api_key, user_prompt, image_pil):
    if not api_key: return "éŒ¯èª¤ï¼šæœªè¨­å®š Gemini API Keyã€‚"
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel("gemini-1.5-flash-latest")
        response = model.generate_content([user_prompt, image_pil])
        st.session_state.pending_image_for_main_gemini = None
        return response.text
    except Exception as e:
        st.error(f"Gemini åœ–ç‰‡åˆ†æè«‹æ±‚å¤±æ•—: {e}")
        return f"éŒ¯èª¤: {e}"

def get_gemini_executive_analysis(api_key, executive_role_name, full_prompt):
    if not api_key: return f"éŒ¯èª¤ï¼šé«˜ç®¡å·¥ä½œæµ ({executive_role_name}) æœªèƒ½ç²å– Gemini API Keyã€‚"
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel("gemini-1.5-pro-latest")
        response = model.generate_content(full_prompt)
        return response.text
    except Exception as e:
        st.error(f"é«˜ç®¡åˆ†æ ({executive_role_name}) å¤±æ•—: {e}")
        return f"éŒ¯èª¤: {e}"

def generate_data_profile(df):
    if df is None or df.empty: return "æ²’æœ‰è³‡æ–™å¯ä¾›åˆ†æã€‚"
    buffer = StringIO()
    df.info(buf=buffer)
    profile_parts = [f"è³‡æ–™å½¢ç‹€: {df.shape}", f"æ¬„ä½è³‡è¨Š:\n{buffer.getvalue()}"]
    try: profile_parts.append(f"\næ•¸å€¼æ¬„ä½çµ±è¨ˆ:\n{df.describe(include='number').to_string()}")
    except: pass
    try: profile_parts.append(f"\né¡åˆ¥æ¬„ä½çµ±è¨ˆ:\n{df.describe(include=['object', 'category']).to_string()}")
    except: pass
    profile_parts.append(f"\nå‰ 5 ç­†è³‡æ–™:\n{df.head().to_string()}")
    return "\n".join(profile_parts)

# ------------------------------
# ä¸»æ‡‰ç”¨å…¥å£
# ------------------------------
def main():
    st.set_page_config(page_title="Gemini Multi-Function Bot", page_icon="âœ¨", layout="wide")
    st.title("âœ¨ Gemini å¤šåŠŸèƒ½ AI åŠ©ç† (æ··åˆæ¶æ§‹ç‰ˆ)")

    # --- åˆå§‹åŒ– Session States ---
    keys_to_init = {
        "retriever_chain": None, "uploaded_file_path": None, "last_uploaded_filename": None,
        "pending_image_for_main_gemini": None, "chat_histories": {},
        "executive_workflow_stage": "idle", "executive_user_query": "",
        "executive_data_profile_str": "", "executive_rag_context": "", "cfo_analysis_text": "",
        "coo_analysis_text": "", "ceo_summary_text": ""
    }
    for key, default_value in keys_to_init.items():
        if key not in st.session_state:
            st.session_state[key] = default_value

    # --- å´é‚Šæ¬„ä»‹é¢ ---
    with st.sidebar:
        st.header("âš™ï¸ API é‡‘é‘°è¨­å®š")
        st.text_input("è«‹è¼¸å…¥æ‚¨çš„ Google Gemini API Key", type="password", key="gemini_api_key_input")
        st.text_input("è«‹è¼¸å…¥æ‚¨çš„ OpenAI API Key", type="password", key="openai_api_key_input")
        st.caption("Gemini ç”¨æ–¼èŠå¤©ï¼ŒOpenAI ç”¨æ–¼ RAG è³‡æ–™åµŒå…¥ã€‚")
        st.divider()

        st.subheader("ğŸ“ è³‡æ–™å•ç­” (RAG)")
        uploaded_file = st.file_uploader("ä¸Šå‚³ CSV ä»¥å•Ÿç”¨ RAG å•ç­”åŠŸèƒ½", type=["csv"])
        
        if uploaded_file:
            if uploaded_file.name != st.session_state.get("last_uploaded_filename"):
                openai_api_key = st.session_state.get("openai_api_key_input") or os.environ.get("OPENAI_API_KEY")
                if not openai_api_key:
                    st.error("è«‹åœ¨å´é‚Šæ¬„è¨­å®šæ‚¨çš„ OpenAI API Keyï¼")
                else:
                    st.session_state.last_uploaded_filename = uploaded_file.name
                    file_path = save_uploaded_file(uploaded_file)
                    st.session_state.uploaded_file_path = file_path
                    st.session_state.retriever_chain = create_lc_retriever(file_path, openai_api_key)
        
        if st.session_state.retriever_chain: st.success("âœ… RAG æª¢ç´¢åŠŸèƒ½å·²å•Ÿç”¨ï¼")

        st.subheader("ğŸ–¼ï¸ åœ–ç‰‡åˆ†æ")
        uploaded_image = st.file_uploader("ä¸Šå‚³åœ–ç‰‡é€²è¡Œåˆ†æ", type=["png", "jpg", "jpeg"])
        if uploaded_image: add_user_image_to_main_chat(uploaded_image)
        
        st.divider()
        if st.button("ğŸ—‘ï¸ æ¸…é™¤æ‰€æœ‰å°è©±èˆ‡è³‡æ–™"):
            # ä¿ç•™ API Key è¼¸å…¥ï¼Œæ¸…é™¤å…¶ä»–æ‰€æœ‰ session state
            api_keys = {
                'gemini_api_key_input': st.session_state.get('gemini_api_key_input'),
                'openai_api_key_input': st.session_state.get('openai_api_key_input')
            }
            st.session_state.clear()
            # å°‡ API Key åŠ å›å»
            for key, value in api_keys.items():
                if value: st.session_state[key] = value

            st.cache_resource.clear()
            st.success("æ‰€æœ‰å°è©±ã€Session è¨˜æ†¶å’Œå¿«å–å·²æ¸…é™¤ï¼")
            st.rerun()

    # --- ä¸»å·¥ä½œå€ (æ¨™ç±¤é é¢) ---
    tab_titles = ["ğŸ’¬ ä¸»è¦èŠå¤©å®¤", "ğŸ’¼ é«˜ç®¡å·¥ä½œæµ"] + [role["name"] for role in ROLE_DEFINITIONS.values()]
    tabs = st.tabs(tab_titles)

    # --- API Key æª¢æŸ¥ ---
    gemini_api_key = st.session_state.get("gemini_api_key_input") or os.environ.get("GOOGLE_API_KEY")
    if not gemini_api_key:
        st.warning("è«‹åœ¨å´é‚Šæ¬„è¼¸å…¥æ‚¨çš„ Google Gemini API Key ä»¥å•Ÿå‹•èŠå¤©åŠŸèƒ½ã€‚")
        st.stop()
    gemini_client = get_gemini_client(gemini_api_key)

    # --- ä¸»è¦èŠå¤©å®¤ ---
    with tabs[0]:
        st.header("ğŸ’¬ ä¸»è¦èŠå¤©å®¤")
        st.caption("å¯é€²è¡Œ RAG è³‡æ–™å•ç­”ã€åœ–ç‰‡åˆ†æã€æˆ–ä¸€èˆ¬å°è©±ã€‚")
        session_id = "main_chat"
        if session_id not in st.session_state.chat_histories: st.session_state.chat_histories[session_id] = []
        
        for msg in st.session_state.chat_histories[session_id]:
            with st.chat_message(msg["role"]): st.markdown(msg["content"])
        
        if user_input := st.chat_input("è«‹å°æ•¸æ“šæˆ–åœ–ç‰‡æå•..."):
            st.session_state.chat_histories[session_id].append({"role": "human", "content": user_input})
            with st.chat_message("human"): st.markdown(user_input)
            
            with st.chat_message("ai"):
                with st.spinner("æ­£åœ¨æ€è€ƒä¸­..."):
                    response = ""
                    # æƒ…å¢ƒ1ï¼šRAG å•ç­”
                    if st.session_state.retriever_chain:
                        retrieved_docs = st.session_state.retriever_chain.invoke(user_input)
                        context = "\n---\n".join([doc.page_content for doc in retrieved_docs])
                        prompt = f"è«‹æ ¹æ“šä¸Šä¸‹æ–‡å›ç­”å•é¡Œã€‚\n[ä¸Šä¸‹æ–‡]:\n{context}\n\n[å•é¡Œ]:\n{user_input}\n\n[å›ç­”]:"
                        response = gemini_client.generate_content(prompt).text
                    # æƒ…å¢ƒ2ï¼šåœ–ç‰‡å•ç­”
                    elif st.session_state.pending_image_for_main_gemini:
                        response = get_gemini_response_for_image(gemini_api_key, user_input, st.session_state.pending_image_for_main_gemini)
                    # æƒ…å¢ƒ3ï¼šä¸€èˆ¬èŠå¤©
                    else:
                        history = st.session_state.chat_histories[session_id][:-1]
                        response = get_gemini_response_with_history(gemini_client, history, user_input)
                    
                    st.markdown(response)
                    st.session_state.chat_histories[session_id].append({"role": "ai", "content": response})

    # --- é«˜ç®¡å·¥ä½œæµ (å·²æ¢å¾©ä¸¦æ•´åˆ RAG) ---
    with tabs[1]:
        st.header("ğŸ’¼ é«˜ç®¡å·¥ä½œæµ (ç”± Gemini Pro + RAG é©…å‹•)")
        st.write("è«‹å…ˆåœ¨å´é‚Šæ¬„ä¸Šå‚³CSVè³‡æ–™ï¼ŒRAG åŠŸèƒ½å°‡è‡ªå‹•ç‚ºé«˜ç®¡æä¾›æ·±å…¥çš„æ•¸æ“šæ´å¯Ÿã€‚")
        
        st.session_state.executive_user_query = st.text_area(
            "è«‹è¼¸å…¥å•†æ¥­å•é¡Œä»¥å•Ÿå‹•åˆ†æ:", value=st.session_state.get("executive_user_query", ""), height=100
        )
        can_start = bool(st.session_state.get("retriever_chain") and st.session_state.get("executive_user_query"))
        
        if st.button("ğŸš€ å•Ÿå‹•/é‡å•Ÿé«˜ç®¡åˆ†æ", disabled=not can_start, key="exec_flow_button"):
             st.session_state.executive_workflow_stage = "cfo_analysis_pending"
             st.session_state.executive_rag_context = ""
             st.session_state.cfo_analysis_text = ""
             st.session_state.coo_analysis_text = ""
             st.session_state.ceo_summary_text = ""
             st.rerun()

        if st.session_state.executive_workflow_stage == "cfo_analysis_pending":
            with st.spinner("RAG æ­£åœ¨æª¢ç´¢ç›¸é—œè³‡æ–™...CFO æ­£åœ¨åˆ†æ..."):
                df = pd.read_csv(st.session_state.uploaded_file_path)
                st.session_state.executive_data_profile_str = generate_data_profile(df)
                
                retriever = st.session_state.retriever_chain
                query = st.session_state.executive_user_query
                retrieved_docs = retriever.invoke(query)
                rag_context = "\n---\n".join([doc.page_content for doc in retrieved_docs])
                st.session_state.executive_rag_context = rag_context

                cfo_prompt = f"ä½œç‚ºè²¡å‹™é•·(CFO)ï¼Œè«‹åŸºæ–¼ä½ çš„å°ˆæ¥­çŸ¥è­˜ï¼Œä¸¦åš´æ ¼åƒè€ƒä»¥ä¸‹æä¾›çš„ã€Œçµ±è¨ˆæ‘˜è¦ã€å’Œã€ŒRAG æª¢ç´¢å‡ºçš„ç›¸é—œæ•¸æ“šã€ï¼Œç‚ºå•†æ¥­å•é¡Œæä¾›è²¡å‹™è§’åº¦çš„ç°¡æ½”åˆ†æã€‚\n\n[å•†æ¥­å•é¡Œ]:\n{query}\n\n[çµ±è¨ˆæ‘˜è¦]:\n{st.session_state.executive_data_profile_str}\n\n[RAG æª¢ç´¢å‡ºçš„ç›¸é—œæ•¸æ“š]:\n{rag_context}"
                response = get_gemini_executive_analysis(gemini_api_key, "CFO", cfo_prompt)
                st.session_state.cfo_analysis_text = response
                st.session_state.executive_workflow_stage = "coo_analysis_pending"
                st.rerun()

        if st.session_state.get('executive_data_profile_str'):
            with st.expander("æŸ¥çœ‹çµ±è¨ˆæ‘˜è¦"):
                st.text(st.session_state.executive_data_profile_str)
        if st.session_state.get('executive_rag_context'):
            with st.expander("æŸ¥çœ‹ RAG æª¢ç´¢å‡ºçš„ç›¸é—œè³‡æ–™"):
                st.markdown(st.session_state.executive_rag_context)

        if st.session_state.cfo_analysis_text:
            st.subheader("ğŸ“Š è²¡å‹™é•· (CFO) åˆ†æ")
            st.markdown(st.session_state.cfo_analysis_text)
        
        if st.session_state.executive_workflow_stage == "coo_analysis_pending":
            with st.spinner("COO æ­£åœ¨åˆ†æ..."):
                coo_prompt = f"ä½œç‚ºç‡Ÿé‹é•·(COO)ï¼Œè«‹åŸºæ–¼å•†æ¥­å•é¡Œã€çµ±è¨ˆæ‘˜è¦ã€RAG æª¢ç´¢æ•¸æ“šä»¥åŠ CFO çš„åˆ†æï¼Œæä¾›ç‡Ÿé‹å±¤é¢çš„ç­–ç•¥èˆ‡æ½›åœ¨é¢¨éšªã€‚\n\n[å•†æ¥­å•é¡Œ]:\n{st.session_state.executive_user_query}\n\n[CFO çš„è²¡å‹™åˆ†æ]:\n{st.session_state.cfo_analysis_text}\n\n[RAG æª¢ç´¢å‡ºçš„ç›¸é—œæ•¸æ“š]:\n{st.session_state.executive_rag_context}\n\n[çµ±è¨ˆæ‘˜è¦]:\n{st.session_state.executive_data_profile_str}"
                response = get_gemini_executive_analysis(gemini_api_key, "COO", coo_prompt)
                st.session_state.coo_analysis_text = response
                st.session_state.executive_workflow_stage = "ceo_summary_pending"
                st.rerun()

        if st.session_state.coo_analysis_text:
            st.subheader("ğŸ­ ç‡Ÿé‹é•· (COO) åˆ†æ")
            st.markdown(st.session_state.coo_analysis_text)

        if st.session_state.executive_workflow_stage == "ceo_summary_pending":
            with st.spinner("CEO æ­£åœ¨é€²è¡Œæœ€çµ‚ç¸½çµ..."):
                ceo_prompt = f"ä½œç‚ºåŸ·è¡Œé•·(CEO)ï¼Œè«‹æ•´åˆæ‰€æœ‰è³‡è¨Šï¼Œæä¾›ä¸€å€‹é«˜å±¤æ¬¡çš„ã€å¯åŸ·è¡Œçš„æ±ºç­–ç¸½çµèˆ‡æ˜ç¢ºçš„è¡Œå‹•å»ºè­°ã€‚\n\n[å•†æ¥­å•é¡Œ]:\n{st.session_state.executive_user_query}\n\n[CFO çš„è²¡å‹™åˆ†æ]:\n{st.session_state.cfo_analysis_text}\n\n[COO çš„ç‡Ÿé‹åˆ†æ]:\n{st.session_state.coo_analysis_text}\n\n[RAG æª¢ç´¢å‡ºçš„ç›¸é—œæ•¸æ“š]:\n{st.session_state.executive_rag_context}"
                response = get_gemini_executive_analysis(gemini_api_key, "CEO", ceo_prompt)
                st.session_state.ceo_summary_text = response
                st.session_state.executive_workflow_stage = "completed"
                st.rerun()

        if st.session_state.ceo_summary_text:
            st.subheader("ğŸ‘‘ åŸ·è¡Œé•· (CEO) æœ€çµ‚æ±ºç­–")
            st.markdown(st.session_state.ceo_summary_text)

    # --- å…¶ä»– AI è§’è‰²æ¨™ç±¤ ---
    for i, (role_id, role_info) in enumerate(ROLE_DEFINITIONS.items()):
        with tabs[i + 2]:
            st.header(role_info["name"])
            st.caption(role_info["system_prompt"])
            session_id = role_info["session_id"]
            if session_id not in st.session_state.chat_histories: st.session_state.chat_histories[session_id] = []

            for msg in st.session_state.chat_histories[session_id]:
                with st.chat_message(msg["role"]): st.markdown(msg["content"])

            if user_input := st.chat_input(f"èˆ‡ {role_info['name']} å°è©±...", key=session_id):
                st.session_state.chat_histories[session_id].append({"role": "human", "content": user_input})
                with st.chat_message("human"): st.markdown(user_input)
                
                with st.chat_message("ai"):
                    with st.spinner("æ­£åœ¨ç”Ÿæˆå›æ‡‰..."):
                        client_with_prompt = get_gemini_client(gemini_api_key)
                        client_with_prompt.system_instruction = role_info["system_prompt"]
                        
                        history = st.session_state.chat_histories[session_id][:-1]
                        response = get_gemini_response_with_history(client_with_prompt, history, user_input)
                        
                        st.markdown(response)
                        st.session_state.chat_histories[session_id].append({"role": "ai", "content": response})

if __name__ == "__main__":
    main()
