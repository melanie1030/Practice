import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import json
import traceback
import re
import os
import dotenv
from io import StringIO
from PIL import Image
import time
import matplotlib.font_manager as fm
import matplotlib
import numpy as np

# --- åŸºç¤ API å’Œè³‡æ–™è™•ç†å¥—ä»¶ ---
import google.generativeai as genai
# --- RAG ç›¸é—œå¥—ä»¶ ---
from openai import OpenAI
import faiss
from langchain_openai import OpenAIEmbeddings
from langchain_community.document_loaders import CSVLoader
from langchain_community.vectorstores import FAISS as LangChainFAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter

# --- åˆå§‹åŒ–èˆ‡å¸¸æ•¸å®šç¾© ---
dotenv.load_dotenv()
UPLOAD_DIR = "uploaded_files"
if not os.path.exists(UPLOAD_DIR): os.makedirs(UPLOAD_DIR)

ROLE_DEFINITIONS = {
    "summarizer": { "name": "ğŸ“ æ‘˜è¦å°ˆå®¶", "system_prompt": "ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ‘˜è¦å°ˆå®¶ã€‚ä½ çš„ä»»å‹™æ˜¯å°‡æä¾›çš„ä»»ä½•æ–‡æœ¬æˆ–å°è©±ï¼Œæ¿ƒç¸®æˆæ¸…æ™°ã€ç°¡æ½”çš„ç¹é«”ä¸­æ–‡æ‘˜è¦ã€‚å°ˆæ³¨æ–¼è¦é»å’Œé—œéµçµè«–ã€‚", "session_id": "summarizer_chat" },
    "creative_writer": { "name": "âœï¸ å‰µæ„ä½œå®¶", "system_prompt": "ä½ æ˜¯ä¸€ä½å……æ»¿æƒ³åƒåŠ›çš„å‰µæ„ä½œå®¶ã€‚ä½ çš„ä»»å‹™æ˜¯å¹«åŠ©ä½¿ç”¨è€…å®Œæˆå‰µæ„å¯«ä½œï¼Œä¾‹å¦‚å¯«æ•…äº‹ã€è©©æ­Œã€åŠ‡æœ¬æˆ–è…¦åŠ›æ¿€ç›ªï¼Œå…¨éƒ¨ä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚", "session_id": "creative_writer_chat" }
}

# --- åŸºç¤è¼”åŠ©å‡½æ•¸ ---
def save_uploaded_file(uploaded_file):
    if not os.path.exists(UPLOAD_DIR): os.makedirs(UPLOAD_DIR)
    file_path = os.path.join(UPLOAD_DIR, uploaded_file.name)
    with open(file_path, "wb") as f: f.write(uploaded_file.getbuffer())
    return file_path

def add_user_image_to_main_chat(uploaded_file):
    try:
        file_path = save_uploaded_file(uploaded_file)
        st.session_state.pending_image_for_main_gemini = Image.open(file_path)
        st.image(st.session_state.pending_image_for_main_gemini, caption="åœ–ç‰‡å·²ä¸Šå‚³ï¼Œå°‡éš¨ä¸‹ä¸€æ¢æ–‡å­—è¨Šæ¯ç™¼é€ã€‚", use_container_width=True)
    except Exception as e: st.error(f"è™•ç†ä¸Šå‚³åœ–ç‰‡æ™‚å‡ºéŒ¯: {e}")

# --- RAG æ ¸å¿ƒå‡½å¼ ---
@st.cache_resource
def create_lc_retriever(file_path: str, openai_api_key: str):
    with st.status("æ­£åœ¨å»ºç«‹ RAG çŸ¥è­˜åº«...", expanded=True) as status:
        try:
            status.update(label="æ­¥é©Ÿ 1/3ï¼šè¼‰å…¥èˆ‡åˆ‡å‰²æ–‡ä»¶...")
            loader = CSVLoader(file_path=file_path, encoding='utf-8')
            documents = loader.load()
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
            docs = text_splitter.split_documents(documents)
            status.update(label=f"æ­¥é©Ÿ 1/3 å®Œæˆï¼å·²åˆ‡å‰²æˆ {len(docs)} å€‹å€å¡Šã€‚")

            status.update(label="æ­¥é©Ÿ 2/3ï¼šå‘¼å« OpenAI API ç”Ÿæˆå‘é‡åµŒå…¥...")
            embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
            vector_store = LangChainFAISS.from_documents(docs, embeddings)
            status.update(label="æ­¥é©Ÿ 2/3 å®Œæˆï¼å‘é‡åµŒå…¥å·²ç”Ÿæˆã€‚")

            status.update(label="æ­¥é©Ÿ 3/3ï¼šæª¢ç´¢å™¨æº–å‚™å®Œæˆï¼", state="complete", expanded=False)
            return vector_store.as_retriever(search_kwargs={'k': 5})
        except Exception as e:
            st.error(f"å»ºç«‹çŸ¥è­˜åº«éç¨‹ä¸­ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}")
            status.update(label="å»ºç«‹å¤±æ•—", state="error")
            return None

# --- Gemini API ç›¸é—œå‡½å¼ ---
def get_gemini_client(api_key):
    genai.configure(api_key=api_key)
    return genai.GenerativeModel("gemini-1.5-flash-latest")

def get_gemini_response_with_history(client, history, user_prompt):
    gemini_history = []
    for msg in history:
        role = "user" if msg["role"] == "human" else "model"
        gemini_history.append({"role": role, "parts": [msg["content"]]})
    chat = client.start_chat(history=gemini_history)
    response = chat.send_message(user_prompt)
    return response.text

def get_gemini_response_for_image(api_key, user_prompt, image_pil):
    if not api_key: return "éŒ¯èª¤ï¼šæœªè¨­å®š Gemini API Keyã€‚"
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel("gemini-1.5-flash-latest")
        response = model.generate_content([user_prompt, image_pil])
        st.session_state.pending_image_for_main_gemini = None
        return response.text
    except Exception as e: return f"éŒ¯èª¤: {e}"

def get_gemini_executive_analysis(api_key, executive_role_name, full_prompt):
    if not api_key: return f"éŒ¯èª¤ï¼šé«˜ç®¡å·¥ä½œæµ ({executive_role_name}) æœªèƒ½ç²å– Gemini API Keyã€‚"
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel("gemini-1.5-pro-latest")
        response = model.generate_content(full_prompt)
        return response.text
    except Exception as e: return f"éŒ¯èª¤: {e}"

def generate_data_profile(df):
    if df is None or df.empty: return "æ²’æœ‰è³‡æ–™å¯ä¾›åˆ†æã€‚"
    buffer = StringIO()
    df.info(buf=buffer)
    profile_parts = [f"è³‡æ–™å½¢ç‹€: {df.shape}", f"æ¬„ä½è³‡è¨Š:\n{buffer.getvalue()}"]
    try: profile_parts.append(f"\næ•¸å€¼æ¬„ä½çµ±è¨ˆ:\n{df.describe(include='number').to_string()}")
    except: pass
    try: profile_parts.append(f"\né¡åˆ¥æ¬„ä½çµ±è¨ˆ:\n{df.describe(include=['object', 'category']).to_string()}")
    except: pass
    profile_parts.append(f"\nå‰ 5 ç­†è³‡æ–™:\n{df.head().to_string()}")
    return "\n".join(profile_parts)

# ------------------------------
# ä¸»æ‡‰ç”¨å…¥å£
# ------------------------------
def main():
    st.set_page_config(page_title="Gemini Multi-Function Bot", page_icon="âœ¨", layout="wide")
    st.title("âœ¨ Gemini å¤šåŠŸèƒ½ AI åŠ©ç† ")

    # --- åˆå§‹åŒ– Session States ---
    keys_to_init = {
        # *** ä¿®æ”¹é–‹é—œé‚è¼¯ (2025/06/22) ***
        "use_rag": False, "use_multi_stage_workflow": False, # é è¨­ä½¿ç”¨å–®ä¸€æ•´åˆå·¥ä½œæµ
        # åŸå§‹ç‹€æ…‹
        "retriever_chain": None, "uploaded_file_path": None, "last_uploaded_filename": None,
        "pending_image_for_main_gemini": None, "chat_histories": {},
        # éšæ®µå¼å·¥ä½œæµç‹€æ…‹
        "executive_workflow_stage": "idle", "executive_user_query": "",
        "executive_data_profile_str": "", "executive_rag_context": "", "cfo_analysis_text": "",
        "coo_analysis_text": "", "ceo_summary_text": "",
        # å–®ä¸€æç¤ºå·¥ä½œæµç‹€æ…‹
        "sp_workflow_stage": "idle", "sp_user_query": "", "sp_final_report": ""
    }
    for key, default_value in keys_to_init.items():
        if key not in st.session_state: st.session_state[key] = default_value

    # --- å´é‚Šæ¬„ä»‹é¢ ---
    with st.sidebar:
        st.header("âš™ï¸ åŠŸèƒ½èˆ‡æ¨¡å¼è¨­å®š")

        # é–‹é—œä¸€ï¼šæ§åˆ¶ RAG åŠŸèƒ½
        st.session_state.use_rag = st.toggle("å•Ÿç”¨ RAG çŸ¥è­˜åº« (éœ€ OpenAI Key)", value=st.session_state.use_rag)
        
        # *** ä¿®æ”¹é–‹é—œé‚è¼¯ (2025/06/22) ***
        # é–‹é—œäºŒï¼šæ§åˆ¶å·¥ä½œæµæ¨¡å¼ï¼Œé è¨­ç‚º OFF (ä½¿ç”¨å–®ä¸€æ•´åˆæµç¨‹)
        st.session_state.use_multi_stage_workflow = st.toggle("å•Ÿç”¨éšæ®µå¼å·¥ä½œæµ (å¤šé‡è¨˜æ†¶)", value=st.session_state.use_multi_stage_workflow, help="é è¨­(OFF): AI ä¸€æ¬¡å®Œæˆæ‰€æœ‰è§’è‰²åˆ†æ (å–®ä¸€è¨˜æ†¶)ã€‚å•Ÿç”¨(ON): AI ä¾åºå®Œæˆå„è§’è‰²åˆ†æ (å¤šé‡è¨˜æ†¶)ï¼Œé–‹éŠ·è¼ƒå¤§ã€‚")
        st.divider()

        st.header("ğŸ”‘ API é‡‘é‘°")
        st.text_input("è«‹è¼¸å…¥æ‚¨çš„ Google Gemini API Key", type="password", key="gemini_api_key_input")
        if st.session_state.use_rag:
            st.text_input("è«‹è¼¸å…¥æ‚¨çš„ OpenAI API Key (RAG åŠŸèƒ½éœ€è¦)", type="password", key="openai_api_key_input")
        st.divider()

        st.header("ğŸ“ è³‡æ–™ä¸Šå‚³")
        uploaded_file = st.file_uploader("ä¸Šå‚³ CSV æª”æ¡ˆ", type=["csv"])
        
        if uploaded_file:
            if uploaded_file.name != st.session_state.get("last_uploaded_filename"):
                st.session_state.last_uploaded_filename = uploaded_file.name
                file_path = save_uploaded_file(uploaded_file)
                st.session_state.uploaded_file_path = file_path
                st.success(f"æª”æ¡ˆ '{uploaded_file.name}' ä¸Šå‚³æˆåŠŸï¼")
                st.session_state.retriever_chain = None 

                if st.session_state.use_rag:
                    openai_api_key = st.session_state.get("openai_api_key_input") or os.environ.get("OPENAI_API_KEY")
                    if not openai_api_key:
                        st.error("RAG åŠŸèƒ½å·²å•Ÿç”¨ï¼Œè«‹åœ¨ä¸Šæ–¹è¼¸å…¥æ‚¨çš„ OpenAI API Keyï¼")
                    else:
                        st.session_state.retriever_chain = create_lc_retriever(file_path, openai_api_key)

        if st.session_state.retriever_chain:
            st.success("âœ… RAG çŸ¥è­˜åº«å·²å•Ÿç”¨ï¼")

        st.header("ğŸ–¼ï¸ åœ–ç‰‡åˆ†æ")
        uploaded_image = st.file_uploader("ä¸Šå‚³åœ–ç‰‡", type=["png", "jpg", "jpeg"])
        if uploaded_image: add_user_image_to_main_chat(uploaded_image)
        
        st.divider()
        if st.button("ğŸ—‘ï¸ æ¸…é™¤æ‰€æœ‰å°è©±èˆ‡è³‡æ–™"):
            settings = {
                'gemini_api_key_input': st.session_state.get('gemini_api_key_input'),
                'openai_api_key_input': st.session_state.get('openai_api_key_input'),
                'use_rag': st.session_state.get('use_rag'),
                'use_multi_stage_workflow': st.session_state.get('use_multi_stage_workflow')
            }
            st.session_state.clear()
            for key, value in settings.items():
                st.session_state[key] = value
            st.cache_resource.clear()
            st.success("æ‰€æœ‰å°è©±ã€Session è¨˜æ†¶å’Œå¿«å–å·²æ¸…é™¤ï¼")
            st.rerun()

    # --- ä¸»å·¥ä½œå€ (æ¨™ç±¤é é¢) ---
    tab_titles = ["ğŸ’¬ ä¸»è¦èŠå¤©å®¤", "ğŸ’¼ é«˜ç®¡å·¥ä½œæµ"] + [role["name"] for role in ROLE_DEFINITIONS.values()]
    tabs = st.tabs(tab_titles)

    # --- API Key æª¢æŸ¥ ---
    gemini_api_key = st.session_state.get("gemini_api_key_input") or os.environ.get("GOOGLE_API_KEY")
    if not gemini_api_key:
        st.warning("è«‹åœ¨å´é‚Šæ¬„è¼¸å…¥æ‚¨çš„ Google Gemini API Key ä»¥å•Ÿå‹•èŠå¤©åŠŸèƒ½ã€‚")
        st.stop()
    gemini_client = get_gemini_client(gemini_api_key)

    # --- ä¸»è¦èŠå¤©å®¤ (tabs[0]) ---
    with tabs[0]:
        st.header("ğŸ’¬ ä¸»è¦èŠå¤©å®¤")
        st.caption("å¯é€²è¡Œä¸€èˆ¬å°è©±ã€åœ–ç‰‡åˆ†æã€‚RAG å•ç­”åŠŸèƒ½å¯ç”±å´é‚Šæ¬„é–‹é—œå•Ÿç”¨ã€‚")
        session_id = "main_chat"
        if session_id not in st.session_state.chat_histories: st.session_state.chat_histories[session_id] = []
        
        for msg in st.session_state.chat_histories[session_id]:
            with st.chat_message(msg["role"]): st.markdown(msg["content"])
        
        if user_input := st.chat_input("è«‹å°æ•¸æ“šã€åœ–ç‰‡æå•æˆ–é–‹å§‹å°è©±..."):
            st.session_state.chat_histories[session_id].append({"role": "human", "content": user_input})
            with st.chat_message("human"): st.markdown(user_input)
            
            with st.chat_message("ai"):
                with st.spinner("æ­£åœ¨æ€è€ƒä¸­..."):
                    response = ""
                    if st.session_state.use_rag and st.session_state.retriever_chain:
                        retrieved_docs = st.session_state.retriever_chain.invoke(user_input)
                        context = "\n---\n".join([doc.page_content for doc in retrieved_docs])
                        prompt = f"è«‹æ ¹æ“šä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”å•é¡Œã€‚\n\n[ä¸Šä¸‹æ–‡]:\n{context}\n\n[å•é¡Œ]:\n{user_input}\n\n[å›ç­”]:"
                        response = gemini_client.generate_content(prompt).text
                    elif st.session_state.pending_image_for_main_gemini:
                        response = get_gemini_response_for_image(gemini_api_key, user_input, st.session_state.pending_image_for_main_gemini)
                    else:
                        history = st.session_state.chat_histories[session_id][:-1]
                        response = get_gemini_response_with_history(gemini_client, history, user_input)
                    
                    st.markdown(response)
                    st.session_state.chat_histories[session_id].append({"role": "ai", "content": response})

    # --- åˆä½µå¾Œçš„é«˜ç®¡å·¥ä½œæµ (tabs[1]) ---
    with tabs[1]:
        st.header("ğŸ’¼ é«˜ç®¡å·¥ä½œæµ")
        # *** ä¿®æ”¹é¡¯ç¤ºé‚è¼¯ (2025/06/22) ***
        st.caption(f"ç›®å‰æ¨¡å¼ï¼š{'éšæ®µå¼å·¥ä½œæµ (å¤šé‡è¨˜æ†¶)' if st.session_state.use_multi_stage_workflow else 'å–®ä¸€æç¤ºæ•´åˆå·¥ä½œæµ (å–®ä¸€è¨˜æ†¶)'} | RAG çŸ¥è­˜åº«ï¼š{'å•Ÿç”¨' if st.session_state.use_rag else 'åœç”¨'}")

        # *** ä¿®æ”¹åˆ¤æ–·é‚è¼¯ (2025/06/22) ***
        # å¦‚æœé–‹é—œç‚º ONï¼ŒåŸ·è¡Œéšæ®µå¼å·¥ä½œæµ
        if st.session_state.use_multi_stage_workflow:
            st.info("**æ–¹æ³•èªªæ˜**ï¼šæ­¤æµç¨‹æ¨¡æ“¬ä¸‰ä½ç¨ç«‹çš„å°ˆå®¶ã€‚æ¯ä¸€æ­¥éƒ½æ˜¯ä¸€æ¬¡ç¨ç«‹çš„ API è«‹æ±‚ï¼Œå¾Œä¸€ä½å°ˆå®¶çš„åˆ†æåŸºæ–¼å‰ä¸€ä½çš„æ›¸é¢å ±å‘Šã€‚")
            st.session_state.executive_user_query = st.text_area("è«‹è¼¸å…¥å•†æ¥­å•é¡Œä»¥å•Ÿå‹•åˆ†æ:", value=st.session_state.get("executive_user_query", ""), height=100, key="original_workflow_query")
            can_start = bool(st.session_state.get("uploaded_file_path") and st.session_state.get("executive_user_query"))
            if st.button("ğŸš€ å•Ÿå‹•éšæ®µå¼åˆ†æ", disabled=not can_start, key="exec_flow_button"):
                st.session_state.executive_workflow_stage = "cfo_analysis_pending"
                st.session_state.cfo_analysis_text = ""
                st.session_state.coo_analysis_text = ""
                st.session_state.ceo_summary_text = ""
                st.session_state.executive_rag_context = ""
                st.rerun()
            
            if st.session_state.executive_workflow_stage == "cfo_analysis_pending":
                with st.spinner("CFO æ­£åœ¨ç¨ç«‹åˆ†æ..."):
                    df = pd.read_csv(st.session_state.uploaded_file_path)
                    data_profile = generate_data_profile(df)
                    st.session_state.executive_data_profile_str = data_profile
                    query = st.session_state.executive_user_query
                    rag_context_for_prompt = ""
                    if st.session_state.use_rag and st.session_state.retriever_chain:
                        retrieved_docs = st.session_state.retriever_chain.invoke(query)
                        rag_context = "\n---\n".join([doc.page_content for doc in retrieved_docs])
                        st.session_state.executive_rag_context = rag_context
                        rag_context_for_prompt = f"\n\n[RAG æª¢ç´¢å‡ºçš„ç›¸é—œæ•¸æ“š]:\n{rag_context}"
                    cfo_prompt = f"ä½œç‚ºè²¡å‹™é•·(CFO)ï¼Œè«‹åŸºæ–¼ä½ çš„å°ˆæ¥­çŸ¥è­˜ï¼Œä¸¦åš´æ ¼åƒè€ƒä»¥ä¸‹æä¾›çš„è³‡æ–™ï¼Œç‚ºå•†æ¥­å•é¡Œæä¾›è²¡å‹™è§’åº¦çš„ç°¡æ½”åˆ†æã€‚\n\n[å•†æ¥­å•é¡Œ]:\n{query}\n\n[çµ±è¨ˆæ‘˜è¦]:\n{data_profile}{rag_context_for_prompt}"
                    st.session_state.cfo_analysis_text = get_gemini_executive_analysis(gemini_api_key, "CFO", cfo_prompt)
                    st.session_state.executive_workflow_stage = "coo_analysis_pending"
                    st.rerun()

            if st.session_state.get('executive_data_profile_str'):
                with st.expander("æŸ¥çœ‹çµ±è¨ˆæ‘˜è¦"): st.text(st.session_state.executive_data_profile_str)
            if st.session_state.use_rag and st.session_state.get('executive_rag_context'):
                with st.expander("æŸ¥çœ‹ RAG æª¢ç´¢å‡ºçš„ç›¸é—œè³‡æ–™"): st.markdown(st.session_state.executive_rag_context)

            if st.session_state.cfo_analysis_text:
                st.subheader("ğŸ“Š è²¡å‹™é•· (CFO) åˆ†æ")
                st.markdown(st.session_state.cfo_analysis_text)
            
            if st.session_state.executive_workflow_stage == "coo_analysis_pending":
                with st.spinner("COO æ­£åœ¨åˆ†æ..."):
                    rag_context_for_prompt = f"\n\n[RAG æª¢ç´¢å‡ºçš„ç›¸é—œæ•¸æ“š]:\n{st.session_state.executive_rag_context}" if st.session_state.executive_rag_context else ""
                    coo_prompt = f"ä½œç‚ºç‡Ÿé‹é•·(COO)ï¼Œè«‹åŸºæ–¼å•†æ¥­å•é¡Œã€çµ±è¨ˆæ‘˜è¦ã€CFO çš„è²¡å‹™åˆ†æä»¥åŠç›¸é—œæ•¸æ“šï¼Œæä¾›ç‡Ÿé‹å±¤é¢çš„ç­–ç•¥èˆ‡æ½›åœ¨é¢¨éšªã€‚\n\n[å•†æ¥­å•é¡Œ]:\n{st.session_state.executive_user_query}\n\n[CFO çš„è²¡å‹™åˆ†æ]:\n{st.session_state.cfo_analysis_text}\n\n[çµ±è¨ˆæ‘˜è¦]:\n{st.session_state.executive_data_profile_str}{rag_context_for_prompt}"
                    st.session_state.coo_analysis_text = get_gemini_executive_analysis(gemini_api_key, "COO", coo_prompt)
                    st.session_state.executive_workflow_stage = "ceo_summary_pending"
                    st.rerun()

            if st.session_state.coo_analysis_text:
                st.subheader("ğŸ­ ç‡Ÿé‹é•· (COO) åˆ†æ")
                st.markdown(st.session_state.coo_analysis_text)

            if st.session_state.executive_workflow_stage == "ceo_summary_pending":
                with st.spinner("CEO æ­£åœ¨ç¸½çµ..."):
                    rag_context_for_prompt = f"\n\n[RAG æª¢ç´¢å‡ºçš„ç›¸é—œæ•¸æ“š]:\n{st.session_state.executive_rag_context}" if st.session_state.executive_rag_context else ""
                    ceo_prompt = f"ä½œç‚ºåŸ·è¡Œé•·(CEO)ï¼Œè«‹æ•´åˆæ‰€æœ‰è³‡è¨Šï¼Œæä¾›ä¸€å€‹é«˜å±¤æ¬¡çš„æ±ºç­–ç¸½çµã€‚\n\n[å•†æ¥­å•é¡Œ]:\n{st.session_state.executive_user_query}\n\n[CFO çš„è²¡å‹™åˆ†æ]:\n{st.session_state.cfo_analysis_text}\n\n[COO çš„ç‡Ÿé‹åˆ†æ]:\n{st.session_state.coo_analysis_text}{rag_context_for_prompt}"
                    st.session_state.ceo_summary_text = get_gemini_executive_analysis(gemini_api_key, "CEO", ceo_prompt)
                    st.session_state.executive_workflow_stage = "completed"
                    st.rerun()
            
            if st.session_state.ceo_summary_text:
                st.subheader("ğŸ‘‘ åŸ·è¡Œé•· (CEO) æœ€çµ‚æ±ºç­–")
                st.markdown(st.session_state.ceo_summary_text)

        # å¦‚æœé–‹é—œç‚º OFFï¼ŒåŸ·è¡Œå–®ä¸€æ•´åˆå·¥ä½œæµ
        else:
            st.info("**æ–¹æ³•èªªæ˜**ï¼šæ­¤ç‚ºé è¨­æµç¨‹ã€‚æ¨¡æ“¬ä¸€å€‹å…¨èƒ½çš„ AI é«˜ç®¡åœ˜éšŠï¼Œåªç™¼é€**ä¸€æ¬¡**è«‹æ±‚ï¼ŒAI åœ¨ä¸€æ¬¡ç”Ÿæˆä¸­å®Œæˆæ‰€æœ‰è§’è‰²æ€è€ƒã€‚")
            st.session_state.sp_user_query = st.text_area("è«‹è¼¸å…¥å•†æ¥­å•é¡Œä»¥å•Ÿå‹•åˆ†æ:", value=st.session_state.get("sp_user_query", ""), height=100, key="sp_workflow_query")
            can_start_sp = bool(st.session_state.get("uploaded_file_path") and st.session_state.get("sp_user_query"))
            if st.button("ğŸš€ å•Ÿå‹•æ•´åˆåˆ†æ", disabled=not can_start_sp, key="sp_flow_button"):
                st.session_state.sp_workflow_stage = "running"
                with st.spinner("AI é«˜ç®¡åœ˜éšŠæ­£åœ¨é€²è¡Œå…¨é¢åˆ†æ..."):
                    df = pd.read_csv(st.session_state.uploaded_file_path)
                    data_profile = generate_data_profile(df)
                    query = st.session_state.sp_user_query
                    rag_context_str = ""
                    if st.session_state.use_rag and st.session_state.retriever_chain:
                        retrieved_docs = st.session_state.retriever_chain.invoke(query)
                        rag_context = "\n---\n".join([doc.page_content for doc in retrieved_docs])
                        rag_context_str = f"\n\n**[RAG æª¢ç´¢å‡ºçš„ç›¸é—œæ•¸æ“š]:**\n{rag_context}"
                    
                    mega_prompt = f"""ä½ æ˜¯ä¸€å€‹é ‚å°–çš„ AI å•†æ¥­åˆ†æåœ˜éšŠï¼Œèƒ½å¤ åœ¨ä¸€æ¬¡æ€è€ƒä¸­æ‰®æ¼”å¤šå€‹é«˜ç®¡è§’è‰²ã€‚ä½ çš„ä»»å‹™æ˜¯é‡å°çµ¦å®šçš„å•†æ¥­å•é¡Œå’Œæ•¸æ“šï¼Œç”Ÿæˆä¸€ä»½åŒ…å«ä¸‰å€‹éƒ¨åˆ†çš„å®Œæ•´åˆ†æå ±å‘Šã€‚

è«‹åš´æ ¼æŒ‰ç…§ä»¥ä¸‹çµæ§‹å’Œè¦æ±‚é€²è¡Œè¼¸å‡ºï¼Œä½¿ç”¨ Markdown æ¨™é¡Œä¾†å€åˆ†æ¯å€‹éƒ¨åˆ†ï¼š
---
### ğŸ“Š è²¡å‹™é•· (CFO) åˆ†æ
åœ¨æ­¤éƒ¨åˆ†ï¼Œè«‹å®Œå…¨ä»¥è²¡å‹™é•·çš„è§’åº¦æ€è€ƒã€‚å°ˆæ³¨æ–¼è²¡å‹™æŒ‡æ¨™ã€æˆæœ¬æ•ˆç›Šã€æŠ•è³‡å›å ±ç‡ã€æ¯›åˆ©ç‡ã€æ½›åœ¨çš„è²¡å‹™é¢¨éšªç­‰ã€‚ä½ çš„åˆ†æå¿…é ˆå®Œå…¨åŸºæ–¼æä¾›çš„æ•¸æ“šã€‚

### ğŸ­ ç‡Ÿé‹é•· (COO) åˆ†æ
åœ¨æ­¤éƒ¨åˆ†ï¼Œè½‰æ›ç‚ºç‡Ÿé‹é•·çš„è§’è‰²ã€‚ä½ éœ€è¦æ€è€ƒï¼Œåœ¨CFOæœƒæå‡ºçš„è²¡å‹™è€ƒé‡ä¸‹ï¼Œç‡Ÿé‹ä¸Šæ˜¯å¦å¯è¡Œï¼Ÿåˆ†ææ½›åœ¨çš„æµç¨‹ã€ä¾›æ‡‰éˆã€äººåŠ›è³‡æºæˆ–åŸ·è¡Œé¢¨éšªã€‚ä½ çš„åˆ†æéœ€è¦å‹™å¯¦ä¸”è‘—é‡æ–¼å¯åŸ·è¡Œæ€§ã€‚

### ğŸ‘‘ åŸ·è¡Œé•· (CEO) æœ€çµ‚æ±ºç­–
åœ¨æ­¤éƒ¨åˆ†ï¼Œä½œç‚ºCEOï¼Œè«‹ç¶œåˆä¸Šè¿°çš„è²¡å‹™(CFO)å’Œç‡Ÿé‹(COO)åˆ†æã€‚ä¸è¦é‡è¤‡ç´°ç¯€ï¼Œè€Œæ˜¯æä¾›ä¸€å€‹é«˜å±¤æ¬¡çš„æˆ°ç•¥ç¸½çµã€‚æœ€çµ‚ï¼Œçµ¦å‡ºä¸€å€‹æ˜ç¢ºã€æœæ–·çš„**æ±ºç­–**ï¼ˆä¾‹å¦‚ï¼šæ‰¹å‡†ã€é§å›ã€éœ€è¦æ›´å¤šè³‡æ–™ï¼‰ï¼Œä¸¦åˆ—å‡º 2-3 å€‹æœ€é‡è¦çš„**å¾ŒçºŒè¡Œå‹•å»ºè­°**ã€‚
---
ç¾åœ¨ï¼Œè«‹æ ¹æ“šä»¥ä¸‹è³‡è¨Šé–‹å§‹åˆ†æï¼š

**[å•†æ¥­å•é¡Œ]:**
{query}

**[è³‡æ–™çµ±è¨ˆæ‘˜è¦]:**
{data_profile}{rag_context_str}
"""
                    response = get_gemini_executive_analysis(gemini_api_key, "IntegratedTeam", mega_prompt)
                    st.session_state.sp_final_report = response
                    st.session_state.sp_workflow_stage = "completed"
                    st.rerun()

            if st.session_state.sp_workflow_stage == "completed" and st.session_state.sp_final_report:
                st.subheader("ğŸ“ˆ AI é«˜ç®¡åœ˜éšŠæ•´åˆå ±å‘Š")
                st.markdown(st.session_state.sp_final_report)


    # --- å…¶ä»– AI è§’è‰²æ¨™ç±¤ (tabs[2] å’Œ tabs[3]) ---
    role_tab_offset = 2 
    for i, (role_id, role_info) in enumerate(ROLE_DEFINITIONS.items()):
        with tabs[i + role_tab_offset]:
            st.header(role_info["name"])
            st.caption(role_info["system_prompt"])
            session_id = role_info["session_id"]
            if session_id not in st.session_state.chat_histories: st.session_state.chat_histories[session_id] = []
            for msg in st.session_state.chat_histories[session_id]:
                with st.chat_message(msg["role"]): st.markdown(msg["content"])
            if user_input := st.chat_input(f"èˆ‡ {role_info['name']} å°è©±...", key=session_id):
                st.session_state.chat_histories[session_id].append({"role": "human", "content": user_input})
                with st.chat_message("human"): st.markdown(user_input)
                with st.chat_message("ai"):
                    with st.spinner("æ­£åœ¨ç”Ÿæˆå›æ‡‰..."):
                        history = st.session_state.chat_histories[session_id][:-1]
                        final_prompt = f"{role_info['system_prompt']}\n\n{user_input}"
                        response = get_gemini_response_with_history(gemini_client, history, final_prompt)
                        st.markdown(response)
                        st.session_state.chat_histories[session_id].append({"role": "ai", "content": response})

if __name__ == "__main__":
    main()
