import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import json
import traceback
import re
import os
import dotenv
import base64
from io import BytesIO, StringIO
from PIL import Image
import google.generativeai as genai
import time
import matplotlib.font_manager as fm
import matplotlib
import sys

# --- LangChain and Gemini Imports ---
from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent
from langchain_google_genai import ChatGoogleGenerativeAI

# --- Role & Workflow Definitions (Now simplified and Gemini-focused) ---
ROLE_DEFINITIONS = {
    "summarizer": {
        "name": "ğŸ“ æ‘˜è¦å°ˆå®¶",
        "system_prompt": "ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ‘˜è¦å°ˆå®¶ã€‚ä½ çš„ä»»å‹™æ˜¯å°‡æä¾›çš„ä»»ä½•æ–‡æœ¬æˆ–å°è©±ï¼Œæ¿ƒç¸®æˆæ¸…æ™°ã€ç°¡æ½”çš„ç¹é«”ä¸­æ–‡æ‘˜è¦ã€‚å°ˆæ³¨æ–¼è¦é»å’Œé—œéµçµè«–ã€‚",
        "messages_key": "summarizer_messages",
        "chat_session_key": "summarizer_chat_session",
    },
    "creative_writer": {
        "name": "âœï¸ å‰µæ„ä½œå®¶",
        "system_prompt": "ä½ æ˜¯ä¸€ä½å……æ»¿æƒ³åƒåŠ›çš„å‰µæ„ä½œå®¶ã€‚ä½ çš„ä»»å‹™æ˜¯å¹«åŠ©ä½¿ç”¨è€…å®Œæˆå‰µæ„å¯«ä½œï¼Œä¾‹å¦‚å¯«æ•…äº‹ã€è©©æ­Œã€åŠ‡æœ¬æˆ–è…¦åŠ›æ¿€ç›ªï¼Œå…¨éƒ¨ä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚",
        "messages_key": "creative_writer_messages",
        "chat_session_key": "creative_writer_chat_session",
    }
}

EXECUTIVE_ROLE_IDS = {
    "CFO": "cfo_exec",
    "COO": "coo_exec",
    "CEO": "ceo_exec",
}


# --- ä¸­æ–‡å­—å‹è¨­å®š ---
try:
    font_path = "./fonts/msjh.ttc"
    fm.fontManager.addfont(font_path)
    matplotlib.rcParams['font.family'] = fm.FontProperties(fname=font_path).get_name()
    matplotlib.rcParams['axes.unicode_minus'] = False
except Exception as e:
    print(f"ä¸­æ–‡å­—å‹è¼‰å…¥å¤±æ•—: {e}")
    st.warning(f"ä¸­æ–‡å­—å‹è¼‰å…¥å¤±æ•—ï¼Œåœ–è¡¨ä¸­çš„ä¸­æ–‡å¯èƒ½ç„¡æ³•æ­£å¸¸é¡¯ç¤ºã€‚è«‹ç¢ºèªå­—å‹è·¯å¾‘ '{font_path}' æ˜¯å¦æ­£ç¢ºã€‚")

# --- åˆå§‹åŒ–è¨­ç½® ---
dotenv.load_dotenv()
UPLOAD_DIR = "uploaded_files"
if not os.path.exists(UPLOAD_DIR):
    os.makedirs(UPLOAD_DIR)

MAX_MESSAGES_PER_STREAM = 12

# --- åŸºç¤è¼”åŠ©å‡½æ•¸ ---
def debug_log(msg):
    if st.session_state.get("debug_mode", False):
        if "debug_logs" not in st.session_state: st.session_state.debug_logs = []
        st.session_state.debug_logs.append(f"**LOG ({time.strftime('%H:%M:%S')}):** {msg}")
        print(f"DEBUG LOG: {msg}")

def debug_error(msg):
    if st.session_state.get("debug_mode", False):
        if "debug_errors" not in st.session_state: st.session_state.debug_errors = []
        st.session_state.debug_errors.append(f"**ERROR ({time.strftime('%H:%M:%S')}):** {msg}")
        print(f"DEBUG ERROR: {msg}")

def save_uploaded_file(uploaded_file):
    if not os.path.exists(UPLOAD_DIR): os.makedirs(UPLOAD_DIR)
    file_path = os.path.join(UPLOAD_DIR, uploaded_file.name)
    with open(file_path, "wb") as f:
        f.write(uploaded_file.getbuffer())
    return file_path

def append_message_to_stream(message_stream_key, role, content):
    if message_stream_key not in st.session_state: st.session_state[message_stream_key] = []
    st.session_state[message_stream_key].append({"role": role, "content": content})
    if len(st.session_state[message_stream_key]) > MAX_MESSAGES_PER_STREAM:
        st.session_state[message_stream_key] = st.session_state[message_stream_key][-MAX_MESSAGES_PER_STREAM:]

def add_user_image_to_main_chat(uploaded_file):
    try:
        file_path = save_uploaded_file(uploaded_file)
        image_pil = Image.open(file_path)
        # For a Gemini-only app, we always handle it this way
        st.session_state.pending_image_for_main_gemini = image_pil
        st.image(image_pil, caption="åœ–ç‰‡å·²ä¸Šå‚³ï¼Œå°‡éš¨ä¸‹ä¸€æ¢æ–‡å­—è¨Šæ¯ç™¼é€ã€‚", use_container_width=True)
        debug_log(f"åœ–ç‰‡å·²æš«å­˜ï¼Œå¾…èˆ‡æ–‡å­—ä¸€åŒç™¼é€ (Gemini): {file_path}.")
    except Exception as e:
        st.error(f"è™•ç†ä¸Šå‚³åœ–ç‰‡æ™‚å‡ºéŒ¯: {e}")
        debug_error(f"Error in add_user_image_to_main_chat: {e}, Traceback: {traceback.format_exc()}")

# --- Gemini Pandas Agent æ ¸å¿ƒå‡½æ•¸ ---
def create_pandas_agent(file_path: str):
    gemini_api_key = st.session_state.get("gemini_api_key_input") or st.secrets.get("GEMINI_API_KEY")
    if not gemini_api_key:
        st.error("å»ºç«‹ Gemini è³‡æ–™åˆ†æä»£ç†éœ€è¦ API Keyã€‚")
        return None
    try:
        df = pd.read_csv(file_path)
        llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest", google_api_key=gemini_api_key, convert_system_message_to_human=True)
        agent = create_pandas_dataframe_agent(llm, df, verbose=st.session_state.get("debug_mode", False), handle_parsing_errors=True, allow_dangerous_code=True)
        return agent
    except Exception as e:
        st.error(f"å»ºç«‹è³‡æ–™åˆ†æä»£ç†æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None

def query_pandas_agent(agent, query: str):
    if not agent: return "éŒ¯èª¤ï¼šè³‡æ–™åˆ†æä»£ç†æœªåˆå§‹åŒ–ã€‚"
    prompt = f"è«‹é‡å°ä»¥ä¸‹å•é¡Œé€²è¡Œåˆ†æï¼Œä¸¦ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼š\nå•é¡Œ: \"{query}\""
    try:
        response = agent.invoke({"input": prompt})
        return response.get("output", "ä»£ç†æ²’æœ‰æä¾›æœ‰æ•ˆçš„è¼¸å‡ºã€‚")
    except Exception as e:
        st.error(f"ä»£ç†åœ¨è™•ç†æ‚¨çš„è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return f"ä»£ç†åŸ·è¡Œæ™‚å‡ºéŒ¯: {e}"

# --- Gemini é€šç”¨èŠå¤©å‡½æ•¸ ---
def get_gemini_response_main_chat(user_prompt, image_pil=None):
    api_key = st.session_state.get("gemini_api_key_input") or st.secrets.get("GEMINI_API_KEY")
    if not api_key: return "éŒ¯èª¤ï¼šæœªè¨­å®š Gemini API Keyã€‚"
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel("gemini-1.5-flash-latest")
        content_parts = []
        if image_pil:
            content_parts.append(image_pil)
        content_parts.append(user_prompt)
        response = model.generate_content(content_parts)
        # Clear the pending image after use
        if "pending_image_for_main_gemini" in st.session_state:
            st.session_state.pending_image_for_main_gemini = None
        return response.text
    except Exception as e:
        st.error(f"Gemini API è«‹æ±‚å¤±æ•—: {e}")
        return f"éŒ¯èª¤: {e}"

def get_gemini_response_for_generic_role(role_id, user_input_text):
    api_key = st.session_state.get("gemini_api_key_input") or st.secrets.get("GEMINI_API_KEY")
    if not api_key: return "éŒ¯èª¤ï¼šæœªè¨­å®š Gemini API Keyã€‚"
    try:
        genai.configure(api_key=api_key)
        role_info = ROLE_DEFINITIONS[role_id]
        model = genai.GenerativeModel("gemini-1.5-flash-latest", system_instruction=role_info["system_prompt"])
        response = model.generate_content(user_input_text)
        return response.text
    except Exception as e:
        st.error(f"'{role_info['name']}' è§’è‰²åŸ·è¡Œæ™‚å‡ºéŒ¯: {e}")
        return f"éŒ¯èª¤: {e}"

# --- Gemini é«˜ç®¡å·¥ä½œæµå‡½æ•¸ ---
def get_gemini_executive_analysis(executive_role_name, full_prompt):
    api_key = st.session_state.get("gemini_api_key_input") or st.secrets.get("GEMINI_API_KEY")
    if not api_key: return f"éŒ¯èª¤ï¼šé«˜ç®¡å·¥ä½œæµ ({executive_role_name}) æœªèƒ½ç²å– Gemini API Keyã€‚"
    try:
        genai.configure(api_key=api_key)
        # Use a more powerful model for executive reasoning
        model = genai.GenerativeModel("gemini-1.5-pro-latest")
        response = model.generate_content(full_prompt)
        return response.text
    except Exception as e:
        st.error(f"é«˜ç®¡åˆ†æ ({executive_role_name}) å¤±æ•—: {e}")
        return f"éŒ¯èª¤: {e}"

# --- è³‡æ–™æ‘˜è¦å‡½æ•¸ ---
def generate_data_profile(df):
    if df is None or df.empty: return "æ²’æœ‰è³‡æ–™å¯ä¾›åˆ†æã€‚"
    buffer = StringIO()
    df.info(buf=buffer)
    profile_parts = [f"è³‡æ–™å½¢ç‹€: {df.shape}", f"æ¬„ä½è³‡è¨Š:\n{buffer.getvalue()}"]
    try: profile_parts.append(f"\næ•¸å€¼æ¬„ä½çµ±è¨ˆ:\n{df.describe(include='number').to_string()}")
    except: pass
    try: profile_parts.append(f"\né¡åˆ¥æ¬„ä½çµ±è¨ˆ:\n{df.describe(include=['object', 'category']).to_string()}")
    except: pass
    profile_parts.append(f"\nå‰ 5 ç­†è³‡æ–™:\n{df.head().to_string()}")
    return "\n".join(profile_parts)

# ------------------------------
# ä¸»æ‡‰ç”¨å…¥å£ (æœ€çµ‚ Gemini æ•´åˆç‰ˆ)
# ------------------------------
def main():
    st.set_page_config(page_title="Gemini Multi-Function Bot", page_icon="âœ¨", layout="wide")
    st.title("âœ¨ Gemini å¤šåŠŸèƒ½ AI åŠ©ç†")

    # --- åˆå§‹åŒ–æ‰€æœ‰éœ€è¦çš„ Session States ---
    # ä¸»èŠå¤©å®¤èˆ‡ä»£ç†
    if "messages" not in st.session_state: st.session_state.messages = []
    if "pandas_agent" not in st.session_state: st.session_state.pandas_agent = None
    if "uploaded_file_path" not in st.session_state: st.session_state.uploaded_file_path = None
    if "last_uploaded_filename" not in st.session_state: st.session_state.last_uploaded_filename = None
    if "pending_image_for_main_gemini" not in st.session_state: st.session_state.pending_image_for_main_gemini = None
    
    # é«˜ç®¡å·¥ä½œæµ
    if "executive_workflow_stage" not in st.session_state: st.session_state.executive_workflow_stage = "idle"
    if "executive_user_query" not in st.session_state: st.session_state.executive_user_query = ""
    if "executive_data_profile_str" not in st.session_state: st.session_state.executive_data_profile_str = ""
    if "cfo_analysis_text" not in st.session_state: st.session_state.cfo_analysis_text = ""
    if "coo_analysis_text" not in st.session_state: st.session_state.coo_analysis_text = ""
    if "ceo_summary_text" not in st.session_state: st.session_state.ceo_summary_text = ""
    for exec_id_key in ["cfo_exec_messages", "coo_exec_messages", "ceo_exec_messages"]:
        if exec_id_key not in st.session_state: st.session_state[exec_id_key] = []

    # å…¶ä»–è§’è‰²
    for role_id, role_info in ROLE_DEFINITIONS.items():
        if role_info["messages_key"] not in st.session_state: st.session_state[role_info["messages_key"]] = []
        if role_info["chat_session_key"] not in st.session_state: st.session_state[role_info["chat_session_key"]] = None

    # é™¤éŒ¯æ¨¡å¼
    if "debug_mode" not in st.session_state: st.session_state.debug_mode = False
    if "debug_logs" not in st.session_state: st.session_state.debug_logs = []
    if "debug_errors" not in st.session_state: st.session_state.debug_errors = []

    # --- å´é‚Šæ¬„ä»‹é¢ ---
    with st.sidebar:
        st.header("âš™ï¸ è¨­å®š")
        
        # --- Gemini API Key è¼¸å…¥ ---
        st.text_input(
            "è«‹è¼¸å…¥æ‚¨çš„ Google Gemini API Key",
            value=st.session_state.get("gemini_api_key_input", ""),
            type="password",
            key="gemini_api_key_input"
        )
        st.caption("å„ªå…ˆä½¿ç”¨æ­¤è™•è¼¸å…¥çš„é‡‘é‘°ï¼Œè‹¥ç‚ºç©ºå‰‡å˜—è©¦å¾é›²ç«¯ Secrets æˆ– .env æª”æ¡ˆè¼‰å…¥ã€‚")
        
        st.divider()

        # --- åŠŸèƒ½å€ ---
        st.subheader("ğŸ“ è³‡æ–™åˆ†æ")
        uploaded_file = st.file_uploader("ä¸Šå‚³ CSV ä»¥å•Ÿç”¨è³‡æ–™åˆ†æä»£ç†", type=["csv"])
        if uploaded_file:
            if uploaded_file.name != st.session_state.get("last_uploaded_filename") or not st.session_state.get("pandas_agent"):
                st.session_state.last_uploaded_filename = uploaded_file.name
                file_path = save_uploaded_file(uploaded_file)
                st.session_state.uploaded_file_path = file_path
                with st.spinner("æ­£åœ¨åˆå§‹åŒ– Gemini è³‡æ–™åˆ†æä»£ç†..."):
                    st.session_state.pandas_agent = create_pandas_agent(file_path)
        
        if st.session_state.get("pandas_agent"):
            st.success("âœ… è³‡æ–™åˆ†æä»£ç†å·²å•Ÿç”¨ï¼")

        st.subheader("ğŸ–¼ï¸ åœ–ç‰‡åˆ†æ")
        uploaded_image = st.file_uploader("ä¸Šå‚³åœ–ç‰‡é€²è¡Œåˆ†æ", type=["png", "jpg", "jpeg"])
        if uploaded_image:
            add_user_image_to_main_chat(uploaded_image)
        
        st.divider()

        # --- æ¸…é™¤æŒ‰éˆ•èˆ‡åµéŒ¯ ---
        if st.button("ğŸ—‘ï¸ æ¸…é™¤å°è©±èˆ‡è³‡æ–™"):
            # åªæ¸…é™¤å°è©±å’Œå·¥ä½œç‹€æ…‹ï¼Œä¿ç•™ API Key
            keys_to_clear = [
                "messages", "pandas_agent", "uploaded_file_path", "last_uploaded_filename",
                "pending_image_for_main_gemini", "executive_workflow_stage", "executive_user_query",
                "executive_data_profile_str", "cfo_analysis_text", "coo_analysis_text",
                "ceo_summary_text", "cfo_exec_messages", "coo_exec_messages", "ceo_exec_messages",
                "debug_logs", "debug_errors"
            ]
            # æ¸…é™¤æ‰€æœ‰è§’è‰²çš„å°è©±ç´€éŒ„
            for role_info in ROLE_DEFINITIONS.values():
                keys_to_clear.append(role_info["messages_key"])
                keys_to_clear.append(role_info["chat_session_key"])

            for key in keys_to_clear:
                if key in st.session_state:
                    st.session_state.pop(key)
            st.success("æ‰€æœ‰å°è©±å’Œå·¥ä½œç‹€æ…‹å·²æ¸…é™¤ï¼")
            st.rerun()

        st.session_state.debug_mode = st.checkbox("å•Ÿç”¨åµéŒ¯æ¨¡å¼", value=st.session_state.get("debug_mode", False))
        if st.session_state.debug_mode:
            with st.expander("ğŸ› ï¸ åµéŒ¯è³‡è¨Š"):
                st.json(st.session_state.get("debug_logs", []))
                st.json(st.session_state.get("debug_errors", []))

    # --- ä¸»å·¥ä½œå€ (æ¨™ç±¤é é¢) ---
    tab_titles = ["ğŸ’¬ ä¸»è¦èŠå¤©å®¤", "ğŸ’¼ é«˜ç®¡å·¥ä½œæµ"] + [role["name"] for role in ROLE_DEFINITIONS.values()]
    tabs = st.tabs(tab_titles)

    # ä¸»è¦èŠå¤©å®¤æ¨™ç±¤
    with tabs[0]:
        st.header("ğŸ’¬ ä¸»è¦èŠå¤©å®¤ (åˆ†ææ•¸æ“šèˆ‡åœ–ç‰‡)")
        for msg in st.session_state.messages:
            with st.chat_message(msg["role"]):
                if isinstance(msg["content"], list): # è™•ç†åœ–ç‰‡
                    for item in msg["content"]: st.image(item)
                else:
                    st.markdown(msg["content"])
        
        if st.session_state.get("pending_image_for_main_gemini"):
            st.chat_message("user").image(st.session_state.pending_image_for_main_gemini)

        if user_input := st.chat_input("è«‹å°æ•¸æ“šæˆ–åœ–ç‰‡æå•..."):
            append_message_to_stream("messages", "user", user_input)
            st.rerun()

        if st.session_state.messages and st.session_state.messages[-1]["role"] == "user":
            last_prompt = st.session_state.messages[-1]["content"]
            with st.chat_message("assistant"):
                with st.spinner("Gemini æ­£åœ¨æ€è€ƒä¸­..."):
                    response = ""
                    if st.session_state.get("pandas_agent"):
                        response = query_pandas_agent(st.session_state.pandas_agent, last_prompt)
                    else:
                        pending_image = st.session_state.get("pending_image_for_main_gemini")
                        response = get_gemini_response_main_chat(last_prompt, pending_image)
                    st.markdown(response)
                    append_message_to_stream("messages", "assistant", response)

    # é«˜ç®¡å·¥ä½œæµæ¨™ç±¤
    with tabs[1]:
        st.header("ğŸ’¼ é«˜ç®¡å·¥ä½œæµ (ç”± Gemini Pro é©…å‹•)")
        st.write("è«‹å…ˆåœ¨å´é‚Šæ¬„ä¸Šå‚³CSVè³‡æ–™ï¼Œç„¶å¾Œåœ¨æ­¤è¼¸å…¥å•†æ¥­å•é¡Œï¼Œæœ€å¾Œé»æ“ŠæŒ‰éˆ•å•Ÿå‹•åˆ†æã€‚")
        st.session_state.executive_user_query = st.text_area(
            "è«‹è¼¸å…¥å•†æ¥­å•é¡Œä»¥å•Ÿå‹•åˆ†æ:", 
            value=st.session_state.get("executive_user_query", ""), 
            height=100
        )
        can_start = bool(st.session_state.get("uploaded_file_path") and st.session_state.get("executive_user_query"))
        
        if st.button("ğŸš€ å•Ÿå‹•/é‡å•Ÿé«˜ç®¡åˆ†æ", disabled=not can_start):
             # é‡ç½®ç‹€æ…‹ä»¥é‡æ–°é–‹å§‹
            st.session_state.executive_workflow_stage = "data_profiling_pending"
            st.session_state.executive_data_profile_str = ""
            st.session_state.cfo_analysis_text = ""
            st.session_state.coo_analysis_text = ""
            st.session_state.ceo_summary_text = ""
            st.rerun()
        
        # --- å·¥ä½œæµç‹€æ…‹æ©Ÿ ---
        if st.session_state.executive_workflow_stage == "data_profiling_pending":
            with st.spinner("æ­£åœ¨ç”Ÿæˆè³‡æ–™æ‘˜è¦..."):
                df = pd.read_csv(st.session_state.uploaded_file_path)
                st.session_state.executive_data_profile_str = generate_data_profile(df)
                st.session_state.executive_workflow_stage = "cfo_analysis_pending"
                st.rerun()

        if st.session_state.executive_data_profile_str:
            with st.expander("æŸ¥çœ‹è³‡æ–™æ‘˜è¦"):
                st.text(st.session_state.executive_data_profile_str)

        # --- CFO åˆ†æéšæ®µ ---
        if st.session_state.executive_workflow_stage == "cfo_analysis_pending":
            with st.spinner("CFO æ­£åœ¨åˆ†æ... (Gemini Pro)"):
                cfo_prompt = f"""ä½œç‚ºè²¡å‹™é•·(CFO)ï¼Œè«‹åŸºæ–¼ä»¥ä¸‹å•†æ¥­å•é¡Œå’Œè³‡æ–™æ‘˜è¦ï¼Œæä¾›è²¡å‹™è§’åº¦çš„ç°¡æ½”åˆ†æï¼ŒåŒ…æ‹¬æˆæœ¬ã€ç‡Ÿæ”¶ã€åˆ©æ½¤ç­‰æ½›åœ¨å½±éŸ¿ã€‚

                å•†æ¥­å•é¡Œ: {st.session_state.executive_user_query}
                
                è³‡æ–™æ‘˜è¦:
                {st.session_state.executive_data_profile_str}
                """
                response = get_gemini_executive_analysis("CFO", cfo_prompt)
                st.session_state.cfo_analysis_text = response
                st.session_state.executive_workflow_stage = "coo_analysis_pending"
                st.rerun()
        
        if st.session_state.cfo_analysis_text:
            st.subheader("ğŸ“Š è²¡å‹™é•· (CFO) åˆ†æ")
            st.markdown(st.session_state.cfo_analysis_text)
        
        # --- COO åˆ†æéšæ®µ (å·²è£œå…¨) ---
        if st.session_state.executive_workflow_stage == "coo_analysis_pending":
            with st.spinner("COO æ­£åœ¨åˆ†æ... (Gemini Pro)"):
                coo_prompt = f"""ä½œç‚ºç‡Ÿé‹é•·(COO)ï¼Œè«‹åŸºæ–¼ä»¥ä¸‹å•†æ¥­å•é¡Œã€è³‡æ–™æ‘˜è¦å’Œè²¡å‹™é•·(CFO)çš„åˆ†æï¼Œæä¾›ç‡Ÿé‹å’ŒåŸ·è¡Œå±¤é¢çš„ç­–ç•¥èˆ‡æ½›åœ¨é¢¨éšªã€‚è«‹ä¿æŒç°¡æ½”æœ‰åŠ›ã€‚

                å•†æ¥­å•é¡Œ: {st.session_state.executive_user_query}
                
                è³‡æ–™æ‘˜è¦:
                {st.session_state.executive_data_profile_str}

                CFO çš„è²¡å‹™åˆ†æ:
                {st.session_state.cfo_analysis_text}
                """
                response = get_gemini_executive_analysis("COO", coo_prompt)
                st.session_state.coo_analysis_text = response
                st.session_state.executive_workflow_stage = "ceo_summary_pending" # æ›´æ–°ç‹€æ…‹åˆ°ä¸‹ä¸€æ­¥
                st.rerun()

        if st.session_state.coo_analysis_text:
            st.subheader("ğŸ­ ç‡Ÿé‹é•· (COO) åˆ†æ")
            st.markdown(st.session_state.coo_analysis_text)

        # --- CEO ç¸½çµéšæ®µ (å·²è£œå…¨) ---
        if st.session_state.executive_workflow_stage == "ceo_summary_pending":
            with st.spinner("CEO æ­£åœ¨é€²è¡Œæœ€çµ‚ç¸½çµ... (Gemini Pro)"):
                ceo_prompt = f"""ä½œç‚ºåŸ·è¡Œé•·(CEO)ï¼Œè«‹æ•´åˆä»¥ä¸‹æ‰€æœ‰è³‡è¨Šï¼ˆåŸå§‹å•†æ¥­å•é¡Œã€è³‡æ–™æ‘˜è¦ã€CFOçš„è²¡å‹™åˆ†æã€COOçš„ç‡Ÿé‹åˆ†æï¼‰ï¼Œæä¾›ä¸€å€‹é«˜å±¤æ¬¡çš„æ±ºç­–ç¸½çµèˆ‡æ˜ç¢ºçš„è¡Œå‹•å»ºè­°ã€‚

                å•†æ¥­å•é¡Œ: {st.session_state.executive_user_query}

                è³‡æ–™æ‘˜è¦:
                {st.session_state.executive_data_profile_str}

                CFO çš„è²¡å‹™åˆ†æ:
                {st.session_state.cfo_analysis_text}

                COO çš„ç‡Ÿé‹åˆ†æ:
                {st.session_state.coo_analysis_text}
                """
                response = get_gemini_executive_analysis("CEO", ceo_prompt)
                st.session_state.ceo_summary_text = response
                st.session_state.executive_workflow_stage = "completed" # æ¨™è¨˜å·¥ä½œæµå®Œæˆ
                st.rerun()

        if st.session_state.ceo_summary_text:
            st.subheader("ğŸ‘‘ åŸ·è¡Œé•· (CEO) æœ€çµ‚æ±ºç­–")
            st.markdown(st.session_state.ceo_summary_text)


    # å…¶ä»– AI è§’è‰²æ¨™ç±¤
    for i, (role_id, role_info) in enumerate(ROLE_DEFINITIONS.items()):
        with tabs[i + 2]:
            st.header(role_info["name"])
            st.caption(role_info["system_prompt"])
            message_key = role_info["messages_key"]
            for msg in st.session_state[message_key]:
                with st.chat_message(msg["role"]):
                    st.markdown(msg["content"])
            if user_input := st.chat_input(f"èˆ‡ {role_info['name']} å°è©±..."):
                append_message_to_stream(message_key, "user", user_input)
                # ç›´æ¥åœ¨è¼¸å…¥å¾Œè™•ç†å›æ‡‰ï¼Œé¿å…éœ€è¦å…©æ¬¡rerun
                with st.chat_message("user"):
                    st.markdown(user_input)
                with st.chat_message("assistant"):
                    with st.spinner("æ­£åœ¨ç”Ÿæˆå›æ‡‰..."):
                        response = get_gemini_response_for_generic_role(role_id, user_input)
                        st.markdown(response)
                        append_message_to_stream(message_key, "assistant", response)


if __name__ == "__main__":
    main()
