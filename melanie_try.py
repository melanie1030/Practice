import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import json
import traceback
import re
import os
import dotenv
from io import StringIO
from PIL import Image
import time
import matplotlib.font_manager as fm
import matplotlib
import numpy as np

# --- åŸºç¤ API å’Œè³‡æ–™è™•ç†å¥—ä»¶ ---
import google.generativeai as genai
from openai import OpenAI
import faiss

# --- åªåœ¨ RAG å»ºç«‹/æª¢ç´¢æ™‚ä½¿ç”¨çš„ LangChain å…ƒä»¶ ---
from langchain_openai import OpenAIEmbeddings
from langchain_community.document_loaders import CSVLoader
from langchain_community.vectorstores import FAISS as LangChainFAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter

# --- åˆå§‹åŒ–èˆ‡å¸¸æ•¸å®šç¾© ---
dotenv.load_dotenv()
UPLOAD_DIR = "uploaded_files"
if not os.path.exists(UPLOAD_DIR): os.makedirs(UPLOAD_DIR)

ROLE_DEFINITIONS = {
    "summarizer": { "name": "ğŸ“ æ‘˜è¦å°ˆå®¶", "system_prompt": "ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ‘˜è¦å°ˆå®¶ã€‚ä½ çš„ä»»å‹™æ˜¯å°‡æä¾›çš„ä»»ä½•æ–‡æœ¬æˆ–å°è©±ï¼Œæ¿ƒç¸®æˆæ¸…æ™°ã€ç°¡æ½”çš„ç¹é«”ä¸­æ–‡æ‘˜è¦ã€‚å°ˆæ³¨æ–¼è¦é»å’Œé—œéµçµè«–ã€‚", "session_id": "summarizer_chat" },
    "creative_writer": { "name": "âœï¸ å‰µæ„ä½œå®¶", "system_prompt": "ä½ æ˜¯ä¸€ä½å……æ»¿æƒ³åƒåŠ›çš„å‰µæ„ä½œå®¶ã€‚ä½ çš„ä»»å‹™æ˜¯å¹«åŠ©ä½¿ç”¨è€…å®Œæˆå‰µæ„å¯«ä½œï¼Œä¾‹å¦‚å¯«æ•…äº‹ã€è©©æ­Œã€åŠ‡æœ¬æˆ–è…¦åŠ›æ¿€ç›ªï¼Œå…¨éƒ¨ä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚", "session_id": "creative_writer_chat" }
}

# --- åŸºç¤è¼”åŠ©å‡½æ•¸ ---
def save_uploaded_file(uploaded_file):
    if not os.path.exists(UPLOAD_DIR): os.makedirs(UPLOAD_DIR)
    file_path = os.path.join(UPLOAD_DIR, uploaded_file.name)
    with open(file_path, "wb") as f: f.write(uploaded_file.getbuffer())
    return file_path

def add_user_image_to_main_chat(uploaded_file):
    try:
        file_path = save_uploaded_file(uploaded_file)
        st.session_state.pending_image_for_main_gemini = Image.open(file_path)
        st.image(st.session_state.pending_image_for_main_gemini, caption="åœ–ç‰‡å·²ä¸Šå‚³ï¼Œå°‡éš¨ä¸‹ä¸€æ¢æ–‡å­—è¨Šæ¯ç™¼é€ã€‚", use_container_width=True)
    except Exception as e: st.error(f"è™•ç†ä¸Šå‚³åœ–ç‰‡æ™‚å‡ºéŒ¯: {e}")

# --- æ··åˆæ¶æ§‹ RAG æ ¸å¿ƒå‡½å¼ ---
@st.cache_resource
def create_lc_retriever(file_path: str, openai_api_key: str):
    with st.status("æ­£åœ¨ä½¿ç”¨ LangChain å»ºç«‹çŸ¥è­˜åº«...", expanded=True) as status:
        try:
            status.update(label="æ­¥é©Ÿ 1/3ï¼šè¼‰å…¥èˆ‡åˆ‡å‰²æ–‡ä»¶...")
            loader = CSVLoader(file_path=file_path, encoding='utf-8')
            documents = loader.load()
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
            docs = text_splitter.split_documents(documents)
            status.update(label=f"æ­¥é©Ÿ 1/3 å®Œæˆï¼å·²åˆ‡å‰²æˆ {len(docs)} å€‹å€å¡Šã€‚")

            status.update(label="æ­¥é©Ÿ 2/3ï¼šå‘¼å« OpenAI API ç”Ÿæˆå‘é‡åµŒå…¥...")
            embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
            vector_store = LangChainFAISS.from_documents(docs, embeddings)
            status.update(label="æ­¥é©Ÿ 2/3 å®Œæˆï¼å‘é‡åµŒå…¥å·²ç”Ÿæˆã€‚")

            status.update(label="æ­¥é©Ÿ 3/3ï¼šæª¢ç´¢å™¨æº–å‚™å®Œæˆï¼", state="complete", expanded=False)
            return vector_store.as_retriever(search_kwargs={'k': 5})
        except Exception as e:
            st.error(f"å»ºç«‹çŸ¥è­˜åº«éç¨‹ä¸­ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}")
            status.update(label="å»ºç«‹å¤±æ•—", state="error")
            return None

# --- Gemini API ç›¸é—œå‡½å¼ (æ‰‹å‹•æ§åˆ¶) ---
def get_gemini_client(api_key):
    genai.configure(api_key=api_key)
    return genai.GenerativeModel("gemini-1.5-flash-latest")

def get_gemini_response_with_history(client, history, user_prompt):
    gemini_history = []
    for msg in history:
        role = "user" if msg["role"] == "human" else "model"
        gemini_history.append({"role": role, "parts": [msg["content"]]})
    
    chat = client.start_chat(history=gemini_history)
    response = chat.send_message(user_prompt)
    return response.text

def get_gemini_response_for_image(api_key, user_prompt, image_pil):
    if not api_key: return "éŒ¯èª¤ï¼šæœªè¨­å®š Gemini API Keyã€‚"
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel("gemini-1.5-flash-latest")
        response = model.generate_content([user_prompt, image_pil])
        st.session_state.pending_image_for_main_gemini = None
        return response.text
    except Exception as e:
        st.error(f"Gemini åœ–ç‰‡åˆ†æè«‹æ±‚å¤±æ•—: {e}")
        return f"éŒ¯èª¤: {e}"

def get_gemini_executive_analysis(api_key, executive_role_name, full_prompt):
    if not api_key: return f"éŒ¯èª¤ï¼šé«˜ç®¡å·¥ä½œæµ ({executive_role_name}) æœªèƒ½ç²å– Gemini API Keyã€‚"
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel("gemini-1.5-pro-latest")
        response = model.generate_content(full_prompt)
        return response.text
    except Exception as e:
        st.error(f"é«˜ç®¡åˆ†æ ({executive_role_name}) å¤±æ•—: {e}")
        return f"éŒ¯èª¤: {e}"

def generate_data_profile(df):
    if df is None or df.empty: return "æ²’æœ‰è³‡æ–™å¯ä¾›åˆ†æã€‚"
    buffer = StringIO()
    df.info(buf=buffer)
    profile_parts = [f"è³‡æ–™å½¢ç‹€: {df.shape}", f"æ¬„ä½è³‡è¨Š:\n{buffer.getvalue()}"]
    try: profile_parts.append(f"\næ•¸å€¼æ¬„ä½çµ±è¨ˆ:\n{df.describe(include='number').to_string()}")
    except: pass
    try: profile_parts.append(f"\né¡åˆ¥æ¬„ä½çµ±è¨ˆ:\n{df.describe(include=['object', 'category']).to_string()}")
    except: pass
    profile_parts.append(f"\nå‰ 5 ç­†è³‡æ–™:\n{df.head().to_string()}")
    return "\n".join(profile_parts)

# ------------------------------
# ä¸»æ‡‰ç”¨å…¥å£
# ------------------------------
def main():
    st.set_page_config(page_title="Gemini Multi-Function Bot", page_icon="âœ¨", layout="wide")
    st.title("âœ¨ Gemini å¤šåŠŸèƒ½ AI åŠ©ç† ")

    # --- åˆå§‹åŒ– Session States ---
    keys_to_init = {
        # åŸå§‹ç‹€æ…‹
        "retriever_chain": None, "uploaded_file_path": None, "last_uploaded_filename": None,
        "pending_image_for_main_gemini": None, "chat_histories": {},
        # éšæ®µå¼å·¥ä½œæµç‹€æ…‹
        "executive_workflow_stage": "idle", "executive_user_query": "",
        "executive_data_profile_str": "", "executive_rag_context": "", "cfo_analysis_text": "",
        "coo_analysis_text": "", "ceo_summary_text": "",
        # *** æ–°å¢ï¼šå–®ä¸€æç¤ºæ•´åˆå·¥ä½œæµç‹€æ…‹ ***
        "sp_workflow_stage": "idle", "sp_user_query": "",
        "sp_final_report": ""
    }
    for key, default_value in keys_to_init.items():
        if key not in st.session_state:
            st.session_state[key] = default_value

    # --- å´é‚Šæ¬„ä»‹é¢ ---
    with st.sidebar:
        st.header("âš™ï¸ API é‡‘é‘°è¨­å®š")
        st.text_input("è«‹è¼¸å…¥æ‚¨çš„ Google Gemini API Key", type="password", key="gemini_api_key_input")
        st.text_input("è«‹è¼¸å…¥æ‚¨çš„ OpenAI API Key", type="password", key="openai_api_key_input")
        st.caption("Gemini ç”¨æ–¼èŠå¤©ï¼ŒOpenAI ç”¨æ–¼ RAG è³‡æ–™åµŒå…¥ã€‚")
        st.divider()

        st.subheader("ğŸ“ è³‡æ–™å•ç­” (RAG)")
        uploaded_file = st.file_uploader("ä¸Šå‚³ CSV ä»¥å•Ÿç”¨ RAG å•ç­”åŠŸèƒ½", type=["csv"])
        
        if uploaded_file:
            if uploaded_file.name != st.session_state.get("last_uploaded_filename"):
                openai_api_key = st.session_state.get("openai_api_key_input") or os.environ.get("OPENAI_API_KEY")
                if not openai_api_key:
                    st.error("è«‹åœ¨å´é‚Šæ¬„è¨­å®šæ‚¨çš„ OpenAI API Keyï¼")
                else:
                    st.session_state.last_uploaded_filename = uploaded_file.name
                    file_path = save_uploaded_file(uploaded_file)
                    st.session_state.uploaded_file_path = file_path
                    st.session_state.retriever_chain = create_lc_retriever(file_path, openai_api_key)
        
        if st.session_state.retriever_chain: st.success("âœ… RAG æª¢ç´¢åŠŸèƒ½å·²å•Ÿç”¨ï¼")

        st.subheader("ğŸ–¼ï¸ åœ–ç‰‡åˆ†æ")
        uploaded_image = st.file_uploader("ä¸Šå‚³åœ–ç‰‡é€²è¡Œåˆ†æ", type=["png", "jpg", "jpeg"])
        if uploaded_image: add_user_image_to_main_chat(uploaded_image)
        
        st.divider()
        if st.button("ğŸ—‘ï¸ æ¸…é™¤æ‰€æœ‰å°è©±èˆ‡è³‡æ–™"):
            api_keys = {
                'gemini_api_key_input': st.session_state.get('gemini_api_key_input'),
                'openai_api_key_input': st.session_state.get('openai_api_key_input')
            }
            st.session_state.clear()
            for key, value in api_keys.items():
                if value: st.session_state[key] = value

            st.cache_resource.clear()
            st.success("æ‰€æœ‰å°è©±ã€Session è¨˜æ†¶å’Œå¿«å–å·²æ¸…é™¤ï¼")
            st.rerun()

    # --- ä¸»å·¥ä½œå€ (æ¨™ç±¤é é¢) ---
    tab_titles = ["ğŸ’¬ ä¸»è¦èŠå¤©å®¤", "ğŸ’¼ éšæ®µå¼å·¥ä½œæµ (åŸç‰ˆ)", "ğŸ§© å–®ä¸€æç¤ºæ•´åˆå·¥ä½œæµ (åˆä½µç‰ˆ)"] + [role["name"] for role in ROLE_DEFINITIONS.values()]
    tabs = st.tabs(tab_titles)

    # --- API Key æª¢æŸ¥ ---
    gemini_api_key = st.session_state.get("gemini_api_key_input") or os.environ.get("GOOGLE_API_KEY")
    if not gemini_api_key:
        st.warning("è«‹åœ¨å´é‚Šæ¬„è¼¸å…¥æ‚¨çš„ Google Gemini API Key ä»¥å•Ÿå‹•èŠå¤©åŠŸèƒ½ã€‚")
        st.stop()
    gemini_client = get_gemini_client(gemini_api_key)

    # --- ä¸»è¦èŠå¤©å®¤ (tabs[0]) ---
    with tabs[0]:
        st.header("ğŸ’¬ ä¸»è¦èŠå¤©å®¤")
        st.caption("å¯é€²è¡Œ RAG è³‡æ–™å•ç­”ã€åœ–ç‰‡åˆ†æã€æˆ–ä¸€èˆ¬å°è©±ã€‚")
        session_id = "main_chat"
        if session_id not in st.session_state.chat_histories: st.session_state.chat_histories[session_id] = []
        
        for msg in st.session_state.chat_histories[session_id]:
            with st.chat_message(msg["role"]): st.markdown(msg["content"])
        
        if user_input := st.chat_input("è«‹å°æ•¸æ“šæˆ–åœ–ç‰‡æå•..."):
            st.session_state.chat_histories[session_id].append({"role": "human", "content": user_input})
            with st.chat_message("human"): st.markdown(user_input)
            
            with st.chat_message("ai"):
                with st.spinner("æ­£åœ¨æ€è€ƒä¸­..."):
                    response = ""
                    if st.session_state.retriever_chain:
                        retrieved_docs = st.session_state.retriever_chain.invoke(user_input)
                        context = "\n---\n".join([doc.page_content for doc in retrieved_docs])
                        prompt = f"è«‹æ ¹æ“šä¸Šä¸‹æ–‡å›ç­”å•é¡Œã€‚\n[ä¸Šä¸‹æ–‡]:\n{context}\n\n[å•é¡Œ]:\n{user_input}\n\n[å›ç­”]:"
                        response = gemini_client.generate_content(prompt).text
                    elif st.session_state.pending_image_for_main_gemini:
                        response = get_gemini_response_for_image(gemini_api_key, user_input, st.session_state.pending_image_for_main_gemini)
                    else:
                        history = st.session_state.chat_histories[session_id][:-1]
                        response = get_gemini_response_with_history(gemini_client, history, user_input)
                    
                    st.markdown(response)
                    st.session_state.chat_histories[session_id].append({"role": "ai", "content": response})

    # --- (åŸç‰ˆ) éšæ®µå¼é«˜ç®¡å·¥ä½œæµ (tabs[1]) ---
    with tabs[1]:
        st.header("ğŸ’¼ éšæ®µå¼å·¥ä½œæµ (åŸç‰ˆ)")
        st.info("**æ–¹æ³•èªªæ˜**ï¼šæ­¤æµç¨‹æ¨¡æ“¬ä¸‰ä½ç¨ç«‹çš„å°ˆå®¶ã€‚æ¯ä¸€æ­¥éƒ½æ˜¯ä¸€æ¬¡ç¨ç«‹çš„ API è«‹æ±‚ï¼Œå¾Œä¸€ä½å°ˆå®¶çš„åˆ†æåŸºæ–¼å‰ä¸€ä½çš„æ›¸é¢å ±å‘Šã€‚é€™æ˜¯ä¸€ç¨®**æ¥åŠ›è³½å¼**çš„è¨˜æ†¶å‚³éã€‚")
        st.write("è«‹å…ˆåœ¨å´é‚Šæ¬„ä¸Šå‚³CSVè³‡æ–™ã€‚")
        
        st.session_state.executive_user_query = st.text_area(
            "è«‹è¼¸å…¥å•†æ¥­å•é¡Œä»¥å•Ÿå‹•åˆ†æ:", value=st.session_state.get("executive_user_query", ""), height=100, key="original_workflow_query"
        )
        can_start = bool(st.session_state.get("retriever_chain") and st.session_state.get("executive_user_query"))
        
        if st.button("ğŸš€ å•Ÿå‹•/é‡å•Ÿéšæ®µå¼åˆ†æ", disabled=not can_start, key="exec_flow_button"):
            st.session_state.executive_workflow_stage = "cfo_analysis_pending"
            st.session_state.cfo_analysis_text = ""
            st.session_state.coo_analysis_text = ""
            st.session_state.ceo_summary_text = ""
            st.rerun()

        if st.session_state.executive_workflow_stage == "cfo_analysis_pending":
            with st.spinner("CFO æ­£åœ¨ç¨ç«‹åˆ†æ..."):
                df = pd.read_csv(st.session_state.uploaded_file_path)
                data_profile = generate_data_profile(df)
                retriever = st.session_state.retriever_chain
                query = st.session_state.executive_user_query
                retrieved_docs = retriever.invoke(query)
                rag_context = "\n---\n".join([doc.page_content for doc in retrieved_docs])
                
                # ç‚ºå¾ŒçºŒ expander ä¿å­˜ï¼Œå³ä½¿åœ¨é€™å€‹æµç¨‹ä¸­å®ƒå€‘æ˜¯ä¸€èµ·ç”Ÿæˆçš„
                st.session_state.executive_data_profile_str = data_profile
                st.session_state.executive_rag_context = rag_context

                cfo_prompt = f"ä½œç‚ºè²¡å‹™é•·(CFO)ï¼Œè«‹åŸºæ–¼ä½ çš„å°ˆæ¥­çŸ¥è­˜ï¼Œä¸¦åš´æ ¼åƒè€ƒä»¥ä¸‹æä¾›çš„ã€Œçµ±è¨ˆæ‘˜è¦ã€å’Œã€ŒRAG æª¢ç´¢å‡ºçš„ç›¸é—œæ•¸æ“šã€ï¼Œç‚ºå•†æ¥­å•é¡Œæä¾›è²¡å‹™è§’åº¦çš„ç°¡æ½”åˆ†æã€‚\n\n[å•†æ¥­å•é¡Œ]:\n{query}\n\n[çµ±è¨ˆæ‘˜è¦]:\n{data_profile}\n\n[RAG æª¢ç´¢å‡ºçš„ç›¸é—œæ•¸æ“š]:\n{rag_context}"
                response = get_gemini_executive_analysis(gemini_api_key, "CFO", cfo_prompt)
                st.session_state.cfo_analysis_text = response
                st.session_state.executive_workflow_stage = "coo_analysis_pending"
                st.rerun()
        
        # é¡¯ç¤ºè³‡æ–™å€å¡Š
        if st.session_state.get('executive_data_profile_str'):
            with st.expander("æŸ¥çœ‹çµ±è¨ˆæ‘˜è¦"): st.text(st.session_state.executive_data_profile_str)
        if st.session_state.get('executive_rag_context'):
            with st.expander("æŸ¥çœ‹ RAG æª¢ç´¢å‡ºçš„ç›¸é—œè³‡æ–™"): st.markdown(st.session_state.executive_rag_context)

        if st.session_state.cfo_analysis_text:
            st.subheader("ğŸ“Š è²¡å‹™é•· (CFO) åˆ†æ")
            st.markdown(st.session_state.cfo_analysis_text)
        
        if st.session_state.executive_workflow_stage == "coo_analysis_pending":
            with st.spinner("COO æ­£åœ¨åŸºæ–¼ CFO å ±å‘Šé€²è¡Œåˆ†æ..."):
                coo_prompt = f"ä½œç‚ºç‡Ÿé‹é•·(COO)ï¼Œè«‹åŸºæ–¼å•†æ¥­å•é¡Œã€çµ±è¨ˆæ‘˜è¦ã€RAG æª¢ç´¢æ•¸æ“šä»¥åŠä¸‹æ–¹ CFO çš„åˆ†æï¼Œæä¾›ç‡Ÿé‹å±¤é¢çš„ç­–ç•¥èˆ‡æ½›åœ¨é¢¨éšªã€‚\n\n[å•†æ¥­å•é¡Œ]:\n{st.session_state.executive_user_query}\n\n[CFO çš„è²¡å‹™åˆ†æ]:\n{st.session_state.cfo_analysis_text}\n\n[RAG æª¢ç´¢å‡ºçš„ç›¸é—œæ•¸æ“š]:\n{st.session_state.executive_rag_context}\n\n[çµ±è¨ˆæ‘˜è¦]:\n{st.session_state.executive_data_profile_str}"
                response = get_gemini_executive_analysis(gemini_api_key, "COO", coo_prompt)
                st.session_state.coo_analysis_text = response
                st.session_state.executive_workflow_stage = "ceo_summary_pending"
                st.rerun()

        if st.session_state.coo_analysis_text:
            st.subheader("ğŸ­ ç‡Ÿé‹é•· (COO) åˆ†æ")
            st.markdown(st.session_state.coo_analysis_text)

        if st.session_state.executive_workflow_stage == "ceo_summary_pending":
            with st.spinner("CEO æ­£åœ¨æ•´åˆæ‰€æœ‰å ±å‘Šé€²è¡Œç¸½çµ..."):
                ceo_prompt = f"ä½œç‚ºåŸ·è¡Œé•·(CEO)ï¼Œè«‹æ•´åˆæ‰€æœ‰è³‡è¨Šï¼ŒåŒ…å«åŸå§‹å•é¡Œã€CFO çš„è²¡å‹™åˆ†æã€COO çš„ç‡Ÿé‹åˆ†æä»¥åŠç›¸é—œæ•¸æ“šï¼Œæä¾›ä¸€å€‹é«˜å±¤æ¬¡çš„ã€å¯åŸ·è¡Œçš„æ±ºç­–ç¸½çµèˆ‡æ˜ç¢ºçš„è¡Œå‹•å»ºè­°ã€‚\n\n[å•†æ¥­å•é¡Œ]:\n{st.session_state.executive_user_query}\n\n[CFO çš„è²¡å‹™åˆ†æ]:\n{st.session_state.cfo_analysis_text}\n\n[COO çš„ç‡Ÿé‹åˆ†æ]:\n{st.session_state.coo_analysis_text}\n\n[RAG æª¢ç´¢å‡ºçš„ç›¸é—œæ•¸æ“š]:\n{st.session_state.executive_rag_context}"
                response = get_gemini_executive_analysis(gemini_api_key, "CEO", ceo_prompt)
                st.session_state.ceo_summary_text = response
                st.session_state.executive_workflow_stage = "completed"
                st.rerun()

        if st.session_state.ceo_summary_text:
            st.subheader("ğŸ‘‘ åŸ·è¡Œé•· (CEO) æœ€çµ‚æ±ºç­–")
            st.markdown(st.session_state.ceo_summary_text)


    # --- *** (æ–°ç‰ˆ) å–®ä¸€æç¤ºæ•´åˆå·¥ä½œæµ (tabs[2]) *** ---
    with tabs[2]:
        st.header("ğŸ§© å–®ä¸€æç¤ºæ•´åˆå·¥ä½œæµ (åˆä½µç‰ˆ)")
        st.info("**æ–¹æ³•èªªæ˜**ï¼šæ­¤æµç¨‹æ¨¡æ“¬ä¸€å€‹å…¨èƒ½çš„ AI é«˜ç®¡åœ˜éšŠã€‚æˆ‘å€‘åªç™¼é€**ä¸€æ¬¡**åŒ…å«äº†å®Œæ•´æŒ‡ä»¤çš„è«‹æ±‚ï¼Œè¦æ±‚ AI åœ¨ä¸€æ¬¡ç”Ÿæˆä¸­ï¼ŒåŒæ™‚å®Œæˆ CFOã€COOã€CEO çš„æ€è€ƒï¼Œä¸¦ç”¢å‡ºçµæ§‹åŒ–å ±å‘Šã€‚é€™æ˜¯ä¸€ç¨®**å–®æ¬¡è«‹æ±‚ã€å…§éƒ¨æ•´åˆ**çš„è¨˜æ†¶æ¨¡å¼ã€‚")
        st.write("è«‹åŒæ¨£å…ˆåœ¨å´é‚Šæ¬„ä¸Šå‚³CSVè³‡æ–™ã€‚")
        
        st.session_state.sp_user_query = st.text_area(
             "è«‹è¼¸å…¥å•†æ¥­å•é¡Œä»¥å•Ÿå‹•åˆ†æ:", value=st.session_state.get("sp_user_query", ""), height=100, key="sp_workflow_query"
        )
        can_start_sp = bool(st.session_state.get("retriever_chain") and st.session_state.get("sp_user_query"))

        if st.button("ğŸš€ å•Ÿå‹•å–®ä¸€æç¤ºæ•´åˆåˆ†æ", disabled=not can_start_sp, key="sp_flow_button"):
            st.session_state.sp_workflow_stage = "running"
            st.session_state.sp_final_report = ""
            
            with st.spinner("AI é«˜ç®¡åœ˜éšŠæ­£åœ¨é€²è¡Œå…¨é¢åˆ†æ..."):
                # 1. æº–å‚™è³‡æ–™
                df = pd.read_csv(st.session_state.uploaded_file_path)
                data_profile = generate_data_profile(df)
                retriever = st.session_state.retriever_chain
                query = st.session_state.sp_user_query
                retrieved_docs = retriever.invoke(query)
                rag_context = "\n---\n".join([doc.page_content for doc in retrieved_docs])
                
                # 2. å»ºç«‹è¶…ç´šæç¤º (Mega Prompt)
                mega_prompt = f"""
ä½ æ˜¯ä¸€å€‹é ‚å°–çš„ AI å•†æ¥­åˆ†æåœ˜éšŠï¼Œèƒ½å¤ åœ¨ä¸€æ¬¡æ€è€ƒä¸­æ‰®æ¼”å¤šå€‹é«˜ç®¡è§’è‰²ã€‚ä½ çš„ä»»å‹™æ˜¯é‡å°çµ¦å®šçš„å•†æ¥­å•é¡Œå’Œæ•¸æ“šï¼Œç”Ÿæˆä¸€ä»½åŒ…å«ä¸‰å€‹éƒ¨åˆ†çš„å®Œæ•´åˆ†æå ±å‘Šã€‚

è«‹åš´æ ¼æŒ‰ç…§ä»¥ä¸‹çµæ§‹å’Œè¦æ±‚é€²è¡Œè¼¸å‡ºï¼Œä½¿ç”¨ Markdown æ¨™é¡Œä¾†å€åˆ†æ¯å€‹éƒ¨åˆ†ï¼š

---

### ğŸ“Š è²¡å‹™é•· (CFO) åˆ†æ
åœ¨æ­¤éƒ¨åˆ†ï¼Œè«‹å®Œå…¨ä»¥è²¡å‹™é•·çš„è§’åº¦æ€è€ƒã€‚å°ˆæ³¨æ–¼è²¡å‹™æŒ‡æ¨™ã€æˆæœ¬æ•ˆç›Šã€æŠ•è³‡å›å ±ç‡ã€æ¯›åˆ©ç‡ã€æ½›åœ¨çš„è²¡å‹™é¢¨éšªç­‰ã€‚ä½ çš„åˆ†æå¿…é ˆå®Œå…¨åŸºæ–¼æä¾›çš„æ•¸æ“šã€‚

### ğŸ­ ç‡Ÿé‹é•· (COO) åˆ†æ
åœ¨æ­¤éƒ¨åˆ†ï¼Œè½‰æ›ç‚ºç‡Ÿé‹é•·çš„è§’è‰²ã€‚ä½ éœ€è¦æ€è€ƒï¼Œåœ¨CFOæœƒæå‡ºçš„è²¡å‹™è€ƒé‡ä¸‹ï¼Œç‡Ÿé‹ä¸Šæ˜¯å¦å¯è¡Œï¼Ÿåˆ†ææ½›åœ¨çš„æµç¨‹ã€ä¾›æ‡‰éˆã€äººåŠ›è³‡æºæˆ–åŸ·è¡Œé¢¨éšªã€‚ä½ çš„åˆ†æéœ€è¦å‹™å¯¦ä¸”è‘—é‡æ–¼å¯åŸ·è¡Œæ€§ã€‚

### ğŸ‘‘ åŸ·è¡Œé•· (CEO) æœ€çµ‚æ±ºç­–
åœ¨æ­¤éƒ¨åˆ†ï¼Œä½œç‚ºCEOï¼Œè«‹ç¶œåˆä¸Šè¿°çš„è²¡å‹™(CFO)å’Œç‡Ÿé‹(COO)åˆ†æã€‚ä¸è¦é‡è¤‡ç´°ç¯€ï¼Œè€Œæ˜¯æä¾›ä¸€å€‹é«˜å±¤æ¬¡çš„æˆ°ç•¥ç¸½çµã€‚æœ€çµ‚ï¼Œçµ¦å‡ºä¸€å€‹æ˜ç¢ºã€æœæ–·çš„**æ±ºç­–**ï¼ˆä¾‹å¦‚ï¼šæ‰¹å‡†ã€é§å›ã€éœ€è¦æ›´å¤šè³‡æ–™ï¼‰ï¼Œä¸¦åˆ—å‡º 2-3 å€‹æœ€é‡è¦çš„**å¾ŒçºŒè¡Œå‹•å»ºè­°**ã€‚

---

ç¾åœ¨ï¼Œè«‹æ ¹æ“šä»¥ä¸‹è³‡è¨Šé–‹å§‹åˆ†æï¼š

**[å•†æ¥­å•é¡Œ]:**
{query}

**[è³‡æ–™çµ±è¨ˆæ‘˜è¦]:**
{data_profile}

**[RAG æª¢ç´¢å‡ºçš„ç›¸é—œæ•¸æ“š]:**
{rag_context}
"""
                # 3. åŸ·è¡Œä¸€æ¬¡ API è«‹æ±‚
                response = get_gemini_executive_analysis(gemini_api_key, "IntegratedTeam", mega_prompt)
                st.session_state.sp_final_report = response
                st.session_state.sp_workflow_stage = "completed"
                st.rerun()

        if st.session_state.sp_workflow_stage == "completed" and st.session_state.sp_final_report:
            st.subheader("ğŸ“ˆ AI é«˜ç®¡åœ˜éšŠæ•´åˆå ±å‘Š")
            st.markdown(st.session_state.sp_final_report)


    # --- å…¶ä»– AI è§’è‰²æ¨™ç±¤ (tabs[3] å’Œ tabs[4]) ---
    role_tab_offset = 3 
    for i, (role_id, role_info) in enumerate(ROLE_DEFINITIONS.items()):
        with tabs[i + role_tab_offset]:
            st.header(role_info["name"])
            st.caption(role_info["system_prompt"])
            session_id = role_info["session_id"]
            if session_id not in st.session_state.chat_histories: st.session_state.chat_histories[session_id] = []

            for msg in st.session_state.chat_histories[session_id]:
                with st.chat_message(msg["role"]): st.markdown(msg["content"])

            if user_input := st.chat_input(f"èˆ‡ {role_info['name']} å°è©±...", key=session_id):
                st.session_state.chat_histories[session_id].append({"role": "human", "content": user_input})
                with st.chat_message("human"): st.markdown(user_input)
                
                with st.chat_message("ai"):
                    with st.spinner("æ­£åœ¨ç”Ÿæˆå›æ‡‰..."):
                        client_with_prompt = get_gemini_client(gemini_api_key)
                        # æ³¨æ„: genai for python çš„ system_instruction æ˜¯ Preview åŠŸèƒ½
                        try:
                            client_with_prompt.system_instruction = role_info["system_prompt"]
                        except: # å‚™ç”¨æ–¹æ¡ˆ
                             pass
                        
                        history = st.session_state.chat_histories[session_id][:-1]
                        
                        # å‚™ç”¨æ–¹æ¡ˆï¼šå¦‚æœ system_instruction ä¸å¯ç”¨æˆ–å¤±æ•ˆ
                        final_prompt = f"{role_info['system_prompt']}\n\n{user_input}"
                        
                        response = get_gemini_response_with_history(client_with_prompt, history, final_prompt)
                        
                        st.markdown(response)
                        st.session_state.chat_histories[session_id].append({"role": "ai", "content": response})

if __name__ == "__main__":
    main()
