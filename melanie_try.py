import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import json
import traceback
import re
import os
import dotenv
from io import StringIO
from PIL import Image
import time
import matplotlib.font_manager as fm
import matplotlib
import numpy as np

# --- åŸºç¤ API å’Œè³‡æ–™è™•ç†å¥—ä»¶ ---
import google.generativeai as genai
from openai import OpenAI
import faiss

# --- åˆå§‹åŒ–èˆ‡å¸¸æ•¸å®šç¾© ---
dotenv.load_dotenv()
UPLOAD_DIR = "uploaded_files"
if not os.path.exists(UPLOAD_DIR): os.makedirs(UPLOAD_DIR)

ROLE_DEFINITIONS = {
    "summarizer": { "name": "ğŸ“ æ‘˜è¦å°ˆå®¶", "system_prompt": "ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ‘˜è¦å°ˆå®¶...", "session_id": "summarizer_chat" },
    "creative_writer": { "name": "âœï¸ å‰µæ„ä½œå®¶", "system_prompt": "ä½ æ˜¯ä¸€ä½å……æ»¿æƒ³åƒåŠ›çš„å‰µæ„ä½œå®¶...", "session_id": "creative_writer_chat" }
}

# --- åŸºç¤è¼”åŠ©å‡½æ•¸ (å¹¾ä¹ä¸è®Š) ---
def save_uploaded_file(uploaded_file):
    if not os.path.exists(UPLOAD_DIR): os.makedirs(UPLOAD_DIR)
    file_path = os.path.join(UPLOAD_DIR, uploaded_file.name)
    with open(file_path, "wb") as f: f.write(uploaded_file.getbuffer())
    return file_path

def add_user_image_to_main_chat(uploaded_file):
    try:
        file_path = save_uploaded_file(uploaded_file)
        st.session_state.pending_image_for_main_gemini = Image.open(file_path)
        st.image(st.session_state.pending_image_for_main_gemini, caption="åœ–ç‰‡å·²ä¸Šå‚³...", use_container_width=True)
    except Exception as e: st.error(f"è™•ç†ä¸Šå‚³åœ–ç‰‡æ™‚å‡ºéŒ¯: {e}")

# --- å¾é›¶æ‰“é€  RAG æ ¸å¿ƒå‡½å¼ ---

def split_text(text, chunk_size=1000, chunk_overlap=100):
    """ç°¡å–®çš„æ–‡æœ¬åˆ‡å‰²å‡½å¼"""
    chunks = []
    for i in range(0, len(text), chunk_size - chunk_overlap):
        chunks.append(text[i:i + chunk_size])
    return chunks

def get_openai_embeddings(texts: list, client: OpenAI, model="text-embedding-3-small"):
    """ä½¿ç”¨ OpenAI API ç²å–åµŒå…¥å‘é‡"""
    response = client.embeddings.create(input=texts, model=model)
    return [item.embedding for item in response.data]

@st.cache_resource
def create_knowledge_base(file_path: str, openai_api_key: str):
    """å¾ CSV æª”æ¡ˆå»ºç«‹ä¸€å€‹ FAISS å‘é‡çŸ¥è­˜åº« (ç„¡ LangChain)"""
    with st.status("æ­£åœ¨å»ºç«‹çŸ¥è­˜åº«...", expanded=True) as status:
        try:
            status.update(label="æ­¥é©Ÿ 1/3ï¼šæ­£åœ¨è®€å–èˆ‡åˆ‡å‰²æ–‡ä»¶...")
            df = pd.read_csv(file_path, encoding='utf-8')
            df.dropna(how='all', inplace=True)
            df.fillna("", inplace=True)
            # å°‡æ‰€æœ‰æ¬„ä½åˆä½µç‚ºä¸€å€‹é•·æ–‡æœ¬ï¼Œä¸¦ç‚ºæ¯è¡Œå»ºç«‹ä¸€å€‹ document
            documents = df.apply(lambda row: ' | '.join(row.astype(str)), axis=1).tolist()
            
            all_chunks = []
            for doc in documents:
                all_chunks.extend(split_text(doc))
            status.update(label=f"æ­¥é©Ÿ 1/3 å®Œæˆï¼å·²åˆ‡å‰²æˆ {len(all_chunks)} å€‹æ–‡æœ¬å€å¡Šã€‚")

            status.update(label="æ­¥é©Ÿ 2/3ï¼šæ­£åœ¨å‘¼å« OpenAI API ç”Ÿæˆå‘é‡åµŒå…¥...")
            client = OpenAI(api_key=openai_api_key)
            embeddings = get_openai_embeddings(all_chunks, client)
            status.update(label="æ­¥é©Ÿ 2/3 å®Œæˆï¼å‘é‡åµŒå…¥å·²ç”Ÿæˆã€‚")

            status.update(label="æ­¥é©Ÿ 3/3ï¼šæ­£åœ¨å»ºç«‹ FAISS ç´¢å¼•...")
            dimension = len(embeddings[0])
            index = faiss.IndexFlatL2(dimension)
            index.add(np.array(embeddings).astype('float32'))
            status.update(label="çŸ¥è­˜åº«å·²å°±ç·’ï¼", state="complete", expanded=False)

            return {"index": index, "chunks": all_chunks, "client": client}
        except Exception as e:
            st.error(f"å»ºç«‹çŸ¥è­˜åº«éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
            status.update(label="å»ºç«‹å¤±æ•—", state="error")
            return None

def get_rag_response(query: str, kb: dict, gemini_client):
    """åŸ·è¡Œ RAG æŸ¥è©¢"""
    # 1. æª¢ç´¢ (Retrieve)
    client = kb["client"]
    query_embedding = get_openai_embeddings([query], client)[0]
    D, I = kb["index"].search(np.array([query_embedding]).astype('float32'), k=5)
    retrieved_chunks = [kb["chunks"][i] for i in I[0]]
    context = "\n---\n".join(retrieved_chunks)

    # 2. å¢å¼· (Augment) & 3. ç”Ÿæˆ (Generate)
    prompt = f"""
    è«‹æ ¹æ“šä»¥ä¸‹æä¾›çš„ã€Œä¸Šä¸‹æ–‡ã€ä¾†å›ç­”å•é¡Œã€‚è«‹åªä½¿ç”¨ä¸Šä¸‹æ–‡ä¸­çš„è³‡è¨Šã€‚

    [ä¸Šä¸‹æ–‡]:
    {context}

    [å•é¡Œ]:
    {query}

    [å›ç­”]:
    """
    response = gemini_client.generate_content(prompt)
    return response.text

# --- Gemini API ç›¸é—œå‡½å¼ ---
def get_gemini_client(api_key):
    genai.configure(api_key=api_key)
    return genai.GenerativeModel("gemini-1.5-flash-latest")

def get_gemini_response_with_history(client, history, user_prompt):
    # å°‡æˆ‘å€‘çš„ history æ ¼å¼è½‰æ›ç‚º gemini çš„æ ¼å¼
    gemini_history = []
    for msg in history:
        role = "user" if msg["role"] == "human" else "model"
        gemini_history.append({"role": role, "parts": [msg["content"]]})
    
    chat = client.start_chat(history=gemini_history)
    response = chat.send_message(user_prompt)
    return response.text

# --- ä¸»æ‡‰ç”¨å…¥å£ ---
def main():
    st.set_page_config(page_title="Gemini Multi-Function Bot", page_icon="âœ¨", layout="wide")
    st.title("âœ¨ Gemini å¤šåŠŸèƒ½ AI åŠ©ç† (ç„¡ LangChain ç‰ˆ)")

    # --- åˆå§‹åŒ– Session States ---
    if "knowledge_base" not in st.session_state: st.session_state.knowledge_base = None
    if "uploaded_file_path" not in st.session_state: st.session_state.uploaded_file_path = None
    if "last_uploaded_filename" not in st.session_state: st.session_state.last_uploaded_filename = None
    if "pending_image_for_main_gemini" not in st.session_state: st.session_state.pending_image_for_main_gemini = None
    if "chat_histories" not in st.session_state: st.session_state.chat_histories = {}

    # --- å´é‚Šæ¬„ä»‹é¢ ---
    with st.sidebar:
        st.header("âš™ï¸ API é‡‘é‘°è¨­å®š")
        st.text_input("è«‹è¼¸å…¥æ‚¨çš„ Google Gemini API Key", type="password", key="gemini_api_key_input")
        st.text_input("è«‹è¼¸å…¥æ‚¨çš„ OpenAI API Key", type="password", key="openai_api_key_input")
        st.caption("æœ¬æ‡‰ç”¨ä½¿ç”¨ Gemini é€²è¡ŒèŠå¤©ï¼Œä½¿ç”¨ OpenAI é€²è¡Œè³‡æ–™åµŒå…¥ã€‚")
        st.divider()

        st.subheader("ğŸ“ è³‡æ–™å•ç­” (RAG)")
        uploaded_file = st.file_uploader("ä¸Šå‚³ CSV ä»¥å•Ÿç”¨ RAG å•ç­”åŠŸèƒ½", type=["csv"])
        
        if uploaded_file:
            if uploaded_file.name != st.session_state.get("last_uploaded_filename"):
                openai_api_key = st.session_state.get("openai_api_key_input") or os.environ.get("OPENAI_API_KEY")
                if not openai_api_key:
                    st.error("è«‹åœ¨å´é‚Šæ¬„æˆ– Secrets ä¸­è¨­å®šæ‚¨çš„ OpenAI API Keyï¼")
                else:
                    st.session_state.last_uploaded_filename = uploaded_file.name
                    file_path = save_uploaded_file(uploaded_file)
                    st.session_state.uploaded_file_path = file_path
                    st.session_state.knowledge_base = create_knowledge_base(file_path, openai_api_key)
        
        if st.session_state.knowledge_base: st.success("âœ… RAG å•ç­”åŠŸèƒ½å·²å•Ÿç”¨ï¼")
        
        st.divider()
        if st.button("ğŸ—‘ï¸ æ¸…é™¤æ‰€æœ‰å°è©±èˆ‡è³‡æ–™"):
            st.session_state.clear()
            st.cache_resource.clear()
            st.success("æ‰€æœ‰å°è©±ã€Session è¨˜æ†¶å’Œå¿«å–å·²æ¸…é™¤ï¼")
            st.rerun()

    # --- ä¸»å·¥ä½œå€ (æ¨™ç±¤é é¢) ---
    tab_titles = ["ğŸ’¬ ä¸»è¦èŠå¤©å®¤"] + [role["name"] for role in ROLE_DEFINITIONS.values()]
    tabs = st.tabs(tab_titles)

    # --- API Key æª¢æŸ¥ ---
    gemini_api_key = st.session_state.get("gemini_api_key_input") or os.environ.get("GOOGLE_API_KEY")
    if not gemini_api_key:
        st.warning("è«‹åœ¨å´é‚Šæ¬„è¼¸å…¥æ‚¨çš„ Google Gemini API Key ä»¥å•Ÿå‹•èŠå¤©åŠŸèƒ½ã€‚")
        st.stop()
    gemini_client = get_gemini_client(gemini_api_key)

    # --- ä¸»è¦èŠå¤©å®¤ ---
    with tabs[0]:
        st.header("ğŸ’¬ ä¸»è¦èŠå¤©å®¤")
        session_id = "main_chat"
        if session_id not in st.session_state.chat_histories: st.session_state.chat_histories[session_id] = []
        
        # é¡¯ç¤ºæ­·å²è¨Šæ¯
        for msg in st.session_state.chat_histories[session_id]:
            with st.chat_message(msg["role"]): st.markdown(msg["content"])
        
        if user_input := st.chat_input("è«‹å°æ•¸æ“šæˆ–åœ–ç‰‡æå•..."):
            st.session_state.chat_histories[session_id].append({"role": "human", "content": user_input})
            with st.chat_message("human"): st.markdown(user_input)
            
            with st.chat_message("ai"):
                with st.spinner("æ­£åœ¨æ€è€ƒä¸­..."):
                    response = ""
                    # RAG å•ç­”
                    if st.session_state.knowledge_base:
                        response = get_rag_response(user_input, st.session_state.knowledge_base, gemini_client)
                    # ä¸€èˆ¬èŠå¤©
                    else:
                        history = st.session_state.chat_histories[session_id][:-1] # ä¸åŒ…å«ç•¶å‰å•é¡Œ
                        response = get_gemini_response_with_history(gemini_client, history, user_input)
                    
                    st.markdown(response)
                    st.session_state.chat_histories[session_id].append({"role": "ai", "content": response})

    # --- å…¶ä»– AI è§’è‰²æ¨™ç±¤ ---
    for i, (role_id, role_info) in enumerate(ROLE_DEFINITIONS.items()):
        with tabs[i + 2]:
            st.header(role_info["name"])
            st.caption(role_info["system_prompt"])
            session_id = role_info["session_id"]
            if session_id not in st.session_state.chat_histories: st.session_state.chat_histories[session_id] = []

            # é¡¯ç¤ºæ­·å²è¨Šæ¯
            for msg in st.session_state.chat_histories[session_id]:
                with st.chat_message(msg["role"]): st.markdown(msg["content"])

            if user_input := st.chat_input(f"èˆ‡ {role_info['name']} å°è©±..."):
                st.session_state.chat_histories[session_id].append({"role": "human", "content": user_input})
                with st.chat_message("human"): st.markdown(user_input)
                
                with st.chat_message("ai"):
                    with st.spinner("æ­£åœ¨ç”Ÿæˆå›æ‡‰..."):
                        # ç‚ºè§’è‰²å°ˆå®¶åŠ ä¸Š system_prompt
                        client_with_prompt = get_gemini_client(gemini_api_key)
                        client_with_prompt.system_instruction = role_info["system_prompt"]
                        
                        history = st.session_state.chat_histories[session_id][:-1]
                        response = get_gemini_response_with_history(client_with_prompt, history, user_input)
                        
                        st.markdown(response)
                        st.session_state.chat_histories[session_id].append({"role": "ai", "content": response})

if __name__ == "__main__":
    main()
