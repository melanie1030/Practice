import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import json
import traceback
import re
import os
import dotenv
from io import StringIO
from PIL import Image
import google.generativeai as genai
import time
import matplotlib.font_manager as fm
import matplotlib
from operator import itemgetter

# --- LangChain Imports ---
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_google_genai import ChatGoogleGenerativeAI
# --- æœ¬åœ°åµŒå…¥æ¨¡å‹ Import ---
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import OpenAIEmbeddings
from langchain_community.document_loaders import CSVLoader
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter

# --- Role & Workflow Definitions ---
ROLE_DEFINITIONS = {
    "summarizer": {
        "name": "ğŸ“ æ‘˜è¦å°ˆå®¶",
        "system_prompt": "ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ‘˜è¦å°ˆå®¶ã€‚ä½ çš„ä»»å‹™æ˜¯å°‡æä¾›çš„ä»»ä½•æ–‡æœ¬æˆ–å°è©±ï¼Œæ¿ƒç¸®æˆæ¸…æ™°ã€ç°¡æ½”çš„ç¹é«”ä¸­æ–‡æ‘˜è¦ã€‚å°ˆæ³¨æ–¼è¦é»å’Œé—œéµçµè«–ã€‚",
        "session_id": "summarizer_session",
    },
    "creative_writer": {
        "name": "âœï¸ å‰µæ„ä½œå®¶",
        "system_prompt": "ä½ æ˜¯ä¸€ä½å……æ»¿æƒ³åƒåŠ›çš„å‰µæ„ä½œå®¶ã€‚ä½ çš„ä»»å‹™æ˜¯å¹«åŠ©ä½¿ç”¨è€…å®Œæˆå‰µæ„å¯«ä½œï¼Œä¾‹å¦‚å¯«æ•…äº‹ã€è©©æ­Œã€åŠ‡æœ¬æˆ–è…¦åŠ›æ¿€ç›ªï¼Œå…¨éƒ¨ä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚",
        "session_id": "creative_writer_session",
    }
}
EXECUTIVE_ROLE_IDS = { "CFO": "cfo_exec", "COO": "coo_exec", "CEO": "ceo_exec" }

# --- åˆå§‹åŒ–è¨­ç½® ---
dotenv.load_dotenv()
UPLOAD_DIR = "uploaded_files"
if not os.path.exists(UPLOAD_DIR): os.makedirs(UPLOAD_DIR)

# --- åŸºç¤è¼”åŠ©å‡½æ•¸ ---
def save_uploaded_file(uploaded_file):
    if not os.path.exists(UPLOAD_DIR): os.makedirs(UPLOAD_DIR)
    file_path = os.path.join(UPLOAD_DIR, uploaded_file.name)
    with open(file_path, "wb") as f: f.write(uploaded_file.getbuffer())
    return file_path

def add_user_image_to_main_chat(uploaded_file):
    try:
        file_path = save_uploaded_file(uploaded_file)
        st.session_state.pending_image_for_main_gemini = Image.open(file_path)
        st.image(st.session_state.pending_image_for_main_gemini, caption="åœ–ç‰‡å·²ä¸Šå‚³ï¼Œå°‡éš¨ä¸‹ä¸€æ¢æ–‡å­—è¨Šæ¯ç™¼é€ã€‚", use_container_width=True)
    except Exception as e: st.error(f"è™•ç†ä¸Šå‚³åœ–ç‰‡æ™‚å‡ºéŒ¯: {e}")

# --- LangChain æ ¸å¿ƒå‡½å¼ ---
def get_session_history(session_id: str) -> BaseChatMessageHistory:
    if "history_store" not in st.session_state: st.session_state.history_store = {}
    if session_id not in st.session_state.history_store:
        st.session_state.history_store[session_id] = ChatMessageHistory()
    return st.session_state.history_store[session_id]

# é€™æ˜¯æ›¿æ›å¾Œçš„å‡½å¼
@st.cache_resource
def create_vector_db_with_openai(file_path: str, openai_api_key: str): # è®“å‡½å¼æ¥æ”¶ key
    """
    ä½¿ç”¨ç©©å®šæ€§é«˜çš„ OpenAI API ä¾†å»ºç«‹å‘é‡è³‡æ–™åº«ã€‚
    """
    with st.status("æ­£åœ¨åˆå§‹åŒ–çŸ¥è­˜åº« (OpenAI æ¨¡å¼)...", expanded=True) as status:
        try:
            # (æ­¥é©Ÿ 1 çš„è¼‰å…¥èˆ‡æ¸…ç†è³‡æ–™é‚è¼¯ä¸è®Š)
            status.update(label="æ­¥é©Ÿ 1/3ï¼šæ­£åœ¨è¼‰å…¥èˆ‡æ¸…ç†è³‡æ–™...")
            df = pd.read_csv(file_path, encoding='utf-8')
            df.dropna(how='all', inplace=True)
            df.fillna("", inplace=True)
            clean_file_path = os.path.join(UPLOAD_DIR, f"clean_{os.path.basename(file_path)}")
            df.to_csv(clean_file_path, index=False)
            
            loader = CSVLoader(file_path=clean_file_path, encoding='utf-8')
            documents = loader.load()
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
            docs = text_splitter.split_documents(documents)
            status.update(label=f"æ­¥é©Ÿ 1/3 å®Œæˆï¼å·²å°‡æ–‡ä»¶åˆ‡å‰²æˆ {len(docs)} å€‹å€å¡Šã€‚")

            # æ­¥é©Ÿ 2ï¼šä½¿ç”¨ OpenAI æ¨¡å‹ç”Ÿæˆå‘é‡åµŒå…¥
            status.update(label="æ­¥é©Ÿ 2/3ï¼šæ­£åœ¨å‘¼å« OpenAI API ç”Ÿæˆå‘é‡åµŒå…¥...")
            
            # --- æ ¸å¿ƒä¿®æ”¹ï¼šç›´æ¥ä½¿ç”¨å‚³å…¥çš„ API Key ---
            embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
            
            vector_store = FAISS.from_documents(docs, embeddings)
            status.update(label="æ­¥é©Ÿ 2/3 å®Œæˆï¼å‘é‡åµŒå…¥å·²ç”Ÿæˆã€‚")

            status.update(label="æ­¥é©Ÿ 3/3ï¼šçŸ¥è­˜åº«æº–å‚™å®Œæˆï¼", state="complete", expanded=False)
            st.success("çŸ¥è­˜åº«å»ºç«‹å®Œæˆï¼")
            return vector_store

        except Exception as e:
            st.error(f"å»ºç«‹çŸ¥è­˜åº«éç¨‹ä¸­ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}")
            # ç•¶å¿«å–å‡½å¼å‡ºéŒ¯æ™‚ï¼Œè¿”å› None
            status.update(label="å»ºç«‹å¤±æ•—", state="error")
            return None

def create_rag_chain(vector_store):
    """å»ºç«‹ä¸€å€‹æ•´åˆäº†ã€Œå°è©±å…§è¨˜æ†¶ã€çš„ RAG éˆ"""
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest", temperature=0.7)
    retriever = vector_store.as_retriever()
    prompt_template = ChatPromptTemplate.from_messages([
        ("system", "è«‹æ ¹æ“šã€Œä¸Šä¸‹æ–‡ã€å›ç­”å•é¡Œã€‚\n[ä¸Šä¸‹æ–‡]:\n{context}"),
        ("human", "{input}"),
    ])
    base_rag_chain = ({ "context": itemgetter("input") | retriever, "input": itemgetter("input")} | prompt_template | llm | StrOutputParser())
    rag_chain_with_history = RunnableWithMessageHistory(base_rag_chain, get_session_history, input_messages_key="input", history_messages_key="history")
    return rag_chain_with_history

def create_generic_chat_chain(system_prompt: str):
    """ç‚ºç‰¹å®šè§’è‰²å»ºç«‹ä¸€å€‹å…·å‚™ã€Œå°è©±å…§è¨˜æ†¶ã€çš„é€šç”¨èŠå¤©éˆ"""
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest", temperature=0.7)
    prompt_template = ChatPromptTemplate.from_messages([("system", system_prompt), ("human", "{input}")])
    base_chain = prompt_template | llm | StrOutputParser()
    chain_with_history = RunnableWithMessageHistory(base_chain, get_session_history, input_messages_key="input", history_messages_key="history")
    return chain_with_history

# --- å…¶ä»– Gemini å‘¼å«å‡½å¼ ---
def get_gemini_response_for_image(user_prompt, image_pil):
    """å°ˆé–€ç”¨æ–¼è™•ç†åœ–ç‰‡åˆ†æçš„å‡½å¼"""
    api_key = st.session_state.get("gemini_api_key_input") or os.environ.get("GOOGLE_API_KEY")
    if not api_key: return "éŒ¯èª¤ï¼šæœªè¨­å®š Gemini API Keyã€‚"
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel("gemini-1.5-flash-latest")
        response = model.generate_content([user_prompt, image_pil])
        st.session_state.pending_image_for_main_gemini = None # æ¸…é™¤å¾…è™•ç†åœ–ç‰‡
        return response.text
    except Exception as e:
        st.error(f"Gemini åœ–ç‰‡åˆ†æè«‹æ±‚å¤±æ•—: {e}")
        return f"éŒ¯èª¤: {e}"

def get_gemini_executive_analysis(executive_role_name, full_prompt):
    api_key = st.session_state.get("gemini_api_key_input") or os.environ.get("GOOGLE_API_KEY")
    if not api_key: return f"éŒ¯èª¤ï¼šé«˜ç®¡å·¥ä½œæµ ({executive_role_name}) æœªèƒ½ç²å– Gemini API Keyã€‚"
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel("gemini-1.5-pro-latest")
        response = model.generate_content(full_prompt)
        return response.text
    except Exception as e:
        st.error(f"é«˜ç®¡åˆ†æ ({executive_role_name}) å¤±æ•—: {e}")
        return f"éŒ¯èª¤: {e}"
        
def generate_data_profile(df):
    if df is None or df.empty: return "æ²’æœ‰è³‡æ–™å¯ä¾›åˆ†æã€‚"
    buffer = StringIO()
    df.info(buf=buffer)
    profile_parts = [f"è³‡æ–™å½¢ç‹€: {df.shape}", f"æ¬„ä½è³‡è¨Š:\n{buffer.getvalue()}"]
    try: profile_parts.append(f"\næ•¸å€¼æ¬„ä½çµ±è¨ˆ:\n{df.describe(include='number').to_string()}")
    except: pass
    try: profile_parts.append(f"\né¡åˆ¥æ¬„ä½çµ±è¨ˆ:\n{df.describe(include=['object', 'category']).to_string()}")
    except: pass
    profile_parts.append(f"\nå‰ 5 ç­†è³‡æ–™:\n{df.head().to_string()}")
    return "\n".join(profile_parts)

# ------------------------------
# ä¸»æ‡‰ç”¨å…¥å£
# ------------------------------
def main():
    st.set_page_config(page_title="Gemini Multi-Function Bot", page_icon="âœ¨", layout="wide")
    st.title("âœ¨ Gemini å¤šåŠŸèƒ½ AI åŠ©ç†")

    # --- åˆå§‹åŒ– Session States ---
    # ç‚ºç¢ºä¿æ‡‰ç”¨ç¨‹å¼åœ¨å„ç¨®æƒ…æ³ä¸‹éƒ½èƒ½æ­£å¸¸é‹è¡Œï¼Œæˆ‘å€‘åœ¨æ­¤åˆå§‹åŒ–æ‰€æœ‰éœ€è¦çš„éµ
    keys_to_init = {
        "rag_chain": None,
        "uploaded_file_path": None,
        "last_uploaded_filename": None,
        "pending_image_for_main_gemini": None,
        "executive_workflow_stage": "idle",
        "executive_user_query": "",
        "executive_data_profile_str": "",
        "cfo_analysis_text": "",
        "coo_analysis_text": "",
        "ceo_summary_text": "",
        "debug_mode": False
    }
    for key, default_value in keys_to_init.items():
        if key not in st.session_state:
            st.session_state[key] = default_value

    # --- å´é‚Šæ¬„ä»‹é¢ ---
    with st.sidebar:
        st.header("âš™ï¸ API é‡‘é‘°è¨­å®š")

        # Gemini API Key è¼¸å…¥æ¡†
        st.text_input(
            "è«‹è¼¸å…¥æ‚¨çš„ Google Gemini API Key",
            value=st.session_state.get("gemini_api_key_input", ""),
            type="password",
            key="gemini_api_key_input"
        )

        # OpenAI API Key è¼¸å…¥æ¡†
        st.text_input(
            "è«‹è¼¸å…¥æ‚¨çš„ OpenAI API Key",
            value=st.session_state.get("openai_api_key_input", ""),
            type="password",
            key="openai_api_key_input"
        )
        
        st.caption("æœ¬æ‡‰ç”¨ä½¿ç”¨ Gemini é€²è¡ŒèŠå¤©ï¼Œä½¿ç”¨ OpenAI é€²è¡Œè³‡æ–™åµŒå…¥ã€‚è«‹æä¾›å…©ç¨®é‡‘é‘°ã€‚å´é‚Šæ¬„è¼¸å…¥çš„å„ªå…ˆç´šæœ€é«˜ã€‚")
        st.divider()

        st.subheader("ğŸ“ è³‡æ–™å•ç­” (RAG)")
        uploaded_file = st.file_uploader("ä¸Šå‚³ CSV ä»¥å•Ÿç”¨ RAG å•ç­”åŠŸèƒ½", type=["csv"])
        
        if uploaded_file:
            if uploaded_file.name != st.session_state.get("last_uploaded_filename"):
                # é¸æ“‡ API Key çš„é‚è¼¯
                openai_api_key = (
                    st.session_state.get("openai_api_key_input")
                    or st.secrets.get("OPENAI_API_KEY")
                    or os.environ.get("OPENAI_API_KEY")
                )

                if not openai_api_key:
                    st.error("è«‹åœ¨å´é‚Šæ¬„æˆ– Secrets ä¸­è¨­å®šæ‚¨çš„ OpenAI API Keyï¼")
                else:
                    st.session_state.last_uploaded_filename = uploaded_file.name
                    file_path = save_uploaded_file(uploaded_file)
                    st.session_state.uploaded_file_path = file_path
                    # å°‡é¸æ“‡å¥½çš„ Key å‚³å…¥å‡½å¼
                    vector_store = create_vector_db_with_openai(file_path, openai_api_key)
                    
                    if vector_store: # æª¢æŸ¥æ˜¯å¦æˆåŠŸå»ºç«‹
                        st.session_state.rag_chain = create_rag_chain(vector_store)
        
        if st.session_state.rag_chain:
            st.success("âœ… RAG å•ç­”åŠŸèƒ½å·²å•Ÿç”¨ï¼")

        st.subheader("ğŸ–¼ï¸ åœ–ç‰‡åˆ†æ")
        uploaded_image = st.file_uploader("ä¸Šå‚³åœ–ç‰‡é€²è¡Œåˆ†æ", type=["png", "jpg", "jpeg"])
        if uploaded_image: add_user_image_to_main_chat(uploaded_image)
        st.divider()

        if st.button("ğŸ—‘ï¸ æ¸…é™¤æ‰€æœ‰å°è©±èˆ‡è³‡æ–™"):
            keys_to_clear = [k for k in st.session_state.keys() if 'api_key_input' not in k]
            for key in keys_to_clear:
                st.session_state.pop(key)
            # æ¸…é™¤æ‰€æœ‰å¿«å–è³‡æº
            st.cache_resource.clear()
            st.success("æ‰€æœ‰å°è©±ã€Session è¨˜æ†¶å’Œå¿«å–å·²æ¸…é™¤ï¼")
            st.rerun()

        st.session_state.debug_mode = st.checkbox("å•Ÿç”¨åµéŒ¯æ¨¡å¼", value=st.session_state.debug_mode)

    # --- ä¸»å·¥ä½œå€ (æ¨™ç±¤é é¢) ---
    tab_titles = ["ğŸ’¬ ä¸»è¦èŠå¤©å®¤", "ğŸ’¼ é«˜ç®¡å·¥ä½œæµ"] + [role["name"] for role in ROLE_DEFINITIONS.values()]
    tabs = st.tabs(tab_titles)

    # --- ä¸»è¦èŠå¤©å®¤ ---
    with tabs[0]:
        st.header("ğŸ’¬ ä¸»è¦èŠå¤©å®¤")
        st.caption("å¯é€²è¡Œ RAG è³‡æ–™å•ç­”ã€åœ–ç‰‡åˆ†æã€æˆ–ä¸€èˆ¬å°è©± (å‡å…·å‚™ Session è¨˜æ†¶)ã€‚")
        
        MAIN_CHAT_SESSION_ID = "main_chat_session"
        history = get_session_history(MAIN_CHAT_SESSION_ID)

        # é¡¯ç¤ºæ­·å²è¨Šæ¯
        for msg in history.messages:
            with st.chat_message(msg.type):
                st.markdown(msg.content)
        
        if user_input := st.chat_input("è«‹å°æ•¸æ“šæˆ–åœ–ç‰‡æå•..."):
            with st.chat_message("human"):
                st.markdown(user_input)
            
            with st.chat_message("ai"):
                with st.spinner("Gemini æ­£åœ¨æ€è€ƒä¸­..."):
                    response = ""
                    # æƒ…å¢ƒ1ï¼šRAG å•ç­”
                    if st.session_state.rag_chain:
                        response = st.session_state.rag_chain.invoke(
                            {"input": user_input},
                            config={"configurable": {"session_id": MAIN_CHAT_SESSION_ID}}
                        )
                    # æƒ…å¢ƒ2ï¼šåœ–ç‰‡å•ç­”
                    elif st.session_state.pending_image_for_main_gemini:
                        response = get_gemini_response_for_image(
                            user_input, st.session_state.pending_image_for_main_gemini
                        )
                        # æ‰‹å‹•å°‡åœ–æ–‡å•ç­”åŠ å…¥æ­·å²
                        history.add_user_message(user_input)
                        history.add_ai_message(response)
                    # æƒ…å¢ƒ3ï¼šä¸€èˆ¬èŠå¤©
                    else:
                        if "general_chat_chain" not in st.session_state:
                            st.session_state.general_chat_chain = create_generic_chat_chain("ä½ æ˜¯ä¸€å€‹æ¨‚æ–¼åŠ©äººçš„ AI åŠ©ç†ã€‚")
                        response = st.session_state.general_chat_chain.invoke(
                            {"input": user_input},
                            config={"configurable": {"session_id": MAIN_CHAT_SESSION_ID}}
                        )
                    st.markdown(response)

    # --- é«˜ç®¡å·¥ä½œæµæ¨™ç±¤ ---
    with tabs[1]:
        st.header("ğŸ’¼ é«˜ç®¡å·¥ä½œæµ (ç”± Gemini Pro é©…å‹•)")
        st.write("è«‹å…ˆåœ¨å´é‚Šæ¬„ä¸Šå‚³CSVè³‡æ–™ï¼Œç„¶å¾Œåœ¨æ­¤è¼¸å…¥å•†æ¥­å•é¡Œï¼Œæœ€å¾Œé»æ“ŠæŒ‰éˆ•å•Ÿå‹•åˆ†æã€‚")
        
        st.session_state.executive_user_query = st.text_area(
            "è«‹è¼¸å…¥å•†æ¥­å•é¡Œä»¥å•Ÿå‹•åˆ†æ:", 
            value=st.session_state.get("executive_user_query", ""), 
            height=100
        )
        can_start = bool(st.session_state.get("uploaded_file_path") and st.session_state.get("executive_user_query"))
        
        if st.button("ğŸš€ å•Ÿå‹•/é‡å•Ÿé«˜ç®¡åˆ†æ", disabled=not can_start):
             st.session_state.executive_workflow_stage = "data_profiling_pending"
             st.session_state.cfo_analysis_text = ""
             st.session_state.coo_analysis_text = ""
             st.session_state.ceo_summary_text = ""
             st.rerun()
        
        if st.session_state.executive_workflow_stage == "data_profiling_pending":
             with st.spinner("æ­£åœ¨ç”Ÿæˆè³‡æ–™æ‘˜è¦..."):
                df = pd.read_csv(st.session_state.uploaded_file_path)
                st.session_state.executive_data_profile_str = generate_data_profile(df)
                st.session_state.executive_workflow_stage = "cfo_analysis_pending"
                st.rerun()

        if st.session_state.get('executive_data_profile_str'):
            with st.expander("æŸ¥çœ‹è³‡æ–™æ‘˜è¦"):
                st.text(st.session_state.executive_data_profile_str)
        
        # --- CFO åˆ†æéšæ®µ ---
        if st.session_state.executive_workflow_stage == "cfo_analysis_pending":
            with st.spinner("CFO æ­£åœ¨åˆ†æ... (Gemini Pro)"):
                cfo_prompt = f"ä½œç‚ºè²¡å‹™é•·(CFO)ï¼Œè«‹åŸºæ–¼å•†æ¥­å•é¡Œ '{st.session_state.executive_user_query}' å’Œä»¥ä¸‹è³‡æ–™æ‘˜è¦ï¼Œæä¾›è²¡å‹™è§’åº¦çš„ç°¡æ½”åˆ†æã€‚\n\nè³‡æ–™æ‘˜è¦:\n{st.session_state.executive_data_profile_str}"
                response = get_gemini_executive_analysis("CFO", cfo_prompt)
                st.session_state.cfo_analysis_text = response
                st.session_state.executive_workflow_stage = "coo_analysis_pending"
                st.rerun()
        
        if st.session_state.cfo_analysis_text:
            st.subheader("ğŸ“Š è²¡å‹™é•· (CFO) åˆ†æ")
            st.markdown(st.session_state.cfo_analysis_text)
        
        # --- COO åˆ†æéšæ®µ ---
        if st.session_state.executive_workflow_stage == "coo_analysis_pending":
            with st.spinner("COO æ­£åœ¨åˆ†æ... (Gemini Pro)"):
                coo_prompt = f"ä½œç‚ºç‡Ÿé‹é•·(COO)ï¼Œè«‹åŸºæ–¼å•†æ¥­å•é¡Œ '{st.session_state.executive_user_query}'ã€è³‡æ–™æ‘˜è¦å’ŒCFOçš„åˆ†æï¼Œæä¾›ç‡Ÿé‹å±¤é¢çš„ç­–ç•¥èˆ‡é¢¨éšªã€‚\n\nCFOåˆ†æ:\n{st.session_state.cfo_analysis_text}\n\nè³‡æ–™æ‘˜è¦:\n{st.session_state.executive_data_profile_str}"
                response = get_gemini_executive_analysis("COO", coo_prompt)
                st.session_state.coo_analysis_text = response
                st.session_state.executive_workflow_stage = "ceo_summary_pending"
                st.rerun()

        if st.session_state.coo_analysis_text:
            st.subheader("ğŸ­ ç‡Ÿé‹é•· (COO) åˆ†æ")
            st.markdown(st.session_state.coo_analysis_text)

        # --- CEO ç¸½çµéšæ®µ ---
        if st.session_state.executive_workflow_stage == "ceo_summary_pending":
            with st.spinner("CEO æ­£åœ¨é€²è¡Œæœ€çµ‚ç¸½çµ... (Gemini Pro)"):
                ceo_prompt = f"ä½œç‚ºåŸ·è¡Œé•·(CEO)ï¼Œè«‹æ•´åˆä»¥ä¸‹æ‰€æœ‰è³‡è¨Šï¼Œæä¾›é«˜å±¤æ¬¡çš„æ±ºç­–ç¸½çµèˆ‡è¡Œå‹•å»ºè­°ã€‚\n\nå•†æ¥­å•é¡Œ: {st.session_state.executive_user_query}\n\nCFOåˆ†æ:\n{st.session_state.cfo_analysis_text}\n\nCOOåˆ†æ:\n{st.session_state.coo_analysis_text}\n\nåŸå§‹è³‡æ–™æ‘˜è¦:\n{st.session_state.executive_data_profile_str}"
                response = get_gemini_executive_analysis("CEO", ceo_prompt)
                st.session_state.ceo_summary_text = response
                st.session_state.executive_workflow_stage = "completed"
                st.rerun()

        if st.session_state.ceo_summary_text:
            st.subheader("ğŸ‘‘ åŸ·è¡Œé•· (CEO) æœ€çµ‚æ±ºç­–")
            st.markdown(st.session_state.ceo_summary_text)

    # --- å…¶ä»– AI è§’è‰²æ¨™ç±¤ ---
    for i, (role_id, role_info) in enumerate(ROLE_DEFINITIONS.items()):
        with tabs[i + 2]:
            st.header(role_info["name"])
            st.caption(role_info["system_prompt"])
            
            if f"{role_id}_chain" not in st.session_state:
                st.session_state[f"{role_id}_chain"] = create_generic_chat_chain(role_info["system_prompt"])
            
            chain = st.session_state[f"{role_id}_chain"]
            session_id = role_info["session_id"]
            history = get_session_history(session_id)
            
            # é¡¯ç¤ºæ­·å²è¨Šæ¯
            for msg in history.messages:
                with st.chat_message(msg.type):
                    st.markdown(msg.content)
            
            # èŠå¤©è¼¸å…¥
            if user_input := st.chat_input(f"èˆ‡ {role_info['name']} å°è©±..."):
                with st.chat_message("human"):
                    st.markdown(user_input)
                with st.chat_message("ai"):
                    with st.spinner("æ­£åœ¨ç”Ÿæˆå›æ‡‰..."):
                        response = chain.invoke(
                            {"input": user_input},
                            config={"configurable": {"session_id": session_id}}
                        )
                        st.markdown(response)
if __name__ == "__main__":
    main()
