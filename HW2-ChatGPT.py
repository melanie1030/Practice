import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import json
import traceback
import re
import os
import dotenv
import base64
from io import BytesIO
from openai import OpenAI  # Custom OpenAI class as per your requirement
from PIL import Image
from streamlit_ace import st_ace

# --- åˆå§‹åŒ–èˆ‡è¨­ç½® ---
dotenv.load_dotenv()

# å®šç¾©å…¨åŸŸè®Šæ•¸
OPENAI_MODELS = ["gpt-4o"]

# --- è¼”åŠ©å‡½æ•¸ ---

def initialize_client(api_key):
    """Initialize OpenAI client with the provided API key."""
    return OpenAI(api_key=api_key) if api_key else None

def load_image_base64(image):
    """Convert an image to Base64 encoding."""
    buffer = BytesIO()
    image.save(buffer, format=image.format)
    return base64.b64encode(buffer.getvalue()).decode('utf-8')

def add_user_image(image):
    """Add an image message to the session state."""
    img_base64 = load_image_base64(image)
    st.session_state.messages.append({
        "role": "user",
        "content": [{"type": "image_url", "image_url": {"url": f"data:image/png;base64,{img_base64}"}}]
    })

def reset_session_messages():
    """Clear conversation history from the session."""
    if "messages" in st.session_state:
        st.session_state.pop("messages")

# --- Chatbot Main Functionality ---

def stream_llm_response(client, model_params):
    """Stream responses from the LLM model."""
    for chunk in client.chat.completions.create(
            model=model_params.get("model", "gpt-4o"),
            messages=st.session_state.messages,
            temperature=model_params.get("temperature", 0.3),
            max_tokens=4096,
            stream=True):
        chunk_text = chunk.choices[0].delta.content or ""
        yield chunk_text

def main():
    # --- é é¢é…ç½® ---
    st.set_page_config(page_title="èŠå¤©æ©Ÿå™¨äºº", page_icon="ğŸ¤–", layout="centered", initial_sidebar_state="expanded")
    st.html("""<h1 style="text-align: center; color: #6ca395;">ğŸ¤– <i>OpenAI èŠå¤©æ©Ÿå™¨äºº</i> </h1>""")

    # --- å´é‚Šæ¬„è¨­ç½® ---
    with st.sidebar:
        st.subheader("ğŸ” è¼¸å…¥æ‚¨çš„APIå¯†é‘°")
        default_api_key = os.getenv("OPENAI_API_KEY", "")
        api_key = st.text_input("OpenAI APIå¯†é‘°", value=default_api_key, type="password")

        client = initialize_client(api_key)
        if not api_key or not client:
            st.warning("â¬…ï¸ è«‹è¼¸å…¥APIå¯†é‘°ä»¥ç¹¼çºŒ...")
            return

        # æ¨¡å‹è¨­ç½®
        model = st.selectbox("é¸æ“‡æ¨¡å‹:", OPENAI_MODELS)
        model_params = {"model": model, "temperature": st.slider("æº«åº¦", 0.0, 2.0, 0.3)}

        # åœ–åƒä¸Šå‚³å’Œæ‹ç…§åŠŸèƒ½
        st.write("### ä¸Šå‚³åœ–åƒæˆ–æ‹ç…§")
        
        # åœ–åƒä¸Šå‚³
        uploaded_img = st.file_uploader("é¸æ“‡ä¸€å¼µåœ–ç‰‡:", type=["png", "jpg", "jpeg"])
        if uploaded_img:
            img = Image.open(uploaded_img)
            add_user_image(img)
            st.success("åœ–åƒå·²ä¸Šå‚³!")

        # ç›¸æ©Ÿæ‹ç…§
        camera_img = st.camera_input("æ‹ç…§")
        if camera_img:
            img = Image.open(camera_img)
            add_user_image(img)
            st.success("æ‹ç…§å·²æˆåŠŸ!")

        # é‡ç½®å°è©±
        st.button("ğŸ—‘ï¸ æ¸…é™¤å°è©±", on_click=reset_session_messages)

    # --- èŠå¤©çª—å£èˆ‡è¨Šæ¯ ---
    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            for content in message["content"]:
                if content["type"] == "text":
                    st.write(content.get("text", ""))
                elif content["type"] == "image_url":
                    st.image(content["image_url"].get("url", ""))

    # --- ç”¨æˆ¶è¼¸å…¥ ---
    prompt = st.chat_input("å—¨ï¼å•æˆ‘ä»»ä½•å•é¡Œ...")
    if prompt:
        st.session_state.messages.append({
            "role": "user",
            "content": [{"type": "text", "text": prompt}]
        })
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            st.write_stream(stream_llm_response(client, model_params))

# ç¨‹å¼å…¥å£é»
if __name__ == "__main__":
    main()
